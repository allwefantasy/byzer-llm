![logo.jpg](https://raw.gitcode.com/allwefantasy11/byzer-llm/attachment/uploads/f5751555-1419-470c-8a33-dfcdc238789d/logo.jpg 'logo.jpg')

<h3 align="center">
ç®€å•ã€é«˜æ•ˆä¸”ä½æˆæœ¬çš„é¢„è®­ç»ƒã€å¾®è°ƒä¸æœåŠ¡ï¼Œæƒ åŠå¤§ä¼—
</h3>

<p align="center">
| <a href="./README.md"><b>English</b></a> | <a href="./README-CN.md"><b>ä¸­æ–‡</b></a> |
</p>

---

æœ€æ–°ä¿¡æ¯ğŸ”¥

- [2024/01] Release Byzer-LLM 0.1.39
- [2023/12] Release Byzer-LLM 0.1.30

---

Byzer-LLM åŸºäº Ray æŠ€æœ¯æ„å»ºï¼Œæ˜¯ä¸€æ¬¾è¦†ç›–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®Œæ•´ç”Ÿå‘½å‘¨æœŸçš„è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€å¾®è°ƒã€éƒ¨ç½²åŠæ¨ç†æœåŠ¡ç­‰é˜¶æ®µã€‚

Byzer-LLM çš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºï¼š

1. å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼šæ”¯æŒé¢„è®­ç»ƒã€å¾®è°ƒã€éƒ¨ç½²å’Œæ¨ç†æœåŠ¡å…¨æµç¨‹
2. å…¼å®¹ Python/SQL API æ¥å£
3. åŸºäº Ray æ¶æ„è®¾è®¡ï¼Œä¾¿äºè½»æ¾æ‰©å±•

---

* [ç‰ˆæœ¬è®°å½•](#ç‰ˆæœ¬è®°å½•) 
* [å®‰è£…æŒ‡å—](#å®‰è£…æŒ‡å—) 
* [å¿«é€Ÿå…¥é—¨](#å¿«é€Ÿå…¥é—¨) 
* [å¦‚ä½•è¿æ¥æ¥è‡ª Ray é›†ç¾¤å¤–éƒ¨çš„æ¨¡å‹](#å¦‚ä½•è¿æ¥æ¥è‡ª-Ray-é›†ç¾¤å¤–éƒ¨çš„æ¨¡å‹)
* åµŒå…¥/é‡æ’åº
    * [åµŒå…¥æ¨¡å‹](#åµŒå…¥æ¨¡å‹)
    * [åµŒå…¥é‡æ’åºæ¨¡å‹](#åµŒå…¥é‡æ’åºæ¨¡å‹)
* [é‡åŒ–](#é‡åŒ–) 
* [æ”¯æŒçš„æ¨¡å‹](#æ”¯æŒçš„æ¨¡å‹) 
* æœåŠ¡ç«¯
    * åç«¯
        * [æ”¯æŒ vLLM ](#æ”¯æŒ-vLLM) 
        * [æ”¯æŒ DeepSpeed](#æ”¯æŒ-DeepSpeed) 
    * [Byzer-LLM å…¼å®¹ OpenAI çš„ RESTful API æœåŠ¡](#å…¼å®¹-OpenAI-RESTful-API-æœåŠ¡)
* å¤§è¯­è¨€æ¨¡å‹ä¸ Python
    * [å‡½æ•°è°ƒç”¨](#å‡½æ•°è°ƒç”¨) 
    * [ä½¿ç”¨ pydantic ç±»å“åº”](#å“åº”æ—¶ä½¿ç”¨-pydantic-ç±») 
    * [å‡½æ•°å®ç°](#å‡½æ•°å®ç°åŠŸèƒ½) 
    * [å¯¹ LLM å‹å¥½çš„å‡½æ•°/æ•°æ®ç±»](#å¤§è¯­è¨€æ¨¡å‹å‹å¥½å‹å‡½æ•°æ•°æ®ç±») 
* æ¨¡å‹é…ç½®   
    * [æ¨¡å‹å…ƒä¿¡æ¯](#æ¨¡å‹å…ƒä¿¡æ¯) 
    * [èŠå¤©æ¨¡æ¿](#å¯¹è¯æ¨¡æ¿) 
    * [LLM é»˜è®¤å‚æ•°](#LLM-é»˜è®¤å‚æ•°) 
* [SaaS æ¨¡å‹](#SaaS-æ¨¡å‹) 
    * [é€šä¹‰åƒé—®](#é€šä¹‰åƒé—®qianwen) 
    * [ç™¾å·](#ç™¾å·baichuan)
    * [azure openai](#azure-openai)
    * [openai](#openai)
    * [æ™ºè°±](#æ™ºè°±zhipu)
    * [æ˜Ÿç«](#æ˜Ÿç«sparkdesk)         
* [å¤šæ¨¡æ€](#å¤šæ¨¡æ€) 
* [StableDiffusion](#StableDiffusion)
* [SQL æ”¯æŒ](#SQL-æ”¯æŒ) 
* [é¢„è®­ç»ƒ](#é¢„è®­ç»ƒ)
* [å¾®è°ƒ](#å¾®è°ƒ)
* [æ–‡ç« ](#æ–‡ç« )
* [è´¡çŒ®æŒ‡å—](#è´¡çŒ®æŒ‡å—)

---

## ç‰ˆæœ¬è®°å½•
- 0.1.39ï¼šæå‡å‡½æ•°åŠŸèƒ½å®ç° / æ›´æ–° SaaS å¼€å‘è€…å¥—ä»¶ï¼ˆSDKï¼‰ / é›†æˆ OpenAI å…¼å®¹ API æœåŠ¡
- 0.1.38ï¼šå‡çº§ saas/sparkdask æ¨¡å‹ç»„ä»¶ / å¼•å…¥åµŒå…¥å¼é‡æ’åºæ¨¡å‹ / å®ç°ä»£ç†æ¶ˆæ¯å­˜å‚¨æ”¯æŒ
- 0.1.37ï¼šå¯¹ saas/zhipu æ¨¡å‹è¿›è¡Œæ›´æ–°ï¼Œæ‚¨å¯ä»¥é€‰ç”¨ glm-4 æˆ– embedding-2 ç”¨äºå¤§è¯­è¨€æ¨¡å‹æˆ–è€…åµŒå…¥åº”ç”¨åœºæ™¯
- 0.1.36ï¼šä¿®æ­£ç”± Byzer-Agent æ›´æ–°æ‰€å¯¼è‡´çš„æ•°æ®åˆ†æä»£ç†æ¨¡å—çš„æ•…éšœ
- 0.1.35ï¼šæ–°å¢ç™¾å· SaaS åµŒå…¥å¼æ¨¡å‹
- 0.1.34ï¼šè¿›ä¸€æ­¥å¼ºåŒ– Byzer-Agent API åŠŸèƒ½å¹¶ä¿®å¤ Byzer-LLM å†…éƒ¨çš„éƒ¨åˆ†é—®é¢˜
- 0.1.33ï¼šè§£å†³å“åº”ç±»å†…éƒ¨é”™è¯¯ / æ–°å¢å¤šé¡¹å‡½æ•°å®ç°
- 0.1.32ï¼šå¯¹ StableDiffusion è¿›è¡Œæ€§èƒ½ä¼˜åŒ–
- 0.1.31ï¼šå¯ç”¨åŒ…å«ä»¤ç‰Œè®¡æ•°ä¿¡æ¯çš„å®æ—¶èŠå¤©åŠŸèƒ½ / å¯¹å¤šæ¨¡æ€æ¨¡å‹èŠå¤©ä½“éªŒè¿›è¡Œäº†ä¼˜åŒ–
- 0.1.30ï¼šåœ¨ vLLM åå°åº”ç”¨èŠå¤©æ¨¡æ¿åŠŸèƒ½
- 0.1.29ï¼šæå‡äº† DataAnalysis ä»£ç†çš„åŠŸèƒ½è¡¨ç°
- 00.1.28ï¼šä¿®å¤è‹¥å¹²å·²çŸ¥ bug
- 0.1.27ï¼šä¿®å¤è‹¥å¹²å·²çŸ¥ bug
- 0.1.26ï¼šæ”¯æŒ QianWen SaaS å¹³å° / å®ç°å®æ—¶èŠå¤©åŠŸèƒ½åœ¨ QianWenSaas ä¸­çš„åº”ç”¨ / è§£å†³éƒ¨åˆ† SaaS æ¨¡å‹å­˜åœ¨çš„é—®é¢˜
- 0.1.24ï¼šæ”¯æŒä»æ¨¡å‹å®ä¾‹ç›´æ¥æå–å…ƒæ•°æ®å¹¶è‡ªåŠ¨é…ç½®æ¨¡æ¿
- 0.1.23ï¼šé€šè¿‡ Python API è¿›è¡Œæ¨¡å‹å¾®è°ƒ / è§£å†³äº†ä¸€äº›ç°æœ‰é—®é¢˜
- 0.1.22ï¼šå¢æ·»äº†å‡½æ•°è°ƒç”¨æ”¯æŒ / å“åº”ç»“æ„é‡‡ç”¨ pydantic ç±»å‹å®šä¹‰
- 0.1.19ï¼šä¿®å¤äº†åµŒå…¥ç›¸å…³é—®é¢˜
- 0.1.18ï¼šå®ç°äº†æµå¼èŠå¤©åŠŸèƒ½ / åŠ å…¥äº†æ¨¡å‹æ¨¡æ¿æ”¯æŒ
- 0.1.17ï¼šæ­¤ç‰ˆæœ¬æœªæœ‰å®è´¨æ€§æ›´æ–°å†…å®¹
- 0.1.16ï¼šå¢å¼ºäº†é’ˆå¯¹ byzer-retrieval çš„ API åŠŸèƒ½
- 0.1.14ï¼šä¸º byzer-retrieval æ·»åŠ äº†è·å–è¡¨æ ¼(get_tables)å’Œæ•°æ®åº“(get_databases)çš„ API æ¥å£
- 0.1.13ï¼šæ”¯æŒ byzer-retrieval èƒ½å¤Ÿå…³é—­é›†ç¾¤æ“ä½œ
- 0.1.12ï¼šåˆæ­¥æ”¯æŒ Python APIï¼ˆå°šå¤„äº alpha æµ‹è¯•é˜¶æ®µï¼‰
- 0.1.5ï¼šæ”¯æŒ Python å°è£…å½¢å¼çš„ [byzer-retrieval](https://github.com/allwefantasy/byzer-retrieval)

---


## å®‰è£…æŒ‡å—

æ¨èé…ç½®ç¯å¢ƒ:

1. Conda:  python==3.10.11  
2. OS:     ubuntu 22.04
3. Cuda:   12.1.0 (å¯é€‰ï¼Œä»…åœ¨æ‚¨ä½¿ç”¨SaaSæ¨¡å‹æ—¶ä½¿ç”¨)

```bash
## Make sure you python version is 3.10.11
pip install -r requirements.txt
## Skip this step if you have no Nvidia GPU
pip install vllm==0.2.6
pip install -U byzerllm
ray start --head
```

è‹¥ä½ çš„ CUDA ç‰ˆæœ¬ä¸º 11.8ï¼Œè¯·å‚ç…§ä»¥ä¸‹é“¾æ¥æ¥å®‰è£… vLLMï¼š
https://docs.vllm.ai/en/latest/getting_started/installation.html

å®‰è£…è¿‡ç¨‹ä¸­éœ€å…³æ³¨çš„å…³é”®ç¯èŠ‚å¦‚ä¸‹ï¼š


```shell
As of now, vLLMâ€™s binaries are compiled on CUDA 12.1 by default. However, you can install vLLM with CUDA 11.8 by running:

# Install vLLM with CUDA 11.8.
export VLLM_VERSION=0.2.6
export PYTHON_VERSION=310
pip install https://github.com/vllm-project/vllm/releases/download/v${VLLM_VERSION}/vllm-${VLLM_VERSION}+cu118-cp${PYTHON_VERSION}-cp${PYTHON_VERSION}-manylinux1_x86_64.whl

# Re-install PyTorch with CUDA 11.8.
pip uninstall torch -y
pip install torch --upgrade --index-url https://download.pytorch.org/whl/cu118

# Re-install xFormers with CUDA 11.8.
pip uninstall xformers -y
pip install --upgrade xformers --index-url https://download.pytorch.org/whl/cu118
```

### åŸå§‹æœºå™¨é…ç½®æŒ‡å—

> æœ¬æ–¹æ¡ˆå·²é’ˆå¯¹ Ubuntu 20.04/22.04 ç‰ˆæœ¬å’Œ CentOS 8.0 æ“ä½œç³»ç»Ÿå®Œæˆæµ‹è¯•

è‹¥æ‚¨æ‰‹å¤´çš„è®¡ç®—æœºå°šå¤„äºåˆå§‹çŠ¶æ€ï¼Œå³æœªå®‰è£… GPU é©±åŠ¨å’Œ CUDA ç¯å¢ƒï¼Œå¯æŒ‰ä»¥ä¸‹æä¾›çš„è„šæœ¬æ­¥éª¤è½»æ¾å®Œæˆæœºå™¨é…ç½®ï¼š

```shell
git clone https://gitee.com/allwefantasy/byzer-llm
cd byzer-llm/setup-machine
```

æ¥ä¸‹æ¥ï¼Œè¯·åˆ‡æ¢è‡³ **ROOT**ï¼Œå¹¶æ‰§è¡Œä»¥ä¸‹å‡†å¤‡å¥½çš„è‡ªåŠ¨åŒ–é…ç½®è„šæœ¬ï¼š

```shell
ROLE=master ./setup-machine.sh
```
ç´§æ¥ç€ï¼Œç³»ç»Ÿå°†ä¸ºæ‚¨æ–°å»ºä¸€ä¸ªåä¸º `byzerllm` çš„ç”¨æˆ·è´¦æˆ·ã€‚

éšåï¼Œè¯·åˆ‡æ¢è‡³è¿™ä¸ªæ–°å»ºçš„ `byzerllm` ç”¨æˆ·èº«ä»½ï¼Œå¹¶æ‰§è¡Œä»¥ä¸‹é…ç½®è„šæœ¬ï¼š

```shell
ROLE=master ./setup-machine.sh
```

è„šæœ¬ä¼šè‡ªåŠ¨ä¸ºæ‚¨å®‰è£…ä»¥ä¸‹å„é¡¹è½¯ä»¶ï¼š

1. 
2. Conda
3. Nvidia Driver 535
4. Cuda 12.1.0
5. Ray 
6. requirements.txt æ–‡ä»¶å†…æ‰€åˆ—ä¸¾çš„æ‰€æœ‰ Python ç¬¬ä¸‰æ–¹åº“
7. Byzer-SQL/Byzer-Notebook å¤§æ•°æ®å¤„ç†ä¸åˆ†æå·¥å…·

è‹¥æ‚¨éœ€è¦å‘ Ray é›†ç¾¤æ‰©å±•æ›´å¤šå·¥ä½œèŠ‚ç‚¹ï¼Œåªéœ€åœ¨æ–°å¢çš„å·¥ä½œèŠ‚ç‚¹ä¸Šé‡å¤ä»¥ä¸Šå®‰è£…æ­¥éª¤ã€‚
è¯·æ³¨æ„ï¼Œåœ¨å·¥ä½œèŠ‚ç‚¹ä¸Šï¼Œ`ROLE` åº”ä¸º `worker`ã€‚

```shell
ROLE=worker ./setup-machine.sh
```

---

## å¿«é€Ÿå…¥é—¨

```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend

ray.init(address="auto",namespace="default",ignore_reinit_error=True)

llm = ByzerLLM()

llm.setup_gpus_per_worker(4).setup_num_workers(1)
llm.setup_infer_backend(InferBackend.transformers)

llm.deploy(model_path="/home/byzerllm/models/openbuddy-llama2-13b64k-v15",
           pretrained_model_type="custom/llama2",
           udf_name="llama2_chat",infer_params={})



llm_client = ByzerLLM()
llm_client.setup_template("llama2_chat","auto")

v = llm.chat_oai(model="llama2_chat",conversations=[{
    "role":"user",
    "content":"hello",
}])

print(v[0].output)
```

ä¸Šè¿°ä»£ç å°†ä¼šåŠ è½½å¹¶éƒ¨ç½²ä¸€ä¸ªåä¸º llama2 çš„æ¨¡å‹ï¼Œç„¶ååˆ©ç”¨è¯¥æ¨¡å‹å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œæ¨ç†åˆ†æã€‚å¦‚æœä½ é€‰æ‹© transformers ä½œä¸ºæ¨ç†åç«¯å¼•æ“ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯éœ€è¦æ‰‹åŠ¨è®¾å®š `pretrained_model_type` å‚æ•°ï¼Œå› ä¸º transformers æœ¬èº«ä¸å…·å¤‡è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹çš„æœºèƒ½ã€‚

Byzer-LLM åŒæ ·æ”¯æŒä»¥ç›¸åŒæ–¹å¼è°ƒç”¨å¹¶éƒ¨ç½²äº‘ç«¯ï¼ˆSaaSï¼‰æ¨¡å‹ã€‚è¿™ä¸€åŠŸèƒ½ä¸ºå¼€æºæ¨¡å‹å’Œäº‘æœåŠ¡æ¨¡å‹æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ“ä½œç•Œé¢ã€‚æ¥ä¸‹æ¥çš„ç¤ºä¾‹ä»£ç å°†å±•ç¤ºå¦‚ä½•éƒ¨ç½²æ¥è‡ª Azure OpenAI çš„æ¨¡å‹ï¼Œå¹¶åœ¨å…¶ååˆ©ç”¨è¿™ä¸ªæ¨¡å‹å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œæ¨ç†å¤„ç†ã€‚


```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend
ray.init(address="auto",namespace="default",ignore_reinit_error=True)

llm = ByzerLLM()

llm.setup_gpus_per_worker(0).setup_num_workers(10)
llm.setup_infer_backend(InferBackend.transformers)

llm.deploy(pretrained_model_type="saas/azure_openai",
           udf_name="azure_openai",
           infer_params={
            "saas.api_type":"azure",
            "saas.api_key"="xxx"
            "saas.api_base"="xxx"
            "saas.api_version"="2023-07-01-preview"
            "saas.deployment_id"="xxxxxx"
           })


llm_client = ByzerLLM()
llm_client.setup_template("azure_openai","auto")

v = llm.chat_oai(model="azure_openai",conversations=[{
    "role":"user",
    "content":"hello",
}])

print(v[0].output)
```

è¯·æ³¨æ„ï¼Œé‰´äº SaaS æ¨¡å‹æ— éœ€ä¾èµ– GPUï¼Œæˆ‘ä»¬æŠŠ `setup_gpus_per_worker` å‚æ•°è®¾ä¸º 0ã€‚å¦å¤–ï¼Œä½ å¯ä»¥å€ŸåŠ© `setup_num_workers` å‚æ•°æ¥è°ƒæ•´æœ€å¤§å¹¶å‘æ‰§è¡Œæ•°ï¼Œç„¶è€Œè¦æ³¨æ„çš„æ˜¯ï¼ŒSaaS æ¨¡å‹è‡ªå¸¦å…¶å¹¶å‘è¯·æ±‚çš„ä¸Šé™ï¼Œå› æ­¤ `setup_num_workers` å‚æ•°æ‰€æ§åˆ¶çš„æ˜¯ Byzer-LLM æ¥å—çš„æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°ï¼Œè€Œéç»å¯¹çš„å¹¶å‘æ‰§è¡Œä¸Šé™ï¼Œå®é™…å¹¶å‘æ‰§è¡Œæ•°ä»éœ€å‚ç…§ SaaS æ¨¡å‹è‡ªèº«çš„å¹¶å‘é™åˆ¶ã€‚

## å¦‚ä½•è¿æ¥æ¥è‡ª Ray é›†ç¾¤å¤–éƒ¨çš„æ¨¡å‹

å»ºè®®çš„æœ€ä½³å®è·µæ˜¯åœ¨æ‚¨çš„ç›®æ ‡è®¾å¤‡ï¼ˆä¾‹å¦‚ Web æœåŠ¡å™¨ï¼‰ä¸Šå¯åŠ¨ä¸€ä¸ªé—²ç½®çš„ Ray å·¥ä½œèŠ‚ç‚¹ï¼š

```shell
ray start --address="xxxxx:6379"  --num-gpus=0 --num-cpus=0 
```

è¿™æ ·ä¸€æ¥ï¼Œæ‚¨ä¾¿å¯ä»¥ä» Ray é›†ç¾¤å¤–éƒ¨é¡ºåˆ©å¯¹æ¥æ‰€éœ€æ¨¡å‹ï¼š

```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend

## connect the ray cluster by the empty worker we started before
## this code should be run once in your prorgram
ray.init(address="auto",namespace="default",ignore_reinit_error=True)

## new a ByzerLLM instance

llm_client = ByzerLLM()
llm_client.setup_template("llama2_chat","auto")

v = llm.chat_oai(model="llama2_chat",conversations=[{
    "role":"user",
    "content":"hello",
}])

print(v[0].output)
```


## åµŒå…¥æ¨¡å‹

ä»¥ä¸‹å±•ç¤ºçš„ä»£ç ç‰‡æ®µæ˜¯ä¸€ä¸ªå…³äºéƒ¨ç½² BGE åµŒå…¥æ¨¡å‹çš„å®é™…æ¡ˆä¾‹

```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend
ray.init(address="auto",namespace="default",ignore_reinit_error=True)
llm = ByzerLLM()

llm.setup_gpus_per_worker(0.4).setup_num_workers(2).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path="/home/byzerllm/models/bge-large-zh",
    pretrained_model_type="custom/bge",
    udf_name="emb",
    infer_params={}
)   
```

è¿™æ ·ï¼Œæ‚¨å°±èƒ½å¤Ÿå°†ä»»æ„ä¸€æ®µæ–‡æœ¬æˆåŠŸè½¬åŒ–ä¸ºå‘é‡è¡¨ç¤ºï¼š

```python
t = llm.emb("emb",LLMRequest(instruction="wow"))
t[0].output
#output: [-0.005588463973253965,
 -0.01747054047882557,
 -0.040633779019117355,
...
 -0.010880181565880775,
 -0.01713103987276554,
 0.017675869166851044,
 -0.010260719805955887,
 ...]
```

Byzer-LLM è¿˜æ”¯æŒäº‘ç«¯ï¼ˆSaaSï¼‰åµŒå…¥æ¨¡å‹æœåŠ¡ã€‚ä¸‹é¢è¿™æ®µä»£ç æ¼”ç¤ºäº†å¦‚ä½•éƒ¨ç½²ä¸€ä¸ªç™¾å·æä¾›çš„åµŒå…¥æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨è¯¥æ¨¡å‹å¯¹è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–å¤„ç†ã€‚

```python
import os
os.environ["RAY_DEDUP_LOGS"] = "0" 

import ray
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.utils.client import ByzerLLM,LLMRequest,LLMResponse,LLMHistoryItem,InferBackend
from byzerllm.utils.client import Templates

ray.init(address="auto",namespace="default",ignore_reinit_error=True)  

llm = ByzerLLM(verbose=True)

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "baichuan_emb"
if llm.is_model_exist(chat_name):
    llm.undeploy(udf_name=chat_name)

llm.deploy(model_path="",
           pretrained_model_type="saas/baichuan",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"",            
            "saas.model":"Baichuan-Text-Embedding"
           })
llm.setup_default_emb_model_name(chat_name)

v = llm.emb(None,LLMRequest(instruction="ä½ å¥½"))
print(v.output)
```

## åµŒå…¥é‡æ’åºæ¨¡å‹

è‹¥æ‚¨æ‰“ç®—åˆ©ç”¨åµŒå…¥é‡æ’åºæ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹å…·ä½“åº”ç”¨ç¤ºä¾‹ã€‚

```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend
ray.init(address="auto",namespace="default",ignore_reinit_error=True)
llm = ByzerLLM()

llm.setup_gpus_per_worker(0.4).setup_num_workers(2).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path="/Users/wanghan/data/bge-reranker-base",
    pretrained_model_type="custom/bge_rerank",
    udf_name="emb_rerank",
    infer_params={}
)   
llm.setup_default_emb_model_name("emb_rerank")
```
æ¥ä¸‹æ¥ï¼Œæ‚¨å¯ä»¥é€šè¿‡å°†æŸ¥è¯¢æ–‡æœ¬å’Œå¾…è¯„ä¼°æ–‡æœ¬é€å…¥é‡æ’åºæ¨¡å‹ï¼Œå¾—åˆ°å®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§å¾—åˆ†ã€‚

```python
sentence_pairs_01 = ['query', 'passage']
t1 = llm.emb_rerank(sentence_pairs=sentence_pairs_01)
print(t1[0].output)
#output [['query', 'passage'], 0.4474925994873047]

sentence_pairs_02 = [['what is panda?', 'hi'], ['what is panda?','The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]
t2 = llm.emb_rerank(sentence_pairs=sentence_pairs_02)
print(t2[0].output)
#output [[['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'], 6.1821160316467285], [['what is panda?', 'hi'], -8.154398918151855]]
```

## é‡åŒ–

å½“åç«¯é‡‡ç”¨ `InferBackend.transformers` æ—¶ï¼Œè¿™é‡Œå±•ç¤ºçš„æ˜¯ä¸€ä¸ªå…³äºâ€œç™¾å·2â€æ¨¡å‹çš„å®ä¾‹åº”ç”¨ã€‚

```python
llm.setup_gpus_per_worker(2).setup_num_workers(1).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/baichuan2",
    udf_name="baichuan2_13_chat",
    infer_params={"quatization":"4"}
)
```
ç›®å‰æ”¯æŒçš„ `quantization`ï¼ˆé‡åŒ–ï¼‰é€‰é¡¹åŒ…æ‹¬ï¼š

1. 4
2. 8
3. true/false

è‹¥å°†è¯¥å‚æ•°è®¾ä¸º trueï¼Œç³»ç»Ÿå°†é‡‡ç”¨ int4 é‡åŒ–çº§åˆ«ã€‚

é’ˆå¯¹åç«¯ä¸º `InferBackend.VLLM` çš„æƒ…å†µï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨â€œæ˜“â€æ¨¡å‹çš„ç¤ºä¾‹ï¼š

è‹¥éœ€è¦éƒ¨ç½²ç»è¿‡é‡åŒ–å‹ç¼©çš„æ¨¡å‹ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹ä»£ç æ ·å¼è®¾ç½® `infer_params` å‚æ•°ï¼š

```python
llm.setup_gpus_per_worker(1).setup_num_workers(1).setup_infer_backend(InferBackend.VLLM)
llm.deploy(
    model_path="/home/winubuntu/models/Yi-6B-Chat-4bits",
    pretrained_model_type="custom/auto",
    udf_name="chat",
    infer_params={"backend.quantization":"AWQ"}
)
```

`backend.quantization` å‚æ•°å¯ä»¥é€‰ç”¨ GPTQ æˆ– AWQ ä¸¤ç§é‡åŒ–æ–¹æ³•ã€‚


## æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨

æ”¯æŒçš„å¼€æº `pretrained_model_type` åŒ…æ‹¬ï¼š

1. custom/llama2
2. bark	
3. whisper	
3. chatglm6b
4. custom/chatglm2
5. moss
6. custom/alpha_moss
7. dolly
8. falcon
9. llama
10. custom/starcode
11. custom/visualglm
12. custom/m3e
13. custom/baichuan
14. custom/bge
15. custom/qwen_vl_chat
16. custom/stable_diffusion
17. custom/zephyr

æ”¯æŒçš„ SaaS `pretrained_model_type` å¦‚ä¸‹ï¼š

1. saas/chatglm	Chatglm130B
2. saas/sparkdesk	æ˜Ÿç«å¤§æ¨¡å‹
3. saas/baichuan	ç™¾å·å¤§æ¨¡å‹
4. saas/zhipu	æ™ºè°±å¤§æ¨¡å‹
5. saas/minimax	MiniMax å¤§æ¨¡å‹
6. saas/qianfan	æ–‡å¿ƒä¸€è¨€
7. saas/azure_openai	
8. saas/openai

è¯·æ³¨æ„ï¼Œæºè‡ª lama/llama2/starcode çš„è¡ç”Ÿæ¨¡å‹ä¹ŸåŒæ ·å—åˆ°æ”¯æŒã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `llama` åŠ è½½ vicuna æ¨¡å‹ã€‚

## æ”¯æŒ vLLM

Byzer-LLM åŒæ ·å…·å¤‡æ”¯æŒå°† vLLM ä½œä¸ºæ¨ç†åç«¯çš„èƒ½åŠ›ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥ä¾æ®ä»¥ä¸‹ä»£ç èŒƒä¾‹ï¼Œéƒ¨ç½²ä¸€ä¸ª vLLMï¼ˆè™šæ‹Ÿå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼‰ï¼Œå¹¶å€Ÿæ­¤æ¨¡å‹å¯¹ç»™å®šæ–‡æœ¬è¿›è¡Œæ™ºèƒ½æ¨ç†å¤„ç†ã€‚

```python
import ray
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend

ray.init(address="auto",namespace="default",ignore_reinit_error=True)
llm = ByzerLLM()

llm.setup_gpus_per_worker(2)
llm.setup_num_workers(1)
llm.setup_infer_backend(InferBackend.VLLM)

llm.deploy(
    model_path="/home/byzerllm/models/openbuddy-zephyr-7b-v14.1",
    pretrained_model_type="custom/auto",
    udf_name="zephyr_chat"",
    infer_params={}
)

v = llm.chat_oai(model="zephyr_chat",conversations=[{
    "role":"user",
    "content":"hello",
}])
print(v[0].output)
```

vLLM ä¸ transformers åç«¯åœ¨ä½¿ç”¨ä¸Šæœ‰ä¸€äº›å¾®å°çš„ä¸åŒç‚¹ï¼š

1. åœ¨ vLLM ä¸­ï¼Œ`pretrained_model_type` å‚æ•°å›ºå®šä¸º `custom/auto`ï¼Œè¿™æ˜¯å› ä¸º vLLM è‡ªå¸¦æ¨¡å‹ç±»å‹è‡ªåŠ¨æ£€æµ‹åŠŸèƒ½ã€‚
2. è‹¥è¦æŒ‡å®šæ¨ç†åç«¯ä¸º vLLMï¼Œè¯·å°† `setup_infer_backend` å‚æ•°è®¾ç½®ä¸º `InferBackend.VLLM`ã€‚
 

### æµå¼å¯¹è¯

è‹¥æ¨¡å‹é‡‡ç”¨äº† vLLM åç«¯è¿›è¡Œéƒ¨ç½²ï¼Œå®ƒè¿˜å°†æ”¯æŒâ€œæµå¼å¯¹è¯â€ç‰¹æ€§ï¼š

è°ƒç”¨ `stream_chat_oai` æ–¹æ³•å¯ä»¥è·å¾—ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œè¿›è€Œé€æ¡æ‹‰å–æ¨¡å‹ç”Ÿæˆçš„å›å¤æ–‡æœ¬ã€‚

```python

llm.setup_default_model_name(chat_model_name) 

t = llm.stream_chat_oai(conversations=[{
    "role":"user",
    "content":"Hello, how are you?"
}])

for line in t:
   print(line+"\n")
```

## æ”¯æŒ DeepSpeed

Byzer-LLM è¿˜æ”¯æŒå°† DeepSpeed ä½œä¸ºæ¨¡å‹æ¨ç†çš„åç«¯æŠ€æœ¯ã€‚ä»¥ä¸‹ä»£ç ç‰‡æ®µå°†å±•ç¤ºå¦‚ä½•éƒ¨ç½² DeepSpeed ä¼˜åŒ–çš„æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨è¯¥æ¨¡å‹å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œæ¨ç†åˆ†æï¼š

```python
import ray
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend

ray.init(address="auto",namespace="default",ignore_reinit_error=True)
llm = ByzerLLM()

llm.setup_gpus_per_worker(4)
llm.setup_num_workers(1)
llm.setup_infer_backend(InferBackend.DeepSpeed)

llm.deploy(
    model_path="/home/byzerllm/models/openbuddy-llama-13b-v5-fp16",
    pretrained_model_type="custom/auto",
    udf_name="llama_chat"",
    infer_params={}
)

llm.chat("llama_chat",LLMRequest(instruction="hello world"))[0].output
```

ä¸Šè¿°ä»£ç ä¸ç”¨äº vLLM çš„ä»£ç åŸºæœ¬ä¸€è‡´ï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äº `InferBackend` è®¾ç½®æˆäº† `InferBackend.DeepSpeed`ã€‚

## å…¼å®¹ OpenAI RESTful API æœåŠ¡

é€šè¿‡æ‰§è¡Œä¸‹åˆ—ä»£ç ç‰‡æ®µï¼Œå³å¯å¯åŠ¨ä¸€ä¸ªèƒ½å¤Ÿä¸ OpenAI å¯¹æ¥çš„ ByzerLLm å¤§è¯­è¨€æ¨¡å‹ RESTful API æœåŠ¡å™¨ï¼š

```shell
ray start --address="xxxxx:6379"  --num-gpus=0 --num-cpus=0 
python -m byzerllm.utils.client.entrypoints.openai.api_server
```

é»˜è®¤æƒ…å†µä¸‹ï¼ŒæœåŠ¡å™¨è¿è¡Œæ—¶ä¼šåœ¨`8000`ç«¯å£ç­‰å¾…è¯·æ±‚ã€‚æ‚¨å¯ä»¥é‡‡ç”¨å¦‚ä¸‹ä»£ç ç‰‡æ®µæ¥éªŒè¯å¹¶æµ‹è¯•è¯¥ API åŠŸèƒ½ï¼š

```python
from openai import OpenAI
client = OpenAI(
    base_url="http://127.0.0.1:8000/v1",
    api_key="xxxx"
)

chat_completion = client.chat.completions.create(    
    model="wenxin_chat",
    messages=[{"role": "user", "content": "å†™ä¸€ä¸ªæ’åºç®—æ³•"}],
    stream=False
)

print(chat_completion.choices[0].message.content)
```

## æµå¼å¯¹è¯

```python

from openai import OpenAI
client = OpenAI(
    base_url="http://127.0.0.1:8000/v1",
    api_key="simple"
)

chat_completion = client.chat.completions.create(    
    model="wenxin_chat",
    messages=[{"role": "user", "content": "å†™ä¸€ä¸ªæ’åºç®—æ³•"}],
    stream=True
)

for chunk in chat_completion:    
    print(chunk.choices[0].delta.content or "", end="")
```

## å‡½æ•°è°ƒç”¨

è¿™æœ‰ä¸€ä¸ªåˆ©ç”¨ QWen 72B æ¨¡å‹è¿›è¡Œå‡½æ•°è°ƒç”¨çš„åŸºç¡€ç¤ºä¾‹ã€‚

éƒ¨ç½²æ¨¡å‹çš„æ­¥éª¤æ¼”ç¤ºï¼š

```python
import ray
ray.init(address="auto",namespace="default") 
llm = ByzerLLM()

model_location="/home/byzerllm/models/Qwen-72B-Chat"

llm.setup_gpus_per_worker(8).setup_num_workers(1).setup_infer_backend(InferBackend.VLLM)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/auto",
    udf_name=chat_model_name,
    infer_params={}
)

llm.setup_default_model_name("chat")
# from 0.1.24 
# llm.setup_auto("chat")
meta = llm.get_meta()
llm.setup_max_model_length("chat",meta.get("max_model_len",32000))
lm.setup_template("chat",Templates.qwen()) 
```

è®©æˆ‘ä»¬ä¸€èµ·å°è¯•ç¼–å†™å‡ ä¸ªPythonå‡½æ•°ï¼Œå…ˆæ¥ä½“éªŒå¦‚ä½•ä½¿ç”¨QWen 72Bæ¨¡å‹ç”Ÿæˆå›å¤ï¼š

```python

from typing import List,Dict,Any,Annotated
import pydantic 
import datetime
from dateutil.relativedelta import relativedelta

def compute_date_range(count:Annotated[int,"æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»å‹"],
                       unit:Annotated[str,"æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»å‹",{"enum":["day","week","month","year"]}])->List[str]:
    '''
    è®¡ç®—æ—¥æœŸèŒƒå›´

    Args:
        count: æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»å‹
        unit: æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»å‹ï¼Œå¯é€‰å€¼ä¸º day,week,month,year
    '''        
    now = datetime.datetime.now()
    now_str = now.strftime("%Y-%m-%d %H:%M:%S")
    if unit == "day":
        return [(now - relativedelta(days=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    elif unit == "week":
        return [(now - relativedelta(weeks=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    elif unit == "month":
        return [(now - relativedelta(months=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    elif unit == "year":
        return [(now - relativedelta(years=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    return ["",""]

def compute_now()->str:
    '''
    è®¡ç®—å½“å‰æ—¶é—´
    '''
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
```

è¿™é‡Œæˆ‘ä»¬ç»™å‡ºäº†ä¸¤ä¸ªä¾¿æ·çš„å‡½æ•°ï¼š

1. compute_date_rangeï¼šæ ¹æ®ç”¨æˆ·æŒ‡å®šçš„æ•°é‡ï¼ˆå¦‚å¤©æ•°ã€å‘¨æ•°ç­‰ï¼‰å’Œå•ä½æ¥è®¡ç®—ä¸€ä¸ªèµ·æ­¢æ—¥æœŸçš„åŒºé—´ã€‚
2. compute_nowï¼šè·å–å½“å‰çš„æ—¥æœŸä¿¡æ¯ã€‚

å½“é¢å¯¹ç”¨æˆ·çš„å…·ä½“é—®é¢˜æ—¶ï¼Œæˆ‘ä»¬ä¼šåˆ©ç”¨æ¨¡å‹è°ƒç”¨è¿™ä¸¤ä¸ªåŠŸèƒ½å·¥å…·ã€‚

```python
t = llm.chat_oai([{
    "content":'''è®¡ç®—å½“å‰æ—¶é—´''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

t[0].values

## output: ['2023-12-18 17:30:49']
```

```python
t = llm.chat_oai([{
    "content":'''æœ€è¿‘ä¸‰ä¸ªæœˆè¶‹åŠ¿''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

t[0].values

## output: [['2023-09-18 17:31:21', '2023-12-18 17:31:21']]
```

```python
t = llm.chat_oai([{
    "content":'''æœ€è¿‘ä¸‰å¤©''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

t[0].values

## output: [['2023-12-15 17:23:38', '2023-12-18 17:23:38']]
```

```python
t = llm.chat_oai([{
    "content":'''ä½ åƒé¥­äº†ä¹ˆï¼Ÿ''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

if t[0].values:
    print(t[0].values[0])
else:
    print(t[0].response.output)   

## output: 'æ‚¨å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½è¯­è¨€æ¨¡å‹ï¼Œæš‚æ—¶æ— æ³•åƒé¥­ã€‚'
```

æ‚¨å¯ä»¥æ£€æŸ¥ `from byzerllm.utils import function_calling_format` ä¸­çš„é»˜è®¤æç¤ºæ¨¡æ¿å‡½æ•°ã€‚å¦‚æœæ¨¡å‹ä½¿ç”¨é»˜è®¤å‡½æ•°æ•ˆæœä¸ä½³ï¼Œæ‚¨å¯ä»¥è®¾ç½®è‡ªå®šä¹‰å‡½æ•°ï¼š

```python
def custom_function_calling_format(prompt:str,tools:List[Callable],tool_choice:Callable)->str:
.....


llm.setup_function_calling_format_func("chat",custom_function_calling_format)
```

## å“åº”æ—¶ä½¿ç”¨ Pydantic ç±»

åœ¨ä¸å¤§è¯­è¨€æ¨¡å‹äº¤è°ˆæ—¶ï¼Œä½ å¯ä»¥è‡ªå®šä¹‰è®¾ç½®ä¸€ä¸ªç±»ä¼¼â€œå“åº”ç±»â€ï¼ˆResponse Classï¼‰çš„ç»“æ„ï¼Œä»¥æ­¤æ¥è§„èŒƒå’Œæ§åˆ¶æ¨¡å‹ç»™å‡ºå›ç­”çš„æ•°æ®æ ¼å¼å’Œç»“æ„

```python
import pydantic 

class Story(pydantic.BaseModel):
    '''
    æ•…äº‹
    '''

    title: str = pydantic.Field(description="æ•…äº‹çš„æ ‡é¢˜")
    body: str = pydantic.Field(description="æ•…äº‹ä¸»ä½“")



t = llm.chat_oai([
{
    "content":f'''è¯·ç»™æˆ‘è®²ä¸ªæ•…äº‹ï¼Œåˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ ‡é¢˜ï¼Œä¸€ä¸ªæ•…äº‹ä¸»ä½“''',
    "role":"user"
},
],response_class=Story)

t[0].value

## output: Story(title='å‹‡æ•¢çš„å°å…”å­', body='åœ¨ä¸€ä¸ªç¾ä¸½çš„æ£®æ—é‡Œï¼Œä½ç€ä¸€åªå¯çˆ±çš„å°å…”å­ã€‚å°å…”å­éå¸¸å‹‡æ•¢ï¼Œæœ‰ä¸€å¤©ï¼Œæ£®æ—é‡Œçš„åŠ¨ç‰©ä»¬éƒ½è¢«å¤§ç°ç‹¼å“åäº†ã€‚åªæœ‰å°å…”å­ç«™å‡ºæ¥ï¼Œç”¨æ™ºæ…§å’Œå‹‡æ°”æ‰“è´¥äº†å¤§ç°ç‹¼ï¼Œä¿æŠ¤äº†æ‰€æœ‰çš„åŠ¨ç‰©ã€‚ä»æ­¤ï¼Œå°å…”å­æˆä¸ºäº†æ£®æ—é‡Œçš„è‹±é›„ã€‚')
```

ä¸Šè¿°ä»£ç ä¼šè®© LLM ç›´æ¥ç”Ÿæˆ Story ç±»çš„å¯¹è±¡ã€‚ä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ› LLM å…ˆç”Ÿæˆæ–‡æœ¬ï¼Œå†ä»æ–‡æœ¬ä¸­æå–ç»“æ„ä¿¡æ¯ï¼Œè¿™æ—¶å¯ä»¥é€šè¿‡è®¾ç½® `response_after_chat=True` æ¥å¯ç”¨è¿™ä¸€è¡Œä¸ºã€‚ä¸è¿‡ï¼Œè¯·æ³¨æ„ï¼Œè¿™æ ·åšä¼šå¯¼è‡´ä¸€å®šçš„æ€§èƒ½æŸè€—ï¼ˆé¢å¤–çš„æ¨ç†è®¡ç®—ï¼‰ã€‚

```python
t = llm.chat_oai([
{
    "content":f'''è¯·ç»™æˆ‘è®²ä¸ªæ•…äº‹ï¼Œåˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ ‡é¢˜ï¼Œä¸€ä¸ªæ•…äº‹ä¸»ä½“''',
    "role":"user"
},
],response_class=Story,response_after_chat=True)

t[0].value
## output: Story(title='æœˆå…‰ä¸‹çš„å®ˆæŠ¤è€…', body='åœ¨ä¸€ä¸ªé¥è¿œçš„å¤è€æ‘åº„é‡Œï¼Œä½ç€ä¸€ä½åå«é˜¿æ˜çš„å¹´è½»äººã€‚é˜¿æ˜æ˜¯ä¸ªå­¤å„¿ï¼Œä»å°åœ¨æ‘é‡Œé•¿å¤§ï¼Œä»¥ç§ç”°ä¸ºç”Ÿã€‚ä»–å–„è‰¯ã€å‹¤åŠ³ï¼Œæ·±å—æ‘æ°‘ä»¬å–œçˆ±ã€‚\n\næ‘å­é‡Œæœ‰ä¸ªä¼ è¯´ï¼Œæ¯å½“æ»¡æœˆæ—¶åˆ†ï¼Œæœˆäº®å¥³ç¥ä¼šåœ¨æ‘å­åå±±çš„å¤æ ‘ä¸‹å‡ºç°ï¼Œèµç¦ç»™é‚£äº›å–„è‰¯çš„äººä»¬ã€‚ç„¶è€Œï¼Œåªæœ‰æœ€çº¯æ´çš„å¿ƒæ‰èƒ½çœ‹åˆ°å¥¹ã€‚å› æ­¤ï¼Œæ¯å¹´çš„è¿™ä¸ªæ—¶å€™ï¼Œé˜¿æ˜éƒ½ä¼šç‹¬è‡ªä¸€äººå‰å¾€åå±±ï¼Œå¸Œæœ›èƒ½å¾—åˆ°å¥³ç¥çš„ç¥ç¦ã€‚\n\nè¿™ä¸€å¹´ï¼Œæ‘å­é­å—äº†ä¸¥é‡çš„æ—±ç¾ï¼Œåº„ç¨¼æ¯é»„ï¼Œäººä»¬ç”Ÿæ´»å›°è‹¦ã€‚é˜¿æ˜å†³å®šå‘æœˆäº®å¥³ç¥ç¥ˆæ±‚é™é›¨ï¼Œæ‹¯æ•‘æ‘å­ã€‚ä»–åœ¨æœˆå…‰ä¸‹è™”è¯šåœ°ç¥ˆç¥·ï¼Œå¸Œæœ›å¥³ç¥èƒ½å¬åˆ°ä»–çš„å‘¼å”¤ã€‚\n\nå°±åœ¨è¿™ä¸ªæ—¶åˆ»ï¼Œæœˆäº®å¥³ç¥å‡ºç°äº†ã€‚å¥¹è¢«é˜¿æ˜çš„å–„è‰¯å’Œæ‰§ç€æ‰€æ„ŸåŠ¨ï¼Œç­”åº”äº†ä»–çš„è¯·æ±‚ã€‚ç¬¬äºŒå¤©æ—©æ™¨ï¼Œå¤©ç©ºä¹Œäº‘å¯†å¸ƒï¼Œå¤§é›¨å€¾ç›†è€Œä¸‹ï¼Œä¹…æ—±çš„åœŸåœ°å¾—åˆ°äº†æ»‹æ¶¦ï¼Œåº„ç¨¼é‡æ–°ç„•å‘ç”Ÿæœºã€‚\n\nä»æ­¤ä»¥åï¼Œæ¯å¹´çš„æ»¡æœˆä¹‹å¤œï¼Œé˜¿æ˜éƒ½ä¼šå»åå±±ç­‰å¾…æœˆäº®å¥³ç¥çš„å‡ºç°ï¼Œä»–æˆä¸ºäº†æ‘æ°‘å¿ƒä¸­çš„å®ˆæŠ¤è€…ï¼Œç”¨ä»–çš„å–„è‰¯å’Œæ‰§ç€ï¼Œå®ˆæŠ¤ç€æ•´ä¸ªæ‘åº„ã€‚è€Œä»–ä¹Ÿç»ˆäºæ˜ç™½ï¼ŒçœŸæ­£çš„å®ˆæŠ¤è€…ï¼Œå¹¶ééœ€è¦è¶…å‡¡çš„åŠ›é‡ï¼Œåªéœ€è¦ä¸€é¢—å……æ»¡çˆ±ä¸å–„è‰¯çš„å¿ƒã€‚')
```

ä½ å¯ä»¥åœ¨ byzerllm.utils æ¨¡å—ä¸­é€šè¿‡ import è¯­å¥å¼•å…¥é»˜è®¤çš„æç¤ºæ¨¡æ¿å‡½æ•°ï¼Œ`from byzerllm.utils import response_class_format,response_class_format_after_chat`ã€‚

å¦‚æœæ¨¡å‹ä½¿ç”¨é»˜è®¤å‡½æ•°çš„æ•ˆæœä¸å°½å¦‚äººæ„ï¼Œä½ å¯ä»¥è®¾ç½®è‡ªå®šä¹‰å‡½æ•°æ¥ä¼˜åŒ–å®ƒï¼š

```python
def custom_response_class_format(prompt:str,cls:pydantic.BaseModel)->str:
.....


llm.setup_response_class_format_func("chat",custom_response_class_format)
```

## å‡½æ•°å®ç°åŠŸèƒ½

Byzer-LLM è¿˜æ”¯æŒå‡½æ•°å®ç°åŠŸèƒ½ã€‚æ‚¨å¯ä»¥å®šä¹‰ä¸€ä¸ªç©ºå‡½æ•°ï¼Œå¹¶ç»“åˆå‡½æ•°å†…çš„æ–‡æ¡£è¯´æ˜/ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œæ¥å¼•å¯¼å¤§è¯­è¨€æ¨¡å‹(LLM)å»å®ç°è¿™ä¸ªå‡½æ•°çš„åŠŸèƒ½ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼š

```python
from byzerllm.utils.client import code_utils,message_utils
from typing import List,Union,Optional
import pydantic

class Time(pydantic.BaseModel):
    time: str = pydantic.Field(...,description="æ—¶é—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")


@llm.impl()
def calculate_current_time()->Time:
    '''
    è®¡ç®—å½“å‰æ—¶é—´
    '''
    pass 


calculate_current_time()
#output: Time(time='2024-01-28')
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œç³»ç»Ÿä¼šæŠŠå‡½æ•°å†…éƒ¨çš„è®¡ç®—è¿‡ç¨‹ï¼ˆå³å‡½æ•°å®ç°ï¼‰ç¼“å­˜èµ·æ¥ï¼Œè¿™æ ·å½“ä¸‹æ¬¡è°ƒç”¨ç›¸åŒå‡½æ•°æ—¶å°±èƒ½è¿…é€Ÿæ‰§è¡Œï¼Œæ— éœ€é‡æ–°è®¡ç®—ã€‚

```python
start = time.monotonic()
calculate_current_time()
print(f"first time cost: {time.monotonic()-start}")

start = time.monotonic()
calculate_current_time()
print(f"second time cost: {time.monotonic()-start}")

# output:
# first time cost: 6.067266260739416
# second time cost: 4.347506910562515e-05
```

è‹¥è¦æ¸…é™¤ç¼“å­˜ï¼Œå¯ä»¥é€šè¿‡è¿è¡Œ `llm.clear_impl_cache()` æ–¹æ³•æ¥å®ç°è¿™ä¸€ç›®çš„ã€‚

æ¥ä¸‹æ¥æ˜¯ä¸€ä¸ªå±•ç¤ºå¦‚ä½•é’ˆå¯¹å¸¦å‚æ•°çš„å‡½æ•°æ‰§è¡Œç»“æœè¿›è¡Œç¼“å­˜å¤„ç†çš„ç¤ºä¾‹ï¼š

```python
from byzerllm.utils.client import code_utils,message_utils
from typing import List,Union,Optional,Annotated
import pydantic
from datetime import datetime

class Time(pydantic.BaseModel):
    time: str = pydantic.Field(...,description="æ—¶é—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")


@llm.impl()
def add_one_day(current_day:Annotated[datetime,"å½“å‰æ—¥æœŸï¼Œç±»å‹æ˜¯datatime.datetime"])->Time:
    '''
    ç»™ä¼ å…¥çš„æ—¥æœŸåŠ ä¸€å¤©ï¼Œå¾—åˆ°æ˜å¤©çš„æ—¶é—´
    '''
    pass 


add_one_day(datetime.now())
# output:Time(time='2024-01-29')
```

æ“ä½œæŒ‡å¼•ï¼š

```python
from byzerllm.utils.client import code_utils,message_utils
from typing import List,Union,Optional
import pydantic

class TimeRange(pydantic.BaseModel):
    '''
    æ—¶é—´åŒºé—´
    æ ¼å¼éœ€è¦å¦‚ä¸‹ï¼š yyyy-MM-dd
    '''  
    
    start: str = pydantic.Field(...,description="å¼€å§‹æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")
    end: str = pydantic.Field(...,description="æˆªæ­¢æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")

@llm.impl(instruction="å»å¹´ä¸‰æœˆåˆ°ä¸ƒæœˆ")
def calculate_time_range()->TimeRange:
    '''
    è®¡ç®—æ—¶é—´åŒºé—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd. 
    '''
    pass 

calculate_time_range()
# output: TimeRange(start='2023-03-01', end='2023-07-31')
```

è‹¥æƒ³å°†ç”¨æˆ·çš„æŸ¥è¯¢é—®é¢˜ç”¨äºæ›¿ä»£åŸå…ˆç”¨æ¥æ¸…é™¤ç¼“å­˜çš„æŒ‡ä»¤ï¼Œå¯ä»¥é‡‡ç”¨å¦‚ä¸‹ä»£ç å®ç°è¿™ä¸€åŠŸèƒ½ï¼š

```python
from byzerllm.utils.client import code_utils,message_utils
from typing import List,Union,Optional
import pydantic

class TimeRange(pydantic.BaseModel):
    '''
    æ—¶é—´åŒºé—´
    æ ¼å¼éœ€è¦å¦‚ä¸‹ï¼š yyyy-MM-dd
    '''  
    
    start: str = pydantic.Field(...,description="å¼€å§‹æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")
    end: str = pydantic.Field(...,description="æˆªæ­¢æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")

def calculate_time_range()->TimeRange:
    '''
    è®¡ç®—æ—¶é—´åŒºé—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd. 
    '''
    pass 


llm.impl(instruction="å»å¹´ä¸‰æœˆåˆ°ä¸ƒæœˆ")(calculate_time_range)()
```

è‹¥æƒ³æ·±å…¥äº†è§£å‡½æ•°å®ç°çš„è¯¦ç»†æƒ…å†µï¼Œå¯åœ¨è°ƒç”¨æ—¶åŠ ä¸Š `verbose=True` å‚æ•°ï¼Œç³»ç»Ÿå°†ä¸ºä½ æä¾›æ›´å¤šç›¸å…³ä¿¡æ¯ï¼š

```python
@llm.impl()
def add_one_day(current_day:Annotated[datetime,"å½“å‰æ—¥æœŸï¼Œç±»å‹æ˜¯datatime.datetime"])->Time:
    '''
    ç»™ä¼ å…¥çš„æ—¥æœŸåŠ ä¸€å¤©ï¼Œå¾—åˆ°æ˜å¤©çš„æ—¶é—´
    '''
    pass 
```

ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨åŸºç¡€çš„ chat_oai å‡½æ•°æ¥å®ç°å‡½æ•°ï¼š

```python
class TimeRange(pydantic.BaseModel):
    '''
    æ—¶é—´åŒºé—´
    æ ¼å¼éœ€è¦å¦‚ä¸‹ï¼š yyyy-MM-dd
    '''  
    
    start: str = pydantic.Field(...,description="å¼€å§‹æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")
    end: str = pydantic.Field(...,description="æˆªæ­¢æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")


def calculate_time_range():
    '''
    è®¡ç®—æ—¶é—´åŒºé—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd. 
    '''
    pass 
    
t = llm.chat_oai([{
    "content":"å»å¹´ä¸‰æœˆåˆ°ä¸ƒæœˆ",
    "role":"user"    
}],impl_func=calculate_time_range,response_class=TimeRange,execute_impl_func=True)
```

ä¸Šè¿°ä»£ç ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªåä¸º `calculate_time_range` çš„å‡½æ•°ï¼Œè¯¥å‡½æ•°ç›®å‰ä¸ºç©ºã€‚æ¥ç€æˆ‘ä»¬åœ¨æ–‡æ¡£å­—ç¬¦ä¸²ä¸­è¯¦ç»†æè¿°äº†å‡½æ•°çš„åŠŸèƒ½ï¼Œå¹¶å®šä¹‰äº†å“åº”ç±» `TimeRange`ï¼Œç¡®ä¿å‡½æ•°è¿”å›ä¸€ä¸ª `TimeRange` å®ä¾‹ã€‚ç”±äºè¯¥å‡½æ•°åº”æœåŠ¡äºè§£ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œæ‰€ä»¥å®ƒçš„å®ç°åº”å½“ä¸ç”¨æˆ·çš„å…·ä½“é—®é¢˜ç´§å¯†ç›¸å…³ã€‚æˆ‘ä»¬ä¸æ˜¯è¦å»å®ç°ä¸€ä¸ªé€šç”¨çš„å‡½æ•°ï¼Œè€Œæ˜¯å®ç°ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ç”¨æˆ·å½“å‰é—®é¢˜è¿›è¡Œè§£ç­”çš„å‡½æ•°ã€‚

æ‰§è¡Œåï¼Œä½ ä¼šå¾—åˆ°å¦‚ä¸‹æ‰€ç¤ºçš„è¾“å‡ºç»“æœï¼š

```python
t[0].value
# start='2023-03-01' end='2023-07-31'
```

å¦‚æœè¿”å›çš„å€¼æ˜¯ None æˆ–ä¸æ­£ç¡®ï¼Œç³»ç»Ÿå°†ä¼šç»™å‡ºé”™è¯¯æç¤ºä¿¡æ¯ï¼š
```python
t[0].metadata.get("resason","")
```

å¦‚æœä½ å®šä¹‰çš„å‡½æ•°å¸¦æœ‰å‚æ•°ï¼Œå¯ä»¥é€šè¿‡ `impl_func_params` å‚æ•°ä¼ é€’ç»™è¯¥å‡½æ•°ï¼š

```python
t = llm.chat_oai([{
    "content":"xxxxx",
    "role":"user"    
}],
impl_func=calculate_time_range,
impl_func_params={},
response_class=TimeRange,execute_impl_func=True)
```

å¦‚æœä½ æƒ³è¦æ›¿æ¢é»˜è®¤çš„æç¤ºæ¨¡æ¿å‡½æ•°ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªç¤ºä¾‹ï¼š

```python
import pydantic
from typing import List,Optional,Union,Callable
from byzerllm.utils import serialize_function_to_json

def function_impl_format2(prompt:str,func:Optional[Union[Callable,str]],
                             cls:Union[pydantic.BaseModel,str])->str:
    
    tool_choice_ser = serialize_function_to_json(func)    
    _cls = ""
    if isinstance(cls, str):
        _cls = cls
    else:
        _cls = cls.schema_json(ensure_ascii=False)
    
    msg = f''''ç”Ÿæˆä¸€ä¸ªpythonå‡½æ•°ï¼Œç»™å‡ºè¯¦ç»†çš„æ€è€ƒé€»è¾‘ï¼Œå¯¹æœ€åç”Ÿæˆçš„å‡½æ•°ä¸è¦è¿›è¡Œç¤ºä¾‹è¯´æ˜ã€‚

ç”Ÿæˆçš„å‡½æ•°çš„åå­—ä»¥åŠå‚æ•°éœ€è¦æ»¡è¶³å¦‚ä¸‹çº¦æŸï¼š

\```json
{tool_choice_ser}
\```

ç”Ÿæˆçš„å‡½æ•°çš„è¿”å›å€¼å¿…é¡»æ˜¯ Json æ ¼å¼ã€‚

ä¸‹é¢æ˜¯ä½¿ç”¨ OpenAPI 3.1. è§„èŒƒæè¿°äº†ä½ éœ€å¦‚ä½•è¿›è¡Œjsonæ ¼å¼çš„ç”Ÿæˆã€‚

\```json
{_cls}
\```

æ ¹æ®ç”¨çš„æˆ·é—®é¢˜,{func.__doc__}ã€‚ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š{prompt}

è¯·ä½ å®ç°è¿™ä¸ªå‡½æ•°ã€‚
''' 
    
    return msg

llm.setup_impl_func_format_func(chat_model_name,function_impl_format2)
```
é»˜è®¤çš„æç¤ºæ¨¡æ¿å‡½æ•°æ˜¯ `function_impl_format`ï¼Œä½ å¯ä»¥åœ¨ `from byzerllm.utils import function_impl_format` è¿™æ®µä»£ç ä¸­æŸ¥çœ‹å…¶æºä»£ç ã€‚

## å¤§è¯­è¨€æ¨¡å‹å‹å¥½å‹å‡½æ•°/æ•°æ®ç±»

è‹¥è¦æå‡å‡½æ•°è°ƒç”¨æˆ–å“åº”ç±»çš„æ€§èƒ½è¡¨ç°ï¼Œåº”å½“ç¡®ä¿ä½ çš„å‡½æ•°ï¼ˆå·¥å…·ï¼‰å’Œæ•°æ®ç±»å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‹å¥½ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹ä¸€æ®µ Python ä»£ç ç¤ºä¾‹ï¼š

```python
def compute_date_range(count:int, unit:str)->List[str]:                   
    now = datetime.datetime.now()
    ....
```

è¿™æ®µä»£ç å¹¶éå¯¹å¤§è¯­è¨€æ¨¡å‹å‹å¥½ï¼Œå› ä¸ºå®ƒéš¾ä»¥è®©äººæˆ–LLMç†è§£å‡½æ•°çš„å…·ä½“ç”¨é€”ä»¥åŠè¾“å…¥å‚æ•°çš„å«ä¹‰ã€‚

å¤§è¯­è¨€æ¨¡å‹å°±å¦‚åŒäººç±»ä¸€æ ·ï¼Œå¾ˆéš¾è®©å®ƒçŸ¥é“ä½•æ—¶æˆ–å¦‚ä½•è°ƒç”¨è¿™ä¸ªå‡½æ•°ã€‚ç‰¹åˆ«æ˜¯å‚æ•°`unit`å®é™…ä¸Šæ˜¯ä¸€ä¸ªæšä¸¾å€¼ï¼Œä½†æ˜¯å¤§è¯­è¨€æ¨¡å‹æ— æ³•è·çŸ¥è¿™ä¸€ä¿¡æ¯ã€‚

å› æ­¤ï¼Œä¸ºäº†è®©å¤§è¯­è¨€æ¨¡å‹æ›´å¥½åœ°ç†è§£ Byzer-LLM ä¸­çš„è¿™ä¸ªå‡½æ•°ï¼Œä½ åº”è¯¥éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š

1. æ·»åŠ ç¬¦åˆ Python è§„èŒƒçš„å‡½æ•°æ³¨é‡Š
2. ä½¿ç”¨ç±»å‹æ³¨è§£ä¸ºæ¯ä¸ªå‚æ•°æä¾›ç±»å‹å’Œæ³¨é‡Šï¼Œå¦‚æœå‚æ•°æ˜¯ä¸€ä¸ªæšä¸¾å€¼ï¼Œè¿˜éœ€è¦æä¾›æšä¸¾çš„æ‰€æœ‰å¯èƒ½å–å€¼ã€‚

ä¸‹é¢æ˜¯æ”¹è¿›åçš„å¯¹å¤§è¯­è¨€æ¨¡å‹å‹å¥½çš„å‡½æ•°å®šä¹‰ç¤ºä¾‹

```python
def compute_date_range(count:Annotated[int,"æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»å‹"],
                       unit:Annotated[str,"æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»å‹",{"enum":["day","week","month","year"]}])->List[str]:
    '''
    è®¡ç®—æ—¥æœŸèŒƒå›´

    Args:
        count: æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»å‹
        unit: æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»å‹ï¼Œå¯é€‰å€¼ä¸º day,week,month,year
    '''        
    now = datetime.datetime.now()
    ....
```

å¦‚æœå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è°ƒç”¨ä½ çš„å‡½æ•°æ—¶å‡ºç°é—®é¢˜ï¼ˆä¾‹å¦‚æä¾›äº†é”™è¯¯çš„å‚æ•°ï¼‰ï¼Œè¯•ç€ä¼˜åŒ–å‡½æ•°æ³¨é‡Šå’Œå‚æ•°çš„ç±»å‹æ³¨è§£æ³¨é‡Šï¼Œä»¥å¸®åŠ©LLMæ›´å¥½åœ°ç†è§£å‡½æ•°çš„æ­£ç¡®ç”¨æ³•å’Œå‚æ•°å«ä¹‰ã€‚

## æ¨¡å‹å…ƒä¿¡æ¯

Byzer-LLM åŒæ ·æ”¯æŒè·å–æ¨¡å‹å®ä¾‹çš„å…ƒä¿¡æ¯ã€‚ä¸‹é¢çš„ä»£ç å°†è·å–åä¸º `chat` çš„æ¨¡å‹å®ä¾‹çš„å…ƒä¿¡æ¯ï¼š

```python
llm.get_meta(model="chat")

#output:
# {'model_deploy_type': 'proprietary',
#  'backend': 'ray/vllm',
#  'max_model_len': 32768,
#  'architectures': ['QWenLMHeadModel']}
```

## å¯¹è¯æ¨¡æ¿

ä¸åŒçš„æ¨¡å‹æ‹¥æœ‰å„è‡ªçš„å¯¹è¯æ¨¡æ¿ï¼ŒByzer-LLM ä¸ºå„ä¸ªæ¨¡å‹æä¾›äº†ä¸€äº›é¢„è®¾çš„å¯¹è¯æ¨¡æ¿ã€‚ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç æ¥è®¾ç½®å¯¹è¯æ¨¡æ¿ï¼š

```python
from byzerllm.utils.client import Templates
llm.setup_template("chat",Templates.qwen()) 
```

ä¸ä»…å¦‚æ­¤ï¼Œæˆ‘ä»¬è¿˜æ”¯æŒä½¿ç”¨ `tokenizer.apply_chat_template` æ–¹æ³•ï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç åº”ç”¨å¯¹è¯æ¨¡æ¿ï¼š

```python
llm.setup_template("chat","auto") 
```

è¦æ˜¯æ¨¡å‹è·Ÿ `tokenizer.apply_chat_template` è¿™ä¸ªå°å·¥å…·ç©ä¸è½¬ï¼Œå®ƒä¼šå‘å‡ºä¿¡å·â€”â€”ä¹Ÿå°±æ˜¯æŠ›å‡ºä¸€ä¸ªå¼‚å¸¸ã€‚è¿™æ—¶å€™ï¼Œä½ å®Œå…¨å¯ä»¥äº²è‡ªå‡ºæ‰‹ï¼Œç”¨ `llm.setup_template` è¿™ä¸ªæ‹›å¼æ¥æ‰‹åŠ¨æ‰“é€ èŠå¤©æ¨¡æ¿ã€‚

æ­¤å¤–ï¼Œä½ è¿˜èƒ½ç”¨ `llm.get_meta` è¿™ä¸ªæ¢æµ‹å™¨å»ç§ç§ï¼Œçœ‹çœ‹å’±å®¶çš„æ¨¡å‹åˆ°åº•æ”¯ä¸æ”¯æŒ `apply_chat_template` è¿™é¡¹æŠ€èƒ½ï¼š

```python
llm.get_meta(model="chat")
```

è¾“å‡ºï¼š

```json
{'model_deploy_type': 'proprietary',
 'backend': 'ray/vllm',
 'support_stream': True,
 'support_chat_template': True,
 'max_model_len': 4096,
 'architectures': ['LlamaForCausalLM']}
```

æ³¨æ„ï¼Œè¿™é¡¹ç‰¹æ€§ä¼šè§¦å‘é¢å¤–çš„RPCè°ƒç”¨ï¼Œå› æ­¤ä¼šé€ æˆä¸€å®šçš„æ€§èƒ½æŸå¤±ã€‚

## LLM é»˜è®¤å‚æ•°

Byzer-LLM åŒæ ·æ”¯æŒä¸ºæ¨¡å‹è®¾ç½®é»˜è®¤ç”Ÿæˆå‚æ•°ã€‚ä»¥ä¸‹ä»£ç å°†ä¸ºåä¸º `chat` çš„æ¨¡å‹å®ä¾‹è®¾ç½®é»˜è®¤ç”Ÿæˆå‚æ•°ï¼š

```python
llm.setup_extra_generation_params("chat",{
    "generation.stop_token_ids":[7]
})
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå¯¹äºæ¨¡å‹å®ä¾‹ `chat`ï¼Œæˆ‘ä»¬å°†æŠŠ `generation.stop_token_ids` å‚æ•°è®¾ç½®ä¸ºæ•°ç»„ `[7]`ã€‚è¿™æ„å‘³ç€æ¯æ¬¡è°ƒç”¨ `chat` æ¨¡å‹æ‰§è¡Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ä½¿ç”¨è¿™ä¸ªé¢„è®¾å€¼ï¼Œå³å°†åœæ­¢ç”Ÿæˆåºåˆ—çš„æ ‡è¯†ç¬¦`generation.stop_token_ids`è®¾ä¸º `[7]` çš„ç‰¹æ®Šæ ‡è®°ã€‚å½“æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é‡åˆ°è¯¥åœç”¨è¯IDæ—¶ï¼Œå°±ä¼šåœæ­¢ç”Ÿæˆæ–°çš„æ–‡æœ¬ç‰‡æ®µã€‚

## å¤šæ¨¡æ€

Byzer å¤§è¯­è¨€æ¨¡å‹ï¼ˆByzer-LLMï¼‰åŒæ ·å…·å¤‡å¤„ç†å¤šæ¨¡æ€æ•°æ®çš„èƒ½åŠ›ã€‚æ¥ä¸‹æ¥å±•ç¤ºçš„ä»£ç ç‰‡æ®µå°†ä¼šéƒ¨ç½²ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹ï¼Œéšåè¿ç”¨æ­¤æ¨¡å‹å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œæ™ºèƒ½æ¨æ–­ã€‚

```python
import ray
from byzerllm.utils.client import ByzerLLM,InferBackend

ray.init(address="auto",namespace="default")   

llm = ByzerLLM()
chat_model_name = "qwen_vl_chat"
model_location = "/home/byzerllm/models/Qwen-VL-Chat"

llm.setup_gpus_per_worker(1).setup_num_workers(1).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/qwen_vl_chat",
    udf_name=chat_model_name,
    infer_params={}
)    
```

éšåï¼Œä½ å¯ä»¥å€ŸåŠ©è¿™ä¸ªæ¨¡å‹è¿›è¡Œå®æ—¶å¯¹è¯äº¤äº’ï¼š

```python
import base64
image_path = "/home/byzerllm/projects/jupyter-workspace/1.jpg"
with open(image_path, "rb") as f:
    image_content = base64.b64encode(f.read()).decode("utf-8")

t = llm.chat_oai(conversations=[{
    "role": "user",
    "content": "è¿™æ˜¯ä»€ä¹ˆ"
}],model=chat_model_name,llm_config={"image":image_content})

t[0].output

# '{"response": "å›¾ä¸­æ˜¯ä¸€åå¥³å­åœ¨æ²™æ»©ä¸Šå’Œç‹—ç©è€ï¼Œæ—è¾¹çš„ç‹—æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ï¼Œå®ƒååœ¨æ²™æ»©ä¸Šï¼Œé¢å¯¹ç€ä¸€åèº«ç©¿æ ¼å­è¡¬è¡«çš„å¥³å­ã€‚å¥³å­çš„è…¿æœ‰äº›æ®‹ç–¾ï¼Œä½†æ˜¯å¥¹ä¾ç„¶åšæŒååœ¨æ²™æ»©ä¸Šå’Œç‹—ç©è€ã€‚å¥¹çš„å³æ‰‹æ‹¿ç€ä¸€ä¸ªå°ç©å…·ï¼Œè¿™ä¸ªç©å…·ä¸Šé¢æœ‰ä¸¤è¡Œé»‘è‰²å­—æ¯ï¼Œå…·ä½“æ˜¯ä»€ä¹ˆå†…å®¹çœ‹ä¸æ¸…æ¥šã€‚å¥¹æ‰“ç®—æŠŠç©å…·æ‰”ç»™æ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚", "history": [{"role": "user", "content": "Picture 1: <img>/tmp/byzerllm/visualglm/images/23eb4cea-cb6e-4f55-8adf-3179ca92ab42.jpg</img>\\nè¿™æ˜¯ä»€ä¹ˆ"}, {"role": "assistant", "content": "å›¾ä¸­æ˜¯ä¸€åå¥³å­åœ¨æ²™æ»©ä¸Šå’Œç‹—ç©è€ï¼Œæ—è¾¹çš„ç‹—æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ï¼Œå®ƒååœ¨æ²™æ»©ä¸Šï¼Œé¢å¯¹ç€ä¸€åèº«ç©¿æ ¼å­è¡¬è¡«çš„å¥³å­ã€‚å¥³å­çš„è…¿æœ‰äº›æ®‹ç–¾ï¼Œä½†æ˜¯å¥¹ä¾ç„¶åšæŒååœ¨æ²™æ»©ä¸Šå’Œç‹—ç©è€ã€‚å¥¹çš„å³æ‰‹æ‹¿ç€ä¸€ä¸ªå°ç©å…·ï¼Œè¿™ä¸ªç©å…·ä¸Šé¢æœ‰ä¸¤è¡Œé»‘è‰²å­—æ¯ï¼Œå…·ä½“æ˜¯ä»€ä¹ˆå†…å®¹çœ‹ä¸æ¸…æ¥šã€‚å¥¹æ‰“ç®—æŠŠç©å…·æ‰”ç»™æ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚"}]}'
```

æ¥ä¸‹æ¥è¿™æ®µä»£ç å¯ä»¥å¸®åŠ©ä½ è¿ç»­ä¸æ–­åœ°ä¸æ¨¡å‹è¿›è¡Œå¤šå›åˆçš„å¯¹è¯äº¤æµï¼š

```python
import json
history = json.loads(t[0].output)["history"]

llm.chat_oai(conversations=history+[{
    "role": "user",
    "content": "èƒ½åœˆå‡ºç‹—ä¹ˆï¼Ÿ"
}],model=chat_model_name,llm_config={"image":image_content})

# [LLMResponse(output='{"response": "<ref>ç‹—</ref><box>(221,425),(511,889)</box>", "history": [{"role"
```

é¦–å…ˆï¼Œæå–ä¸Šæ¬¡å¯¹è¯çš„èŠå¤©è®°å½•ï¼Œç„¶åå°†è¿™éƒ¨åˆ†å†å²å†…å®¹èå…¥æ–°çš„å¯¹è¯ç¯èŠ‚ï¼Œè¿›è€Œç»§ç»­å¼€å±•æ–°çš„å¯¹è¯äº¤æµã€‚

## StableDiffusion

Tyzer å¤§è¯­è¨€æ¨¡å‹ï¼ˆByzer-LLMï¼‰åŒæ ·æ”¯æŒé›†æˆ StableDiffusion æŠ€æœ¯ä½œä¸ºå…¶åº•å±‚æ¨ç†æ¡†æ¶ã€‚æ¥ä¸‹æ¥çš„ä»£ç å°†éƒ¨ç½²ä¸€ä¸ªåŸºäº StableDiffusion çš„æ¨¡å‹ï¼Œå¹¶å€ŸåŠ©æ­¤æ¨¡å‹å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œæ·±åº¦ç†è§£å’Œè§†è§‰ç”Ÿæˆç­‰æ–¹é¢çš„æ™ºèƒ½æ¨æ–­ã€‚

```python
import ray
from byzerllm.utils.client import ByzerLLM,InferBackend

ray.init(address="auto",namespace="default")   

llm = ByzerLLM()
chat_model_name = "sd_chat"
model_location = "/home/byzerllm/models/stable-diffusion-v1-5"

llm.setup_gpus_per_worker(2).setup_num_workers(1).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/stable_diffusion",
    udf_name=chat_model_name,
    infer_params={}
)

def show_image(content):
    from IPython.display import display, Image
    import base64             
    img = Image(base64.b64decode(content))
    display(img)    
    
```

ç„¶åå°±å¯ä»¥é€šè¿‡è¿™ä¸ªæ¨¡å‹è¿›è¡Œå¯¹è¯ï¼š

```python
import json
t = llm.chat_oai(
    conversations=[
        {
            "role":"user",
            "content":"ç”»ä¸€åªçŒ«"
        }
    ],model=chat_model_name,llm_config={"gen.batch_size":3}
)

cats = json.loads(t[0].output)
for res in cats:
    show_image(res["img64"])
```

è¾“å‡ºï¼š

![](./images/cat2.png)

The parameters:
å‚æ•°é…ç½®ï¼š

| å‚æ•°                        | å«ä¹‰                                                         | é»˜è®¤å€¼   |
| --------------------------- | ------------------------------------------------------------ | -------- |
| Instruction                 | prompt                                                       | éç©º     |
| generation.negative_prompt  | åå‘çš„prompt                                                 | ""       |
| generation.sampler_name     | è°ƒåº¦å(unpic, euler_a,euler,ddim,ddpm,deis,dpm2,dpm2-a,dpm++_2m,dpm++_2m_karras,heun,heun_karras,lms,pndm:w) | euler_a  |
| generation.sampling_steps   | ç”Ÿæˆçš„æ­¥éª¤æ•°                                                 | 25       |
| generation.batch_size       | ä¸€æ¬¡ç”Ÿæˆå‡ å¼                                                  | 1        |
| generation.batch_count      | ç”Ÿæˆå‡ æ¬¡                                                     | 1        |
| generation.cfg_scale        | éšæœºæˆ–è´´åˆç¨‹åº¦å€¼,å€¼è¶Šå°ç”Ÿæˆçš„å›¾ç‰‡ç¦»ä½ çš„Tagsæè¿°çš„å†…å®¹å·®è·è¶Šå¤§ | 7.5      |
| generation.seed             | éšæœºç§å­                                                     | -1       |
| generation.width            | å›¾ç‰‡å®½åº¦                                                     | 768      |
| generation.height           | å›¾ç‰‡é«˜åº¦                                                     | 768      |
| generation.enable_hires     | å¼€å¯é«˜åˆ†è¾¨ç‡ä¿®å¤åŠŸèƒ½(å’Œä¸‹é¢ä¸¤ä¸ªä¸€ç»„)                         | false    |
| generation.upscaler_mode    | æ”¾å¤§ç®—æ³•(bilinear, bilinear-antialiased,bicubic,bicubic-antialiased,nearest,nearest-exact) | bilinear |
| generation.scale_slider     | æ”¾å¤§æ¯”ä¾‹                                                     | 1.5      |
| generation.enable_multidiff | å›¾ç‰‡åˆ†å‰²å¤„ç†(å‡å°‘æ˜¾å­˜é”€è€—)(å’Œä¸‹é¢3ä¸ªä¸€ç»„)                    | false    |
| generation.views_batch_size | åˆ†æ‰¹å¤„ç†è§„æ¨¡                                                 | 4        |
| generation.window_size      | åˆ‡å‰²å¤§å°ï¼Œå®½ï¼Œé«˜                                             | 64       |
| generation.stride           | æ­¥é•¿                                                         | 16       |
| generation.init_image       | åˆå§‹åŒ–å›¾ç‰‡ï¼ŒåŸºäºè¿™ä¸ªå›¾ç‰‡å¤„ç†(å¿…é¡»ä¼ è¾“base64åŠ å¯†çš„å›¾ç‰‡) (å’Œä¸‹é¢çš„ä¸€ç»„) | None     |
| generation.strength         | é‡ç»˜å¹…åº¦: å›¾åƒæ¨¡ä»¿è‡ªç”±åº¦ï¼Œè¶Šé«˜è¶Šè‡ªç”±å‘æŒ¥ï¼Œè¶Šä½å’Œå‚è€ƒå›¾åƒè¶Šæ¥è¿‘ï¼Œé€šå¸¸å°äº0.3åŸºæœ¬å°±æ˜¯åŠ æ»¤é•œ | 0.5      |



## SQL æ”¯æŒ

é™¤äº† Python æ¥å£ä¹‹å¤–ï¼ŒByzer-llm åŒæ ·å…¼å®¹ SQL APIã€‚è‹¥è¦ä½¿ç”¨ SQL API åŠŸèƒ½ï¼Œè¯·å…ˆç¡®ä¿å®‰è£… Byzer-SQL è¯­è¨€ã€‚

å¯é‡‡ç”¨å¦‚ä¸‹å‘½ä»¤æ¥å®‰è£… Byzer-SQL è¯­è¨€ï¼š

```bash
git clone https://gitee.com/allwefantasy/byzer-llm
cd byzer-llm/setup-machine
sudo -i 
ROLE=master ./setup-machine.sh
```

å®‰è£…æˆåŠŸåï¼Œæ‚¨å¯ä»¥è®¿é—®æœ¬åœ° Byzer æ§åˆ¶å°ï¼Œåœ°å€ä¸ºï¼š`http://localhost:9002`ã€‚

åœ¨ Byzer æ§åˆ¶å°å†…ï¼Œæ‚¨å¯ä»¥æ‰§è¡Œå¦‚ä¸‹ SQL å‘½ä»¤æ¥éƒ¨ç½² llama2 æ¨¡å‹ï¼Œè¯¥æ¨¡å‹çš„åŠŸèƒ½ä¸å‰è¿° Python ä»£ç ç‰‡æ®µå®Œå…¨ä¸€è‡´ã€‚

```sql
!byzerllm setup single;
!byzerllm setup "num_gpus=4";
!byzerllm setup "maxConcurrency=1";
!byzerllm setup "infer_backend=transformers";

run command as LLM.`` where 
action="infer"
and pretrainedModelType="custom/llama2"
and localModelDir="/home/byzerllm/models/openbuddy-llama-13b-v5-fp16"
and reconnect="false"
and udfName="llama2_chat"
and modelTable="command";

```

æ¥ä¸‹æ¥ï¼Œæ‚¨å¯ä»¥é€šè¿‡è°ƒç”¨åä¸º `llama2_chat` çš„ UDFï¼ˆç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ï¼‰æ¥æ¿€æ´»å’Œä½¿ç”¨è¿™ä¸ªlama2æ¨¡å‹ï¼š

```sql

select 
llama2_chat(llm_param(map(
              "user_role","User",
              "assistant_role","Assistant",
              "system_msg",'You are a helpful assistant. Think it over and answer the user question correctly.',
              "instruction",llm_prompt('
Please remenber my name: {0}              
',array("Zhu William"))

))) as q 
as q1;
```

å½“ä½ ä½¿ç”¨ç±»ä¼¼ `run command as LLM` çš„æ–¹å¼éƒ¨ç½²äº†æ¨¡å‹åï¼Œå°±èƒ½å¤Ÿå¦‚åŒè°ƒç”¨ SQL å‡½æ•°é‚£æ ·æ¥ä½¿ç”¨è¯¥æ¨¡å‹ã€‚è¿™ä¸€ç‰¹ç‚¹æå¤§åœ°ä¾¿åˆ©äº†é‚£äº›å¸Œæœ›å»ºç«‹æ•°æ®åˆ†ææ¨¡å‹æ—¶èå…¥å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„æ•°æ®ç§‘å­¦å®¶ï¼Œä»¥åŠæœŸæœ›åœ¨æ„å»ºæ•°æ®å¤„ç†æµæ°´çº¿æ—¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹åŠŸèƒ½çš„æ•°æ®å·¥ç¨‹å¸ˆä»¬ã€‚

---

### QWen 

åœ¨ ByzerLLM ä¸­ä½¿ç”¨ QWen åŠŸèƒ½æ—¶ï¼Œä½ éœ€è¦æ‰‹åŠ¨è®¾ç½®å‡ ä¸ªå…³é”®å‚æ•°ï¼š

1. è§’è‰²å¯¹åº”å…³ç³»ï¼ˆè§’è‰²æ˜ å°„ï¼‰
2. ç»ˆæ­¢æ ‡è¯†ç¬¦åˆ—è¡¨ï¼ˆç»“æŸç¬¦å·IDåˆ—è¡¨ï¼‰
3. ä»ç”Ÿæˆçš„å›ç­”ä¸­å»é™¤ç»ˆæ­¢æ ‡è¯†ç¬¦ï¼ˆç®€å•åœ°è¯´ï¼Œå°±æ˜¯åœ¨ç”Ÿæˆç»“æœä¸­è£å‰ªæ‰ä»£è¡¨å¯¹è¯ç»“æŸçš„ç‰¹æ®Šç¬¦å·ï¼‰

ä¸ºäº†æ–¹ä¾¿å¤§å®¶æ“ä½œï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªé¢„è®¾æ¨¡æ¿ï¼Œä½ å¯ä»¥è¯•è¯•ä¸‹é¢è¿™æ®µä»£ç ï¼š

```python
from byzerllm.utils.client import Templates

### Here,we setup the template for qwen
llm.setup_template("chat",Templates.qwen())

t = llm.chat_oai(conversations=[{
    "role":"user",
    "content":"ä½ å¥½,ç»™æˆ‘è®²ä¸ª100å­—çš„ç¬‘è¯å§?"
}])
print(t)
```

---
## SaaS æ¨¡å‹

é‰´äºå„ç±» SaaS æ¨¡å¼å…·æœ‰å„è‡ªçš„å®šåˆ¶å‚æ•°ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸ºæ‚¨å‡†å¤‡äº†ä¸€ç³»åˆ— SaaS æ¨¡å‹éƒ¨ç½²æ‰€éœ€çš„æ¨¡æ¿ï¼ŒåŠ©åŠ›æ‚¨è½»æ¾å®Œæˆä¸åŒ SaaS æ¨¡å‹çš„éƒ¨ç½²å·¥ä½œã€‚

### ç™¾å·ï¼ˆbaichuanï¼‰

```python

import ray
from byzerllm.utils.client import ByzerLLM

ray.init(address="auto",namespace="default",ignore_reinit_error=True)   

llm = ByzerLLM(verbose=True)

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "baichuan_chat2"
if llm.is_model_exist(chat_name):
    llm.undeploy(udf_name=chat_name)

llm.deploy(model_path="",
           pretrained_model_type="saas/baichuan",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxxxxx",                        
            "saas.model":"Baichuan2-Turbo"
           })

llm.chat_oai(model=chat_name,conversations=[{
    "role":"user",
    "content":"ä½ å¥½",
}])           
```

é’ˆå¯¹ `saas.model` å‚æ•°ï¼Œè¿™é‡Œæœ‰ä¸€äº›æšä¸¾å€¼å¯ä¾›é€‰æ‹©ï¼š

1. Baichuan2-Turbo
2. Baichuan-Text-Embedding

### é€šä¹‰åƒé—®ï¼ˆqianwenï¼‰

```python
from byzerllm.utils.client import ByzerLLM
llm = ByzerLLM()

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "qianwen_chat"

llm.deploy(model_path="",
           pretrained_model_type="saas/qianwen",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxxxxx",            
            "saas.model":"qwen-turbo"
           })

## here you can use `stream_chat_oai`
v = llm.stream_chat_oai(model=chat_name,conversations=[{
    "role":"user",
    "content":"ä½ å¥½ï¼Œä½ æ˜¯è°",
}],llm_config={"gen.incremental_output":False})

for t in v:
    print(t,flush=True)           
```

é’ˆå¯¹ `saas.model` å‚æ•°ï¼Œè¿™é‡Œæœ‰å‡ ä¸ªé¢„è®¾çš„æšä¸¾å€¼é€‰é¡¹ï¼š

1. qwen-turbo
2. qwen-max

### azure openai

```sql

!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/azure_openai"
and `saas.api_type`="azure"
and `saas.api_key`="xxx"
and `saas.api_base`="xxx"
and `saas.api_version`="2023-07-01-preview"
and `saas.deployment_id`="xxxxx"
and udfName="azure_openai"
and modelTable="command";
```

### openai

```sql

import ray

from byzerllm.utils.client import ByzerLLM

ray.init(address="auto",namespace="default",ignore_reinit_error=True)  

llm = ByzerLLM()

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "openai_chat"

llm.deploy(model_path="",
           pretrained_model_type="saas/official_openai",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxx",            
            "saas.model":"gpt-3.5-turbo-1106"
           })
```

è‹¥æ‚¨éœ€è¦ç”¨åˆ°ç½‘ç»œä»£ç†ï¼Œå¯ä»¥å°è¯•è¿è¡Œå¦‚ä¸‹ä»£ç æ¥é…ç½®ä»£ç†è®¾ç½®ï¼š

```python
llm.deploy(model_path="",
           pretrained_model_type="saas/official_openai",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxx",            
            "saas.model":"gpt-3.5-turbo-1106"
            "saas.base_url": "http://my.test.server.example.com:8083",
            "saas.proxies":"http://my.test.proxy.example.com"
            "saas.local_address":"0.0.0.0"
           })
```


### æ™ºè°±ï¼ˆzhipuï¼‰

```python
import ray

from byzerllm.utils.client import ByzerLLM

ray.init(address="auto",namespace="default",ignore_reinit_error=True)  

llm = ByzerLLM()

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "zhipu_chat"

llm.deploy(model_path="",
           pretrained_model_type="saas/zhipu",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxx",            
            "saas.model":"glm-4"
           })
```

é’ˆå¯¹ `saas.model` å‚æ•°ï¼Œè¿™é‡Œæœ‰å‡ ä¸ªé¢„è®¾çš„æšä¸¾å€¼é€‰é¡¹ï¼š

1. glm-4
2. embedding-2

### minimax

```sql

!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/minimax"
and `saas.api_key`="xxxxxxxxxxxxxxxxxx"
and `saas.group_id`="xxxxxxxxxxxxxxxx"
and `saas.model`="abab5.5-chat"
and `saas.api_url`="https://api.minimax.chat/v1/text/chatcompletion_pro"
and udfName="minimax_saas"
and modelTable="command";

```

### æ˜Ÿç«ï¼ˆsparkdeskï¼‰

```python
import ray

from byzerllm.utils.client import ByzerLLM

ray.init(address="auto",namespace="default",ignore_reinit_error=True)

llm = ByzerLLM()

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "sparkdesk_saas"

if llm.is_model_exist(chat_name):
  llm.undeploy(udf_name=chat_name)

llm.deploy(model_path="",
           pretrained_model_type="saas/sparkdesk",
           udf_name=chat_name,
           infer_params={
             "saas.appid":"xxxxxxx",
             "saas.api_key":"xxxxxxx",
             "saas.api_secret":"xxxxxxx",
             "saas.gpt_url":"wss://spark-api.xf-yun.com/v3.1/chat",
             "saas.domain":"generalv3"
           })

v = llm.chat_oai(model=chat_name,conversations=[{
  "role":"user",
  "content":"your prompt content",
}])
```

SparkDesk V1.5 ç‰ˆæœ¬è¯·æ±‚é“¾æ¥ï¼Œå…³è”çš„åŸŸåå‚æ•°ä¸ºâ€œgeneralâ€ï¼š
`wss://spark-api.xf-yun.com/v1.1/chat`  

SparkDesk V2 ç‰ˆæœ¬è¯·æ±‚é“¾æ¥ï¼Œå…³è”çš„åŸŸåå‚æ•°ä¸ºâ€œgeneralv2â€ï¼š
`wss://spark-api.xf-yun.com/v2.1/chat`  

SparkDesk V3 ç‰ˆæœ¬è¯·æ±‚é“¾æ¥ï¼Œå…³è”çš„åŸŸåå‚æ•°æ›´æ–°ä¸ºâ€œgeneralv3â€ï¼ˆç°å·²æ”¯æŒå‡½æ•°è°ƒç”¨åŠŸèƒ½ï¼‰ï¼š
`wss://spark-api.xf-yun.com/v3.1/chat`  

```sql
!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/sparkdesk"
and `saas.appid`="xxxxxxxxxxxxxxxxxx"
and `saas.api_key`="xxxxxxxxxxxxxxxx"
and `saas.api_secret`="xxxx"
and `gpt_url`="ws://spark-api.xf-yun.com/v1.1/chat"
and udfName="sparkdesk_saas"
and modelTable="command";
```

---

## é¢„è®­ç»ƒ

åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä¼šè®²è§£å¦‚ä½•åˆ©ç”¨ Byzer-llm å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€‚ä¸è¿‡ç›®å‰æ¥çœ‹ï¼ŒByzer-SQL ä¸­çš„é¢„è®­ç»ƒåŠŸèƒ½æ›´ä¸ºæˆç†Ÿï¼Œå› æ­¤æˆ‘ä»¬å°†èšç„¦äºåœ¨ Byzer-SQL ä¸­å±•ç¤ºé¢„è®­ç»ƒè¿™ä¸€åŠŸèƒ½ã€‚

```sql
-- Deepspeed Config
set ds_config='''
{
  "gradient_accumulation_steps": 1,
  "train_micro_batch_size_per_gpu": 1,
  "prescale_gradients": false,
  "zero_allow_untested_optimizer": true,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 1e-8,
      "eps": 1.0e-8,
      "betas": [
        0.9,
        0.95
      ],
      "weight_decay": 0.1
    }
  },
  "tensorboard": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 3,
    "offload_optimizer": {
         "device": "cpu"         
     },           
    "offload_param": {
         "device": "cpu"
    },
    "contiguous_gradients": true,
    "allgather_bucket_size": 1e8,
    "reduce_bucket_size": 1e8,
    "overlap_comm": true,
    "reduce_scatter": true
  },
  "steps_per_print": 16,
  "gradient_clipping": 1.0,
  "wall_clock_breakdown": true,
  "bf16": {
    "enabled": true
  }
}
''';

-- load data
load text.`file:///home/byzerllm/data/raw_data/*`
where wholetext="true" as trainData;

select value as text,file from trainData  as newTrainData;

-- split the data into 12 partitions
run newTrainData as TableRepartition.`` where partitionNum="12" and partitionCols="file" 
as finalTrainData;


-- setup env, we use 12 gpus to pretrain the model
!byzerllm setup sfft;
!byzerllm setup "num_gpus=12";

-- specify the pretrain model type and the pretrained model path
run command as LLM.`` where 
and localPathPrefix="/home/byzerllm/models/sfft/jobs"
and pretrainedModelType="sfft/llama2"
-- original model is from
and localModelDir="/home/byzerllm/models/Llama-2-7b-chat-hf"
-- and localDataDir="/home/byzerllm/data/raw_data"

-- we use async mode to pretrain the model, since the pretrain process will take several days or weeks
-- Ray Dashboard will show the tensorboard address, and then you can monitor the loss
and detached="true"
and keepPartitionNum="true"

-- use deepspeed config, this is optional
and deepspeedConfig='''${ds_config}'''


-- the pretrain data is from finalTrainData table
and inputTable="finalTrainData"
and outputTable="llama2_cn"
and model="command"
-- some hyper parameters
and `sfft.int.max_length`="128"
and `sfft.bool.setup_nccl_socket_ifname_by_ip`="true"
;
```

å› ä¸ºæ·±é€Ÿè®­ç»ƒä¿å­˜çš„æ¨¡å‹æ–‡ä»¶æ ¼å¼ä¸æ‹¥æŠ±è„¸ä¹¦æ‰€ä½¿ç”¨çš„æ¨¡å‹æ–‡ä»¶æ ¼å¼äº’ä¸å…¼å®¹ï¼Œæˆ‘ä»¬å¿…é¡»å°†æ·±é€Ÿæ¨¡å‹æ–‡ä»¶è½¬æ¢ä¸ºæ‹¥æŠ±è„¸ä¹¦èƒ½å¤Ÿè¯†åˆ«çš„æ ¼å¼ã€‚ä¸‹é¢è¿™æ®µä»£ç å°±æ˜¯ç”¨æ¥å®ç°è¿™ä¸€è½¬æ¢ä»»åŠ¡çš„ã€‚

```sql
!byzerllm setup single;

run command as LLM.`` where 
action="convert"
and pretrainedModelType="deepspeed/llama3b"
and modelNameOrPath="/home/byzerllm/models/base_model"
and checkpointDir="/home/byzerllm/data/checkpoints"
and tag="Epoch-1"
and savePath="/home/byzerllm/models/my_3b_test2";
```

ç°åœ¨ï¼Œä½ å·²ç»å¯ä»¥é¡ºåˆ©éƒ¨ç½²ç»è¿‡è½¬æ¢çš„æ¨¡å‹ï¼Œå°†å…¶æŠ•å…¥å®é™…åº”ç”¨äº†ï¼š

```sql
-- éƒ¨ç½²hugginface æ¨¡å‹
!byzerllm setup single;

set node="master";
!byzerllm setup "num_gpus=2";
!byzerllm setup "workerMaxConcurrency=1";

run command as LLM.`` where 
action="infer"
and pretrainedModelType="custom/auto"
and localModelDir="/home/byzerllm/models/my_3b_test2"
and reconnect="false"
and udfName="my_3b_chat"
and modelTable="command";
```

## å¾®è°ƒ

```sql
-- load data, we use the dummy data for finetune
-- data format supported by Byzer-SQLï¼šhttps://docs.byzer.org/#/byzer-lang/zh-cn/byzer-llm/model-sft

load json.`/tmp/upload/dummy_data.jsonl` where
inferSchema="true"
as sft_data;

-- Fintune Llama2
!byzerllm setup sft;
!byzerllm setup "num_gpus=4";

run command as LLM.`` where 
and localPathPrefix="/home/byzerllm/models/sft/jobs"

-- æŒ‡å®šæ¨¡å‹ç±»å‹
and pretrainedModelType="sft/llama2"

-- æŒ‡å®šæ¨¡å‹
and localModelDir="/home/byzerllm/models/Llama-2-7b-chat-hf"
and model="command"

-- æŒ‡å®šå¾®è°ƒæ•°æ®è¡¨
and inputTable="sft_data"

-- è¾“å‡ºæ–°æ¨¡å‹è¡¨
and outputTable="llama2_300"

-- å¾®è°ƒå‚æ•°
and  detached="true"
and `sft.int.max_seq_length`="512";
```

ä½ å¯ä»¥åœ¨Rayä»ªè¡¨æ¿ä¸­æŸ¥çœ‹å¾®è°ƒä½œä¸šè¿›ç¨‹ï¼ˆfinetune actorï¼‰ï¼Œå…¶åç§°é€šå¸¸æ˜¯ `sft-william-xxxxx`ã€‚

å¾…å¾®è°ƒä½œä¸šå®Œæˆä¹‹åï¼Œä½ å¯ä»¥è·å–åˆ°æ¨¡å‹è·¯å¾„ï¼Œè¿™æ ·å°±å¯ä»¥éƒ¨ç½²è¿™ä¸ªç»è¿‡å¾®è°ƒçš„æ¨¡å‹äº†ã€‚

ä»¥ä¸‹æ˜¯å¾®è°ƒä½œä¸šè¿›ç¨‹çš„æ—¥å¿—è®°å½•ï¼š

```
Loading data: /home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_data/data.jsonl3
2
there are 33 data in dataset
*** starting training ***
{'train_runtime': 19.0203, 'train_samples_per_second': 1.735, 'train_steps_per_second': 0.105, 'train_loss': 3.0778136253356934, 'epoch': 0.97}35

***** train metrics *****36  
epoch                    =       0.9737  
train_loss               =     3.077838  
train_runtime            = 0:00:19.0239  
train_samples_per_second =      1.73540  
train_steps_per_second   =      0.10541

[sft-william] Copy /home/byzerllm/models/Llama-2-7b-chat-hf to /home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_model/final/pretrained_model4243              
[sft-william] Train Actor is already finished. You can check the model in: /home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_model/final   
```

ä½ å¯ä»¥ä»è·¯å¾„ `/home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_model/final` ä¸‹è½½å·²å®Œæˆå¾®è°ƒçš„æ¨¡å‹ï¼Œæˆ–è€…å°†æ¨¡å‹å¤åˆ¶åˆ°Rayé›†ç¾¤ä¸­çš„æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ä¸Šã€‚

ç°åœ¨ï¼Œå°è¯•éƒ¨ç½²è¿™ä¸ªç»è¿‡å¾®è°ƒçš„æ¨¡å‹ï¼š

```sql
!byzerllm setup single;
run command as LLM.`` where 
action="infer"
and localPathPrefix="/home/byzerllm/models/infer/jobs"
and localModelDir="/home/byzerllm/models/sft/jobs/sft-william-llama2-alpaca-data-ccb8fb55-382c-49fb-af04-5cbb3966c4e6/finetune_model/final"
and pretrainedModelType="custom/llama2"
and udfName="fintune_llama2_chat"
and modelTable="command";
```

Byzer-LLM åˆ©ç”¨ QLora å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç å°†å¾®è°ƒåçš„æ¨¡å‹ä¸åŸå§‹æ¨¡å‹è¿›è¡Œåˆå¹¶ï¼š

```sql
-- åˆå¹¶lora model + base model

!byzerllm setup single;

run command as LLM.`` where 
action="convert"
and pretrainedModelType="deepspeed/llama"
and model_dir="/home/byzerllm/models/sft/jobs/sft-william-20230912-21-50-10-2529bf9f-493e-40a3-b20f-0369bd01d75d/finetune_model/final/pretrained_model"
and checkpoint_dir="/home/byzerllm/models/sft/jobs/sft-william-20230912-21-50-10-2529bf9f-493e-40a3-b20f-0369bd01d75d/finetune_model/final"
and savePath="/home/byzerllm/models/sft/jobs/sft-william-20230912-21-50-10-2529bf9f-493e-40a3-b20f-0369bd01d75d/finetune_model/merge";

```

## æ–‡ç« 

1. [ä¸€å£æ°”é›†æˆé‚£äº›ä¸ªå¤§æ¨¡å‹ä½ ä¹Ÿè¯•è¯•](https://www.51xpage.com/ai/yi-kou-qi-ji-cheng-na-xie-ge-da-mo-xing-ni-ye-shi-shi-unknown-unknown-man-man-xue-ai006/)
2. [Byzer-LLM å¿«é€Ÿä½“éªŒæ™ºè°± GLM-4](https://mp.weixin.qq.com/s/Zhzn_C9-dKP4Nq49h8yUxw)
3. [å‡½æ•°å®ç°è¶Šé€šç”¨è¶Šå¥½ï¼Ÿæ¥çœ‹çœ‹ Byzer-LLM çš„ Function Implementation å¸¦æ¥çš„ç¼–ç¨‹æ€æƒ³å¤§å˜åŒ–](https://mp.weixin.qq.com/s/_Sx0eC0WqC2M4K1JY9f49Q)
4. [Byzer-LLM ä¹‹ QWen-VL-Chat/StableDiffusionå¤šæ¨¡æ€è¯»å›¾ï¼Œç”Ÿå›¾](https://mp.weixin.qq.com/s/x4g66QvocE5dUlnL1yF9Dw)
5. [åŸºäºByzer-Agent æ¡†æ¶å¼€å‘æ™ºèƒ½æ•°æ®åˆ†æå·¥å…·](https://mp.weixin.qq.com/s/BcoHUEXF24wTjArc7mwNaw)
6. [Byzer-LLM æ”¯æŒåŒæ—¶å¼€æºå’ŒSaaSç‰ˆé€šä¹‰åƒé—®](https://mp.weixin.qq.com/s/VvzMUV654D7IO0He47nv3A)
7. [ç»™å¼€æºå¤§æ¨¡å‹å¸¦æ¥Function Callingã€ Respond With Class](https://mp.weixin.qq.com/s/GTVCYUhR_atYMX9ymp0eCg)







