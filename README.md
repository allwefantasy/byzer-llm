<p align="center">
  <picture>    
    <img alt="Byzer-LLM" src="https://github.com/allwefantasy/byzer-llm/blob/master/docs/source/assets/logos/logo.jpg" width=55%>
  </picture>
</p>

<h3 align="center">
Easy, fast, and cheap pretrain,finetune, serving for everyone
</h3>

<p align="center">
| <a href="#"><b>Documentation</b></a> | <a href="#"><b>Blog</b></a> | | <a href="#"><b>Discord</b></a> |

</p>

---

*Latest News* ğŸ”¥

- [2024/01] Release Byzer-LLM 0.1.33
- [2023/12] Release Byzer-LLM 0.1.30

---

Byzer-LLM is Ray based , a full lifecycle solution for LLM that includes pretrain, fintune, deployment and serving.

The unique features of Byzer-LLM are:

1. Full lifecyle: pretrain and finetune,deploy and serving support
2. Python/SQL API support
3. Ray based, easy to scale

---

* [Versions](#Versions)
* [Installation](#Installation)
* [Quick Start](#Quick-Start)
* [Embedding Model](#Embedding-Model)
* [Quatization](#Quatization)
* [Supported Models](#Supported-Models)
* [vLLM Support](#vLLM-Support)
* [DeepSpeed Support](#DeepSpeed-Support)
* [Function Calling](#Function-Calling)
* [Respond with pydantic class](#Respond-with-pydantic-class)
* [Function Implementation](#Function-Implementation)
* [LLM-Friendly Function/DataClass](#LLM-Friendly-Function/DataClass)
* [Model Meta](#Model-Meta)
* [Chat Template](#Chat-Template)
* [LLM Default Generation Parameters](#LLM-Default-Generation-Parameters)
* [SaaS Models](#SaaS-Models)
* [Multi Modal](#Multi-Modal)
* [StableDiffusion](#StableDiffusion)
* [SQL Support](#SQL-Support)
* [Pretrain](#Pretrain)
* [Finetune](#Finetune)
* [Stream Chat](#Stream-Chat)
* [Contributing](#Contributing)

---

## Versions
- 0.1.33ï¼š Fix Response Class bugs/ Add function implementation
- 0.1.32ï¼š StableDiffusion optimization
- 0.1.31ï¼š Stream Chat with token count information / Optimize multi modal model chat
- 0.1.30ï¼š Apply chat template for vLLM backend
- 0.1.29ï¼š Enhance DataAnalysis Agent
- 0.1.28ï¼š Bug fix
- 0.1.27ï¼š Bug fix
- 0.1.26ï¼š Support QianWen Saas/ Support stream chat in QianWenSaas/ Fix some Saas model bugs
- 0.1.24ï¼š Support get meta from model instance and auto setup template
- 0.1.23ï¼š Fintune with python API/ Fix some bugs
- 0.1.22ï¼š Function Calling support/ Response with pydantic class
- 0.1.19ï¼š Fix embedding bugs
- 0.1.18ï¼š Support stream chat/ Support Model Template
- 0.1.17ï¼š None
- 0.1.16ï¼š Enhance the API for byzer-retrieval
- 0.1.14ï¼š add get_tables/get_databases API for byzer-retrieval
- 0.1.13: support shutdown cluster for byzer-retrieval
- 0.1.12: Support Python API (alpha)
- 0.1.5:  Support python wrapper for [byzer-retrieval](https://github.com/allwefantasy/byzer-retrieval)

---

## Installation

```bash
pip install -r requirements.txt
pip install -U vllm
pip install -U byzerllm
ray start --head
```

---

## Quick Start

```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend

ray.init(address="auto",namespace="default",ignore_reinit_error=True)

llm = ByzerLLM()

llm.setup_gpus_per_worker(4).setup_num_workers(1)
llm.setup_infer_backend(InferBackend.transformers)

llm.deploy(model_path="/home/byzerllm/models/openbuddy-llama2-13b64k-v15",
           pretrained_model_type="custom/llama2",
           udf_name="llama2_chat",infer_params={})

llm.chat("llama2_chat",LLMRequest(instruction="hello world"))[0].output
```

The above code will deploy a llama2 model and then use the model to infer the input text. If you use transformers as the inference backend, you should specify the `pretrained_model_type` manually since the transformers backend can not auto detect the model type.

Byzer-LLM also support `deploy` SaaS model with the same way. This feature provide a unified interface for both open-source model and SaaS model. The following code will deploy a Azure OpenAI model and then use the model to infer the input text.


```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend
ray.init(address="auto",namespace="default",ignore_reinit_error=True)

llm = ByzerLLM()

llm.setup_gpus_per_worker(0).setup_num_workers(10)
llm.setup_infer_backend(InferBackend.transformers)

llm.deploy(pretrained_model_type="saas/azure_openai",
           udf_name="azure_openai",
           infer_params={
            "saas.api_type":"azure",
            "saas.api_key"="xxx"
            "saas.api_base"="xxx"
            "saas.api_version"="2023-07-01-preview"
            "saas.deployment_id"="xxxxxx"
           })

llm.chat("azure_openai",LLMRequest(instruction="hello world"))[0].output
```

Notice that the SaaS model does not need GPU, so we set the `setup_gpus_per_worker` to 0, and you can use `setup_num_workers`
to control max concurrency,how ever, the SaaS model has its own max concurrency limit, the `setup_num_workers` only control the max
concurrency accepted by the Byzer-LLM.

## Embedding Model

The following code is a example of deploying BGE embedding model

```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend
ray.init(address="auto",namespace="default",ignore_reinit_error=True)
llm = ByzerLLM()

llm.setup_gpus_per_worker(0.4).setup_num_workers(2).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path="/home/byzerllm/models/bge-large-zh",
    pretrained_model_type="custom/bge",
    udf_name="emb",
    infer_params={}
)   
```

Then you can convert any text to vector :

```python
t = llm.emb("emb",LLMRequest(instruction="wow"))
t[0].output
#output: [-0.005588463973253965,
 -0.01747054047882557,
 -0.040633779019117355,
...
 -0.010880181565880775,
 -0.01713103987276554,
 0.017675869166851044,
 -0.010260719805955887,
 ...]
```

## Quatization

If the backend is `InferBackend.transformers`, here is the baichuan2 example:

```python
llm.setup_gpus_per_worker(2).setup_num_workers(1).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/baichuan2",
    udf_name="baichuan2_13_chat",
    infer_params={"quatization":"4"}
)
```
The available `quatization` values:

1. 4
2. 8
3. true/false

When it's set true, the int4 will be choosed.

If the bakcend is `InferBackend.VLLM`, here is the Yi example:

If you need to deploy model with Quantization, you can set the `infer_params` as the following code:

```python
llm.setup_gpus_per_worker(1).setup_num_workers(1).setup_infer_backend(InferBackend.VLLM)
llm.deploy(
    model_path="/home/winubuntu/models/Yi-6B-Chat-4bits",
    pretrained_model_type="custom/auto",
    udf_name="chat",
    infer_params={"backend.quantization":"AWQ"}
)
```

The parameter `backend.quantization` can be GPTQ/AWQ.


## Supported Models

The supported open-source `pretrained_model_type` are:

1. custom/llama2
2. bark	
3. whisper	
3. chatglm6b
4. custom/chatglm2
5. moss
6. custom/alpha_moss
7. dolly
8. falcon
9. llama
10. custom/starcode
11. custom/visualglm
12. custom/m3e
13. custom/baichuan
14. custom/bge
15. custom/qwen_vl_chat
16. custom/stable_diffusion
17. custom/zephyr

The supported SaaS `pretrained_model_type` are:

1. saas/chatglm	Chatglm130B
2. saas/sparkdesk	æ˜Ÿç«å¤§æ¨¡å‹
3. saas/baichuan	ç™¾å·å¤§æ¨¡å‹
4. saas/zhipu	æ™ºè°±å¤§æ¨¡å‹
5. saas/minimax	MiniMax å¤§æ¨¡å‹
6. saas/qianfan	æ–‡å¿ƒä¸€è¨€
7. saas/azure_openai	
8. saas/openai

Notice that the derived models from llama/llama2/startcode are also supported. For example, you can use `llama` to load vicuna model.

## vLLM Support

The Byzer-llm also support vLLM as the inference backend. The following code will deploy a vLLM model and then use the model to infer the input text.

```python
import ray
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend

ray.init(address="auto",namespace="default",ignore_reinit_error=True)
llm = ByzerLLM()

llm.setup_gpus_per_worker(2)
llm.setup_num_workers(1)
llm.setup_infer_backend(InferBackend.VLLM)

llm.deploy(
    model_path="/home/byzerllm/models/openbuddy-zephyr-7b-v14.1",
    pretrained_model_type="custom/auto",
    udf_name="zephyr_chat"",
    infer_params={}
)

llm.chat("zephyr_chat",LLMRequest(instruction="hello world"))[0].output
```

There are some tiny differences between the vLLM and the transformers backend. 

1. The `pretrained_model_type` is fixed to `custom/auto` for vLLM, since the vLLM will auto detect the model type.
2. Use `setup_infer_backend` to specify `InferBackend.VLLM` as the inference backend.
 

### Stream Chat

If the model deployed with the backend vLLM, then it also support `stream chat`ï¼š
the `stream_chat_oai` will return a generator, you can use the generator to get the output text.

```python

llm.setup_default_model_name(chat_model_name) 

t = llm.stream_chat_oai(conversations=[{
    "role":"user",
    "content":"Hello, how are you?"
}])

for line in t:
   print(line+"\n")
```

## DeepSpeed Support

The Byzer-llm also support DeepSpeed as the inference backend. The following code will deploy a DeepSpeed model and then use the model to infer the input text.

```python
import ray
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend

ray.init(address="auto",namespace="default",ignore_reinit_error=True)
llm = ByzerLLM()

llm.setup_gpus_per_worker(4)
llm.setup_num_workers(1)
llm.setup_infer_backend(InferBackend.DeepSpeed)

llm.deploy(
    model_path="/home/byzerllm/models/openbuddy-llama-13b-v5-fp16",
    pretrained_model_type="custom/auto",
    udf_name="llama_chat"",
    infer_params={}
)

llm.chat("llama_chat",LLMRequest(instruction="hello world"))[0].output
```

The code above is totally the same as the code for vLLM, except that the `InferBackend` is `InferBackend.DeepSpeed`.


## Function Calling

Here is a simple example for function calling based on QWen 72B

Deploy Model:


```python
import ray
ray.init(address="auto",namespace="default") 
llm = ByzerLLM()

model_location="/home/byzerllm/models/Qwen-72B-Chat"

llm.setup_gpus_per_worker(8).setup_num_workers(1).setup_infer_backend(InferBackend.VLLM)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/auto",
    udf_name=chat_model_name,
    infer_params={}
)

llm.setup_default_model_name("chat")
# from 0.1.24 
# llm.setup_auto("chat")
meta = llm.get_meta()
llm.setup_max_model_length("chat",meta.get("max_model_len",32000))
lm.setup_template("chat",Templates.qwen()) 
```

Try to create some Python functions:

```python

from typing import List,Dict,Any,Annotated
import pydantic 
import datetime
from dateutil.relativedelta import relativedelta

def compute_date_range(count:Annotated[int,"æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»å‹"],
                       unit:Annotated[str,"æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»å‹",{"enum":["day","week","month","year"]}])->List[str]:
    '''
    è®¡ç®—æ—¥æœŸèŒƒå›´

    Args:
        count: æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»å‹
        unit: æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»å‹ï¼Œå¯é€‰å€¼ä¸º day,week,month,year
    '''        
    now = datetime.datetime.now()
    now_str = now.strftime("%Y-%m-%d %H:%M:%S")
    if unit == "day":
        return [(now - relativedelta(days=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    elif unit == "week":
        return [(now - relativedelta(weeks=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    elif unit == "month":
        return [(now - relativedelta(months=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    elif unit == "year":
        return [(now - relativedelta(years=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    return ["",""]

def compute_now()->str:
    '''
    è®¡ç®—å½“å‰æ—¶é—´
    '''
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
```

Here we provide two functions:

1. compute_date_range: compute the date range based on the count and unit
2. compute_now: get the current date

We will use the model to call these tools according to the user's question.

```python
t = llm.chat_oai([{
    "content":'''è®¡ç®—å½“å‰æ—¶é—´''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

t[0].values

## output: ['2023-12-18 17:30:49']
```

```python
t = llm.chat_oai([{
    "content":'''æœ€è¿‘ä¸‰ä¸ªæœˆè¶‹åŠ¿''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

t[0].values

## output: [['2023-09-18 17:31:21', '2023-12-18 17:31:21']]
```

```python
t = llm.chat_oai([{
    "content":'''æœ€è¿‘ä¸‰å¤©''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

t[0].values

## output: [['2023-12-15 17:23:38', '2023-12-18 17:23:38']]
```

```python
t = llm.chat_oai([{
    "content":'''ä½ åƒé¥­äº†ä¹ˆï¼Ÿ''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

if t[0].values:
    print(t[0].values[0])
else:
    print(t[0].response.output)   

## output: 'æ‚¨å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½è¯­è¨€æ¨¡å‹ï¼Œæš‚æ—¶æ— æ³•åƒé¥­ã€‚'
```

You can check the default prompt template function in `from byzerllm.utils import function_calling_format`.
If the model is not work well with the default function, you can setup your custom function:

```python
def custom_function_calling_format(prompt:str,tools:List[Callable],tool_choice:Callable)->str:
.....


llm.setup_function_calling_format_func("chat",custom_function_calling_format)
```

## Respond with pydantic class

When you chat with LLM, you can specify the reponse class, 

```python
import pydantic 

class Story(pydantic.BaseModel):
    '''
    æ•…äº‹
    '''

    title: str = pydantic.Field(description="æ•…äº‹çš„æ ‡é¢˜")
    body: str = pydantic.Field(description="æ•…äº‹ä¸»ä½“")



t = llm.chat_oai([
{
    "content":f'''è¯·ç»™æˆ‘è®²ä¸ªæ•…äº‹ï¼Œåˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ ‡é¢˜ï¼Œä¸€ä¸ªæ•…äº‹ä¸»ä½“''',
    "role":"user"
},
],response_class=Story)

t[0].value

## output: Story(title='å‹‡æ•¢çš„å°å…”å­', body='åœ¨ä¸€ä¸ªç¾ä¸½çš„æ£®æ—é‡Œï¼Œä½ç€ä¸€åªå¯çˆ±çš„å°å…”å­ã€‚å°å…”å­éå¸¸å‹‡æ•¢ï¼Œæœ‰ä¸€å¤©ï¼Œæ£®æ—é‡Œçš„åŠ¨ç‰©ä»¬éƒ½è¢«å¤§ç°ç‹¼å“åäº†ã€‚åªæœ‰å°å…”å­ç«™å‡ºæ¥ï¼Œç”¨æ™ºæ…§å’Œå‹‡æ°”æ‰“è´¥äº†å¤§ç°ç‹¼ï¼Œä¿æŠ¤äº†æ‰€æœ‰çš„åŠ¨ç‰©ã€‚ä»æ­¤ï¼Œå°å…”å­æˆä¸ºäº†æ£®æ—é‡Œçš„è‹±é›„ã€‚')
```

The above code will ask the LLM to generate the Story class directly. However, sometimes we hope the LLM 
generate text first, then extract the structure from the text, you can set `response_after_chat=True` to 
enable this behavior. However, this will bring some performance penalty(additional inference).

```python
t = llm.chat_oai([
{
    "content":f'''è¯·ç»™æˆ‘è®²ä¸ªæ•…äº‹ï¼Œåˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ ‡é¢˜ï¼Œä¸€ä¸ªæ•…äº‹ä¸»ä½“''',
    "role":"user"
},
],response_class=Story,response_after_chat=True)

t[0].value
## output: Story(title='æœˆå…‰ä¸‹çš„å®ˆæŠ¤è€…', body='åœ¨ä¸€ä¸ªé¥è¿œçš„å¤è€æ‘åº„é‡Œï¼Œä½ç€ä¸€ä½åå«é˜¿æ˜çš„å¹´è½»äººã€‚é˜¿æ˜æ˜¯ä¸ªå­¤å„¿ï¼Œä»å°åœ¨æ‘é‡Œé•¿å¤§ï¼Œä»¥ç§ç”°ä¸ºç”Ÿã€‚ä»–å–„è‰¯ã€å‹¤åŠ³ï¼Œæ·±å—æ‘æ°‘ä»¬å–œçˆ±ã€‚\n\næ‘å­é‡Œæœ‰ä¸ªä¼ è¯´ï¼Œæ¯å½“æ»¡æœˆæ—¶åˆ†ï¼Œæœˆäº®å¥³ç¥ä¼šåœ¨æ‘å­åå±±çš„å¤æ ‘ä¸‹å‡ºç°ï¼Œèµç¦ç»™é‚£äº›å–„è‰¯çš„äººä»¬ã€‚ç„¶è€Œï¼Œåªæœ‰æœ€çº¯æ´çš„å¿ƒæ‰èƒ½çœ‹åˆ°å¥¹ã€‚å› æ­¤ï¼Œæ¯å¹´çš„è¿™ä¸ªæ—¶å€™ï¼Œé˜¿æ˜éƒ½ä¼šç‹¬è‡ªä¸€äººå‰å¾€åå±±ï¼Œå¸Œæœ›èƒ½å¾—åˆ°å¥³ç¥çš„ç¥ç¦ã€‚\n\nè¿™ä¸€å¹´ï¼Œæ‘å­é­å—äº†ä¸¥é‡çš„æ—±ç¾ï¼Œåº„ç¨¼æ¯é»„ï¼Œäººä»¬ç”Ÿæ´»å›°è‹¦ã€‚é˜¿æ˜å†³å®šå‘æœˆäº®å¥³ç¥ç¥ˆæ±‚é™é›¨ï¼Œæ‹¯æ•‘æ‘å­ã€‚ä»–åœ¨æœˆå…‰ä¸‹è™”è¯šåœ°ç¥ˆç¥·ï¼Œå¸Œæœ›å¥³ç¥èƒ½å¬åˆ°ä»–çš„å‘¼å”¤ã€‚\n\nå°±åœ¨è¿™ä¸ªæ—¶åˆ»ï¼Œæœˆäº®å¥³ç¥å‡ºç°äº†ã€‚å¥¹è¢«é˜¿æ˜çš„å–„è‰¯å’Œæ‰§ç€æ‰€æ„ŸåŠ¨ï¼Œç­”åº”äº†ä»–çš„è¯·æ±‚ã€‚ç¬¬äºŒå¤©æ—©æ™¨ï¼Œå¤©ç©ºä¹Œäº‘å¯†å¸ƒï¼Œå¤§é›¨å€¾ç›†è€Œä¸‹ï¼Œä¹…æ—±çš„åœŸåœ°å¾—åˆ°äº†æ»‹æ¶¦ï¼Œåº„ç¨¼é‡æ–°ç„•å‘ç”Ÿæœºã€‚\n\nä»æ­¤ä»¥åï¼Œæ¯å¹´çš„æ»¡æœˆä¹‹å¤œï¼Œé˜¿æ˜éƒ½ä¼šå»åå±±ç­‰å¾…æœˆäº®å¥³ç¥çš„å‡ºç°ï¼Œä»–æˆä¸ºäº†æ‘æ°‘å¿ƒä¸­çš„å®ˆæŠ¤è€…ï¼Œç”¨ä»–çš„å–„è‰¯å’Œæ‰§ç€ï¼Œå®ˆæŠ¤ç€æ•´ä¸ªæ‘åº„ã€‚è€Œä»–ä¹Ÿç»ˆäºæ˜ç™½ï¼ŒçœŸæ­£çš„å®ˆæŠ¤è€…ï¼Œå¹¶ééœ€è¦è¶…å‡¡çš„åŠ›é‡ï¼Œåªéœ€è¦ä¸€é¢—å……æ»¡çˆ±ä¸å–„è‰¯çš„å¿ƒã€‚')
```

You can check the default prompt template function in `from byzerllm.utils import response_class_format,response_class_format_after_chat`.
If the model is not work well with the default function, you can setup your custom function:

```python
def custom_response_class_format(prompt:str,cls:pydantic.BaseModel)->str:
.....


llm.setup_response_class_format_func("chat",custom_response_class_format)
```

## Function Implementation

The Byzer-llm also support function implementation. You can define a empty function, and combine the doc in the function/the user's quesion to guide the LLM to implement the function. 

Here is a simple example:

```python
class TimeRange(pydantic.BaseModel):
    '''
    æ—¶é—´åŒºé—´
    æ ¼å¼éœ€è¦å¦‚ä¸‹ï¼š yyyy-MM-dd
    '''  
    
    start: str = pydantic.Field(...,description="å¼€å§‹æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")
    end: str = pydantic.Field(...,description="æˆªæ­¢æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")


def calculate_time_range():
    '''
    è®¡ç®—æ—¶é—´åŒºé—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd. 
    '''
    pass 
    
t = llm.chat_oai([{
    "content":"å»å¹´ä¸‰æœˆåˆ°ä¸ƒæœˆ",
    "role":"user"    
}],impl_func=calculate_time_range,response_class=TimeRange,execute_impl_func=True)
```

The above code , we define a function called `calculate_time_range`, and the function is empty, then we discribe the function in the doc string, and define the response class `TimeRange`, to make sure the return value is a `TimeRange` instance. Since the function should be used to resolve the user's question, so the implementation of the function should be related to the user's question. Instead try to implement a common use function, we can just implement a function which can only resolve the user's current question.

After the execution, you can get the output like this:

```python
t[0].value
# start='2023-03-01' end='2023-07-31'
```

If the value is None or not correct, you can get the error message:

```python
t[0].metadata.get("resason","")
```

If your function has parameters, you can pass the parameters to the function by `impl_func_params`:

```python
t = llm.chat_oai([{
    "content":"xxxxx",
    "role":"user"    
}],
impl_func=calculate_time_range,
impl_func_params={},
response_class=TimeRange,execute_impl_func=True)
```

If you want to replace the default prompt template function, here is a example:

```python
import pydantic
from typing import List,Optional,Union,Callable
from byzerllm.utils import serialize_function_to_json

def function_impl_format2(prompt:str,func:Optional[Union[Callable,str]],
                             cls:Union[pydantic.BaseModel,str])->str:
    
    tool_choice_ser = serialize_function_to_json(func)    
    _cls = ""
    if isinstance(cls, str):
        _cls = cls
    else:
        _cls = cls.schema_json(ensure_ascii=False)
    
    msg = f''''ç”Ÿæˆä¸€ä¸ªpythonå‡½æ•°ï¼Œç»™å‡ºè¯¦ç»†çš„æ€è€ƒé€»è¾‘ï¼Œå¯¹æœ€åç”Ÿæˆçš„å‡½æ•°ä¸è¦è¿›è¡Œç¤ºä¾‹è¯´æ˜ã€‚

ç”Ÿæˆçš„å‡½æ•°çš„åå­—ä»¥åŠå‚æ•°éœ€è¦æ»¡è¶³å¦‚ä¸‹çº¦æŸï¼š

\```json
{tool_choice_ser}
\```

ç”Ÿæˆçš„å‡½æ•°çš„è¿”å›å€¼å¿…é¡»æ˜¯ Json æ ¼å¼ã€‚

ä¸‹é¢æ˜¯ä½¿ç”¨ OpenAPI 3.1. è§„èŒƒæè¿°äº†ä½ éœ€å¦‚ä½•è¿›è¡Œjsonæ ¼å¼çš„ç”Ÿæˆã€‚

\```json
{_cls}
\```

æ ¹æ®ç”¨çš„æˆ·é—®é¢˜,{func.__doc__}ã€‚ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š{prompt}

è¯·ä½ å®ç°è¿™ä¸ªå‡½æ•°ã€‚
''' 
    
    return msg

llm.setup_impl_func_format_func(chat_model_name,function_impl_format2)
```

The default prompt template function is `function_impl_format`, you can check the source code in `from byzerllm.utils import function_impl_format`.


## LLM-Friendly Function/DataClass

If you want to improve the performance of Function Calling or Response Class, you should make your Function(Tool) and Data Class is LLM-Friendly.  

Let's take a look at the following python code:

```python
def compute_date_range(count:int, unit:str)->List[str]:                   
    now = datetime.datetime.now()
    ....
```

This code is not LLM-Friendly Function since it's difficult to know the usage of this funciton and 
what's the meaning of the input parameters.

The LLM just like human, it's hard to let the LLM know when or how to invoke this function. Especially the parameter `unit`
actually is enum value but the LLM no way to get this message.

So, in order to make the LLM knows more about this function in Byzer-LLM, you should 
follow some requirments:

1. Adding pythonic function comment 
2. Use annotated to provide type and comment for every parameter, if the parameter is a enum, then provide enum values.

Here is the LLM-Friendly fuction definision.

```python
def compute_date_range(count:Annotated[int,"æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»å‹"],
                       unit:Annotated[str,"æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»å‹",{"enum":["day","week","month","year"]}])->List[str]:
    '''
    è®¡ç®—æ—¥æœŸèŒƒå›´

    Args:
        count: æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»å‹
        unit: æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»å‹ï¼Œå¯é€‰å€¼ä¸º day,week,month,year
    '''        
    now = datetime.datetime.now()
    ....
```

If the LLM make something wrong to your function (e.g. provide the bad parameters), try to optimize the function comment 
and the parameter Annotated comment.

## Model Meta

The Byzer-llm also support get the model meta information. The following code will get the meta information of model instance called `chat`:

```python

```python
llm.get_meta(model="chat")

#output:
# {'model_deploy_type': 'proprietary',
#  'backend': 'ray/vllm',
#  'max_model_len': 32768,
#  'architectures': ['QWenLMHeadModel']}
```

## Chat Template

The different models have different chat templates, the Byzer-LLM have provide some chat templates for the models. You can use the following code to setup the chat template:

```python
from byzerllm.utils.client import Templates
llm.setup_template("chat",Templates.qwen()) 
```

However, we also support the `tokeninzer.apply_chat_template`, you can use the following code to apply the chat template:

```python
llm.setup_template("chat","auto") 
```

If the model is not work well with the `tokeninzer.apply_chat_template`, this function will raise an exception. In this case, you can use the `llm.setup_template` to setup the chat template manually.

You can also use the `llm.get_meta` to check if the model support the `apply_chat_template`:

```python
llm.get_meta(model="chat")
```

The output:

```json
{'model_deploy_type': 'proprietary',
 'backend': 'ray/vllm',
 'support_stream': True,
 'support_chat_template': True,
 'max_model_len': 4096,
 'architectures': ['LlamaForCausalLM']}
```

Notice that this feature will cause additional RPC call, so it will bring some performance penalty.

## LLM Default Generation Parameters

The Byzer-llm also support setup the default generation parameters for the model. The following code will setup the default generation parameters for the model instance called `chat`:

```python
llm.setup_extra_generation_params("chat",{
    "generation.stop_token_ids":[7]
})
```

In this case, the `generation.stop_token_ids` will be set to `[7]` for the model instance `chat`. Every time you call the `chat` model, the `generation.stop_token_ids` will be set to `[7]` automatically.

## Multi Modal 

The Byzer-llm also support multi modal. The following code will deploy a multi modal model and then use the model to infer the input text.

```python
import ray
from byzerllm.utils.client import ByzerLLM,InferBackend

ray.init(address="auto",namespace="default")   

llm = ByzerLLM()
chat_model_name = "qwen_vl_chat"
model_location = "/home/byzerllm/models/Qwen-VL-Chat"

llm.setup_gpus_per_worker(1).setup_num_workers(1).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/qwen_vl_chat",
    udf_name=chat_model_name,
    infer_params={}
)    
```

Then you can use the model to chat:

```python
import base64
image_path = "/home/byzerllm/projects/jupyter-workspace/1.jpg"
with open(image_path, "rb") as f:
    image_content = base64.b64encode(f.read()).decode("utf-8")

t = llm.chat_oai(conversations=[{
    "role": "user",
    "content": "è¿™æ˜¯ä»€ä¹ˆ"
}],model=chat_model_name,llm_config={"image":image_content})

t[0].output

# '{"response": "å›¾ä¸­æ˜¯ä¸€åå¥³å­åœ¨æ²™æ»©ä¸Šå’Œç‹—ç©è€ï¼Œæ—è¾¹çš„ç‹—æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ï¼Œå®ƒååœ¨æ²™æ»©ä¸Šï¼Œé¢å¯¹ç€ä¸€åèº«ç©¿æ ¼å­è¡¬è¡«çš„å¥³å­ã€‚å¥³å­çš„è…¿æœ‰äº›æ®‹ç–¾ï¼Œä½†æ˜¯å¥¹ä¾ç„¶åšæŒååœ¨æ²™æ»©ä¸Šå’Œç‹—ç©è€ã€‚å¥¹çš„å³æ‰‹æ‹¿ç€ä¸€ä¸ªå°ç©å…·ï¼Œè¿™ä¸ªç©å…·ä¸Šé¢æœ‰ä¸¤è¡Œé»‘è‰²å­—æ¯ï¼Œå…·ä½“æ˜¯ä»€ä¹ˆå†…å®¹çœ‹ä¸æ¸…æ¥šã€‚å¥¹æ‰“ç®—æŠŠç©å…·æ‰”ç»™æ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚", "history": [{"role": "user", "content": "Picture 1: <img>/tmp/byzerllm/visualglm/images/23eb4cea-cb6e-4f55-8adf-3179ca92ab42.jpg</img>\\nè¿™æ˜¯ä»€ä¹ˆ"}, {"role": "assistant", "content": "å›¾ä¸­æ˜¯ä¸€åå¥³å­åœ¨æ²™æ»©ä¸Šå’Œç‹—ç©è€ï¼Œæ—è¾¹çš„ç‹—æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ï¼Œå®ƒååœ¨æ²™æ»©ä¸Šï¼Œé¢å¯¹ç€ä¸€åèº«ç©¿æ ¼å­è¡¬è¡«çš„å¥³å­ã€‚å¥³å­çš„è…¿æœ‰äº›æ®‹ç–¾ï¼Œä½†æ˜¯å¥¹ä¾ç„¶åšæŒååœ¨æ²™æ»©ä¸Šå’Œç‹—ç©è€ã€‚å¥¹çš„å³æ‰‹æ‹¿ç€ä¸€ä¸ªå°ç©å…·ï¼Œè¿™ä¸ªç©å…·ä¸Šé¢æœ‰ä¸¤è¡Œé»‘è‰²å­—æ¯ï¼Œå…·ä½“æ˜¯ä»€ä¹ˆå†…å®¹çœ‹ä¸æ¸…æ¥šã€‚å¥¹æ‰“ç®—æŠŠç©å…·æ‰”ç»™æ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚"}]}'
```

You can chat multi rounds with the following code:

```python
import json
history = json.loads(t[0].output)["history"]

llm.chat_oai(conversations=history+[{
    "role": "user",
    "content": "èƒ½åœˆå‡ºç‹—ä¹ˆï¼Ÿ"
}],model=chat_model_name,llm_config={"image":image_content})

# [LLMResponse(output='{"response": "<ref>ç‹—</ref><box>(221,425),(511,889)</box>", "history": [{"role"
```

Get the history from the previous chat, then add the hisotry to new conversation, then chat again.

## StableDiffusion

The Byzer-llm also support StableDiffusion as the inference backend. The following code will deploy a StableDiffusion model and then use the model to infer the input text.

```python
import ray
from byzerllm.utils.client import ByzerLLM,InferBackend

ray.init(address="auto",namespace="default")   

llm = ByzerLLM()
chat_model_name = "sd_chat"
model_location = "/home/byzerllm/models/stable-diffusion-v1-5"

llm.setup_gpus_per_worker(2).setup_num_workers(1).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/stable_diffusion",
    udf_name=chat_model_name,
    infer_params={}
)

def show_image(content):
    from IPython.display import display, Image
    import base64             
    img = Image(base64.b64decode(content))
    display(img)    
    
```

Then you can chat with the model:

```python
import json
t = llm.chat_oai(
    conversations=[
        {
            "role":"user",
            "content":"ç”»ä¸€åªçŒ«"
        }
    ],model=chat_model_name,llm_config={"gen.batch_size":3}
)

cats = json.loads(t[0].output)
for res in cats:
    show_image(res["img64"])
```

The output:

![](./images/cat2.png)

The parameters:

| å‚æ•°                        | å«ä¹‰                                                         | é»˜è®¤å€¼   |
| --------------------------- | ------------------------------------------------------------ | -------- |
| Instruction                 | prompt                                                       | éç©º     |
| generation.negative_prompt  | åå‘çš„prompt                                                 | ""       |
| generation.sampler_name     | è°ƒåº¦å(unpic, euler_a,euler,ddim,ddpm,deis,dpm2,dpm2-a,dpm++_2m,dpm++_2m_karras,heun,heun_karras,lms,pndm:w) | euler_a  |
| generation.sampling_steps   | ç”Ÿæˆçš„æ­¥éª¤æ•°                                                 | 25       |
| generation.batch_size       | ä¸€æ¬¡ç”Ÿæˆå‡ å¼                                                  | 1        |
| generation.batch_count      | ç”Ÿæˆå‡ æ¬¡                                                     | 1        |
| generation.cfg_scale        | éšæœºæˆ–è´´åˆç¨‹åº¦å€¼,å€¼è¶Šå°ç”Ÿæˆçš„å›¾ç‰‡ç¦»ä½ çš„Tagsæè¿°çš„å†…å®¹å·®è·è¶Šå¤§ | 7.5      |
| generation.seed             | éšæœºç§å­                                                     | -1       |
| generation.width            | å›¾ç‰‡å®½åº¦                                                     | 768      |
| generation.height           | å›¾ç‰‡é«˜åº¦                                                     | 768      |
| generation.enable_hires     | å¼€å¯é«˜åˆ†è¾¨ç‡ä¿®å¤åŠŸèƒ½(å’Œä¸‹é¢ä¸¤ä¸ªä¸€ç»„)                         | false    |
| generation.upscaler_mode    | æ”¾å¤§ç®—æ³•(bilinear, bilinear-antialiased,bicubic,bicubic-antialiased,nearest,nearest-exact) | bilinear |
| generation.scale_slider     | æ”¾å¤§æ¯”ä¾‹                                                     | 1.5      |
| generation.enable_multidiff | å›¾ç‰‡åˆ†å‰²å¤„ç†(å‡å°‘æ˜¾å­˜é”€è€—)(å’Œä¸‹é¢3ä¸ªä¸€ç»„)                    | false    |
| generation.views_batch_size | åˆ†æ‰¹å¤„ç†è§„æ¨¡                                                 | 4        |
| generation.window_size      | åˆ‡å‰²å¤§å°ï¼Œå®½ï¼Œé«˜                                             | 64       |
| generation.stride           | æ­¥é•¿                                                         | 16       |
| generation.init_image       | åˆå§‹åŒ–å›¾ç‰‡ï¼ŒåŸºäºè¿™ä¸ªå›¾ç‰‡å¤„ç†(å¿…é¡»ä¼ è¾“base64åŠ å¯†çš„å›¾ç‰‡) (å’Œä¸‹é¢çš„ä¸€ç»„) | None     |
| generation.strength         | é‡ç»˜å¹…åº¦: å›¾åƒæ¨¡ä»¿è‡ªç”±åº¦ï¼Œè¶Šé«˜è¶Šè‡ªç”±å‘æŒ¥ï¼Œè¶Šä½å’Œå‚è€ƒå›¾åƒè¶Šæ¥è¿‘ï¼Œé€šå¸¸å°äº0.3åŸºæœ¬å°±æ˜¯åŠ æ»¤é•œ | 0.5      |



## SQL Support

In addition to the Python API, Byzer-llm also support SQL API. In order to use the SQL API, you should install Byzer-SQL language first.

Try to install the Byzer-SQL language with the following command:

```bash
git clone https://gitee.com/allwefantasy/byzer-llm
cd byzer-llm/setup-machine
sudo -i 
ROLE=master ./setup-machine.sh
```

After the installation, you can visit the Byzer Console at http://localhost:9002. 

In the Byzer Console, you can run the following SQL to deploy a llama2 model which have the same effect as the Python code above.

```sql
!byzerllm setup single;
!byzerllm setup "num_gpus=4";
!byzerllm setup "maxConcurrency=1";
!byzerllm setup "infer_backend=transformers";

run command as LLM.`` where 
action="infer"
and pretrainedModelType="custom/llama2"
and localModelDir="/home/byzerllm/models/openbuddy-llama-13b-v5-fp16"
and reconnect="false"
and udfName="llama2_chat"
and modelTable="command";

```

Then you can invoke the model with UDF `llama2_chat`:

```sql

select 
llama2_chat(llm_param(map(
              "user_role","User",
              "assistant_role","Assistant",
              "system_msg",'You are a helpful assistant. Think it over and answer the user question correctly.',
              "instruction",llm_prompt('
Please remenber my name: {0}              
',array("Zhu William"))

))) as q 
as q1;
```

Once you deploy the model with `run command as LLM`, then you can ues the model as a SQL function. This feature is very useful for data scientists who want to use LLM in their data analysis or data engineers who want to use LLM in their data pipeline.

---

### QWen

If you use QWen in ByzerLLM, you should sepcify the following parameters mannualy:

1. the role mapping 
2. the stop_token_ids
3. trim the stop tokens from the output

However, we provide a template for this, try to the following code:

```python
from byzerllm.utils.client import Templates

### Here,we setup the template for qwen
llm.setup_template("chat",Templates.qwen())

t = llm.chat_oai(conversations=[{
    "role":"user",
    "content":"ä½ å¥½,ç»™æˆ‘è®²ä¸ª100å­—çš„ç¬‘è¯å§?"
}])
print(t)
```

---
## SaaS Models

Since the different SaaS models have different parameters, here we provide some templates for the SaaS models to help you deploy the SaaS models.

Here is quick example with Python:

```python

import ray
from byzerllm.utils.client import ByzerLLM

ray.init(address="auto",namespace="default",ignore_reinit_error=True)   

llm = ByzerLLM(verbose=True)

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "baichuan_chat2"
if llm.is_model_exist(chat_name):
    llm.undeploy(udf_name=chat_name)

llm.deploy(model_path="",
           pretrained_model_type="saas/baichuan",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxxxxx",            
            "saas.baichuan_api_url":"https://api.baichuan-ai.com/v1/chat/completions",
            "saas.model":"Baichuan2-Turbo"
           })

llm.chat_oai(model=chat_name,conversations=[{
    "role":"user",
    "content":"ä½ å¥½",
}])           
```

Since we use SaaS model, There is no need to specify the gpus, so we set `setup_gpus_per_worker` to 0. However, the SaaS model has its own max concurrency limit, the `setup_num_workers` only control the max concurrency accepted by the Byzer-LLM.


For now, only QianWen Saas support stream chat, here is the example:

```python
from byzerllm.utils.client import ByzerLLM
llm = ByzerLLM(verbose=True)

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "qianwen_chat"
if llm.is_model_exist(chat_name):
    llm.undeploy(udf_name=chat_name)

llm.deploy(model_path="",
           pretrained_model_type="saas/qianwen",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxxxxx",            
            "saas.model":"qwen-turbo"
           })

## here you can use `stream_chat_oai`
v = llm.stream_chat_oai(model=chat_name,conversations=[{
    "role":"user",
    "content":"ä½ å¥½ï¼Œä½ æ˜¯è°",
}],llm_config={"gen.incremental_output":False})

for t in v:
    print(t,flush=True)           
```

To check if a model the support of stream chat, you can check [Model Meta](Model-Meta).


### qianfan


```sql
!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/qianfan"
and `saas.api_key`="xxxxxxxxxxxxxxxxxx"
and `saas.secret_key`="xxxxxxxxxxxxxxxx"
and `saas.model`="ERNIE-Bot-turbo"
and `saas.retry_count`="3"
and `saas.request_timeout`="120"
and reconnect="false"
and udfName="qianfan_saas"
and modelTable="command";

```

### azure openai

```sql

!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/azure_openai"
and `saas.api_type`="azure"
and `saas.api_key`="xxx"
and `saas.api_base`="xxx"
and `saas.api_version`="2023-07-01-preview"
and `saas.deployment_id`="xxxxx"
and udfName="azure_openai"
and modelTable="command";
```

### openai

```sql

!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/azure_openai"
and `saas.api_type`="azure"
and `saas.api_key`="xxx"
and `saas.api_base`="xxx"
and `saas.api_version`="xxxxx"
and `saas.model`="xxxxx"
and udfName="openai_saas"
and modelTable="command";
```

### zhipu

```sql

!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/zhipu"
and `saas.api_key`="xxxxxxxxxxxxxxxxxx"
and `saas.secret_key`="xxxxxxxxxxxxxxxx"
and `saas.model`="chatglm_lite"
and udfName="zhipu_saas"
and modelTable="command";
```

### minimax

```sql

!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/minimax"
and `saas.api_key`="xxxxxxxxxxxxxxxxxx"
and `saas.group_id`="xxxxxxxxxxxxxxxx"
and `saas.model`="abab5.5-chat"
and `saas.api_url`="https://api.minimax.chat/v1/text/chatcompletion_pro"
and udfName="minimax_saas"
and modelTable="command";

```

### sparkdesk

```sql
!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/sparkdesk"
and `saas.appid`="xxxxxxxxxxxxxxxxxx"
and `saas.api_key`="xxxxxxxxxxxxxxxx"
and `saas.api_secret`="xxxx"
and `gpt_url`="ws://spark-api.xf-yun.com/v1.1/chat"
and udfName="sparkdesk_saas"
and modelTable="command";
```

### baichuan

```sql
!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/baichuan"
and `saas.api_key`="xxxxxxxxxxxxxxxxxx"
and `saas.secret_key`="xxxxxxxxxxxxxxxx"
and `saas.baichuan_api_url`="https://api.baichuan-ai.com/v1/chat"
and `saas.model`="Baichuan2-53B"
and udfName="baichuan_saas"
and modelTable="command";
```

---

## Pretrain

This section will introduce how to pretrain a LLM model with Byzer-llm.  However, for now, the pretrain feature is more mature in Byzer-SQL, so we will introduce the pretrain feature in Byzer-SQL.

```sql
-- Deepspeed Config
set ds_config='''
{
  "gradient_accumulation_steps": 1,
  "train_micro_batch_size_per_gpu": 1,
  "prescale_gradients": false,
  "zero_allow_untested_optimizer": true,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 1e-8,
      "eps": 1.0e-8,
      "betas": [
        0.9,
        0.95
      ],
      "weight_decay": 0.1
    }
  },
  "tensorboard": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 3,
    "offload_optimizer": {
         "device": "cpu"         
     },           
    "offload_param": {
         "device": "cpu"
    },
    "contiguous_gradients": true,
    "allgather_bucket_size": 1e8,
    "reduce_bucket_size": 1e8,
    "overlap_comm": true,
    "reduce_scatter": true
  },
  "steps_per_print": 16,
  "gradient_clipping": 1.0,
  "wall_clock_breakdown": true,
  "bf16": {
    "enabled": true
  }
}
''';

-- load data
load text.`file:///home/byzerllm/data/raw_data/*`
where wholetext="true" as trainData;

select value as text,file from trainData  as newTrainData;

-- split the data into 12 partitions
run newTrainData as TableRepartition.`` where partitionNum="12" and partitionCols="file" 
as finalTrainData;


-- setup env, we use 12 gpus to pretrain the model
!byzerllm setup sfft;
!byzerllm setup "num_gpus=12";

-- specify the pretrain model type and the pretrained model path
run command as LLM.`` where 
and localPathPrefix="/home/byzerllm/models/sfft/jobs"
and pretrainedModelType="sfft/llama2"
-- original model is from
and localModelDir="/home/byzerllm/models/Llama-2-7b-chat-hf"
-- and localDataDir="/home/byzerllm/data/raw_data"

-- we use async mode to pretrain the model, since the pretrain process will take several days or weeks
-- Ray Dashboard will show the tensorboard address, and then you can monitor the loss
and detached="true"
and keepPartitionNum="true"

-- use deepspeed config, this is optional
and deepspeedConfig='''${ds_config}'''


-- the pretrain data is from finalTrainData table
and inputTable="finalTrainData"
and outputTable="llama2_cn"
and model="command"
-- some hyper parameters
and `sfft.int.max_length`="128"
and `sfft.bool.setup_nccl_socket_ifname_by_ip`="true"
;
```

Since the deepspeed checkpoint is not compatible with the huggingface checkpoint, we need to convert the deepspeed checkpoint to the huggingface checkpoint. The following code will convert the deepspeed checkpoint to the huggingface checkpoint.

```sql
!byzerllm setup single;

run command as LLM.`` where 
action="convert"
and pretrainedModelType="deepspeed/llama3b"
and modelNameOrPath="/home/byzerllm/models/base_model"
and checkpointDir="/home/byzerllm/data/checkpoints"
and tag="Epoch-1"
and savePath="/home/byzerllm/models/my_3b_test2";
```


Now you can deploy the converted model :

```sql
-- éƒ¨ç½²hugginface æ¨¡å‹
!byzerllm setup single;

set node="master";
!byzerllm setup "num_gpus=2";
!byzerllm setup "workerMaxConcurrency=1";

run command as LLM.`` where 
action="infer"
and pretrainedModelType="custom/auto"
and localModelDir="/home/byzerllm/models/my_3b_test2"
and reconnect="false"
and udfName="my_3b_chat"
and modelTable="command";
```

## Finetune

```sql
-- load data, we use the dummy data for finetune
-- data format supported by Byzer-SQLï¼šhttps://docs.byzer.org/#/byzer-lang/zh-cn/byzer-llm/model-sft

load json.`/tmp/upload/dummy_data.jsonl` where
inferSchema="true"
as sft_data;

-- Fintune Llama2
!byzerllm setup sft;
!byzerllm setup "num_gpus=4";

run command as LLM.`` where 
and localPathPrefix="/home/byzerllm/models/sft/jobs"

-- æŒ‡å®šæ¨¡å‹ç±»å‹
and pretrainedModelType="sft/llama2"

-- æŒ‡å®šæ¨¡å‹
and localModelDir="/home/byzerllm/models/Llama-2-7b-chat-hf"
and model="command"

-- æŒ‡å®šå¾®è°ƒæ•°æ®è¡¨
and inputTable="sft_data"

-- è¾“å‡ºæ–°æ¨¡å‹è¡¨
and outputTable="llama2_300"

-- å¾®è°ƒå‚æ•°
and  detached="true"
and `sft.int.max_seq_length`="512";
```

You can check the finetune actor in the Ray Dashboard, the name of the actor is `sft-william-xxxxx`.

After the finetune actor is finished, you can get the model path, so you can deploy the finetuned model.


Here is the log of the finetune actor:

```
Loading data: /home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_data/data.jsonl3
2
there are 33 data in dataset
*** starting training ***
{'train_runtime': 19.0203, 'train_samples_per_second': 1.735, 'train_steps_per_second': 0.105, 'train_loss': 3.0778136253356934, 'epoch': 0.97}35

***** train metrics *****36  
epoch                    =       0.9737  
train_loss               =     3.077838  
train_runtime            = 0:00:19.0239  
train_samples_per_second =      1.73540  
train_steps_per_second   =      0.10541

[sft-william] Copy /home/byzerllm/models/Llama-2-7b-chat-hf to /home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_model/final/pretrained_model4243              
[sft-william] Train Actor is already finished. You can check the model in: /home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_model/final   
```

You can download the finetuned model from the path `/home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_model/final`, or copy the model to all other node in the Ray cluster.

Try to deploy the finetuned model:

```sql
!byzerllm setup single;
run command as LLM.`` where 
action="infer"
and localPathPrefix="/home/byzerllm/models/infer/jobs"
and localModelDir="/home/byzerllm/models/sft/jobs/sft-william-llama2-alpaca-data-ccb8fb55-382c-49fb-af04-5cbb3966c4e6/finetune_model/final"
and pretrainedModelType="custom/llama2"
and udfName="fintune_llama2_chat"
and modelTable="command";
```

Byzer-LLM use QLora to finetune the model, you can merge the finetuned model with the original model with the following code:

```sql
-- åˆå¹¶lora model + base model

!byzerllm setup single;

run command as LLM.`` where 
action="convert"
and pretrainedModelType="deepspeed/llama"
and model_dir="/home/byzerllm/models/sft/jobs/sft-william-20230912-21-50-10-2529bf9f-493e-40a3-b20f-0369bd01d75d/finetune_model/final/pretrained_model"
and checkpoint_dir="/home/byzerllm/models/sft/jobs/sft-william-20230912-21-50-10-2529bf9f-493e-40a3-b20f-0369bd01d75d/finetune_model/final"
and savePath="/home/byzerllm/models/sft/jobs/sft-william-20230912-21-50-10-2529bf9f-493e-40a3-b20f-0369bd01d75d/finetune_model/merge";

```







