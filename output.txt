##File: /Users/allwefantasy/projects/byzer-llm/README.md
<p align="center">
  <picture>    
    <img alt="Byzer-LLM" src="https://github.com/allwefantasy/byzer-llm/blob/master/docs/source/assets/logos/logo.jpg" width=55%>
  </picture>
</p>

<h3 align="center">
Easy, fast, and cheap pretrain,finetune, serving for everyone
</h3>

<p align="center">
| <a href="./README.md"><b>English</b></a> | <a href="./README-CN.md"><b>ä¸­æ–‡</b></a> |

</p>

---

*Latest News* ðŸ”¥

- [2024/04] Release Byzer-LLM 0.1.75
- [2024/03] Release Byzer-LLM 0.1.55
- [2024/02] Release Byzer-LLM 0.1.40
- [2024/01] Release Byzer-LLM 0.1.39
- [2023/12] Release Byzer-LLM 0.1.30

---


Byzer-LLM is Ray based , a full lifecycle solution for LLM that includes pretrain, fintune, deployment and serving.

The unique features of Byzer-LLM are:

1. Full lifecyle: pretrain and finetune,deploy and serving support
2. Python/SQL API support
3. Ray based, easy to scale

---

* [Versions](#Versions)
* [Installation](#Installation)
* [Quick Start](#Quick-Start)
* [How to connect Models from outside of Ray Cluster](#how-to-connect-models-from-outside-of-ray-cluster)
* Embedding/Rerank
    * [Embedding Model](#Embedding-Model)
    * [Embedding Rerank Model](#Embedding-Rerank-Model)
* [Quatization](#Quatization)
* [Supported Models](#Supported-Models)
* Serving
    * Backend
        * [vLLM Support](#vLLM-Support)
            * [vLLM troubleshooting](#vLLM-troubleshooting)
        * [DeepSpeed Support](#DeepSpeed-Support)
        * [llama_cpp Support](#llama_cpp-Support)
    * [Byzer-LLM OpenAI-Compatible RESTful API server](#byzer-llm-openai-compatible-restful-api-server)
* LLM && Python    
    * [Function Calling](#Function-Calling)
    * [Respond with pydantic class](#Respond-with-pydantic-class)
    * [Function Implementation](#Function-Implementation)
    * [LLM-Friendly Function/DataClass](#llm-friendly-functiondataclass)
    * [Prompt Function && Prompt Class](#Prompt-Function--Prompt-Class)
* Model Configurations    
    * [Model Meta](#Model-Meta)
    * [Chat Template](#Chat-Template)
    * [LLM Default Generation Parameters](#LLM-Default-Generation-Parameters)
    * [Pin Model Worker Mapping](#Pin-Model-Worker-Mapping)
    * [Model Worker Load Balance](#Model-Worker-Load-Balance)
* [SaaS Models](#SaaS-Models)
    * [qianwen/é€šä¹‰åƒé—®](#qianwen/é€šä¹‰åƒé—®)
    * [qianwen_vl/é€šä¹‰åƒé—®å¤šæ¨¡æ€](#qianwen_vlé€šä¹‰åƒé—®å¤šæ¨¡æ€)
    * [yi_vl_plus/01ä¸‡ç‰©å¤šæ¨¡æ€](#yi_vl_plus01ä¸‡ç‰©å¤šæ¨¡æ€)
    * [baichuan/ç™¾å·](#baichuan/ç™¾å·)
    * [azure openai](#azure-openai)
    * [openai](#openai)
    * [zhipu/æ™ºè°±](#zhipu/æ™ºè°±)
    * [sparkdesk/æ˜Ÿç«](#sparkdesk/æ˜Ÿç«)         
    * [AmazonBedrock](#AmazonBedrock)
    * [Claude](#claude)
    * [Gemini](#gemini)
    * [Kimi](#moonshot/kimi)
    * [Volcano TTS](#volcano_tts)
    * [OpenAI TTS](#openai_tts)
    * [Azure TTS](#azure_tts)
    * [OpenAI Image Generation](#openai_image_generation)
* [Multi Modal](#Multi-Modal)
* [StableDiffusion](#StableDiffusion)
* [SQL Support](#SQL-Support)
* [Pretrain](#Pretrain)
* [Finetune](#Finetune)
* [Stream Chat](#Stream-Chat)
* [Articles](#Articles)
* [Third-party Libraries](#Third-party-Libraries)
    * [Llama_index](#Llama_index)
* [Contributing](#Contributing)

---

## Versions
- 0.1.71:  stream_chat_oai/async_chat_oai add delta_mode parameter
- 0.1.53:  Optimize saas/official_openai model
- 0.1.52:  Add Yi-VL-Plus Saas model support
- 0.1.50:  Add byzerllm command line tool
- 0.1.48:  Add qwen_vl_saas model support/ Callback support for Byzer-LLM 
- 0.1.47:  Fix prompt function bugs
- 0.1.46:  Add stream_reply to Byzer-Agent, please check this [link](https://github.com/allwefantasy/byzer-agent)
- 0.1.45:  Optimize Byzer-Agent, please check this [link](https://github.com/allwefantasy/byzer-agent)
- 0.1.44:  Prompt Function/Prompt Class help you to manage and execute your prompts.
- 0.1.43ï¼š add pin_model_worker_mapping to ByzerLLM which can some request to the same worker(if you have a model with multiple workers), add load_balance parameter to ByzerLLM which can control the load balance strategy when the model has multiple workers
and this parameter takes effect only when you deploy model.
- 0.1.42ï¼š when use  tokenizer apply_chat_template we should  add_generation_prompt=True
- 0.1.41ï¼š Fix vLLM bugs / vLLM 0.3.3 Support
- 0.1.40ï¼š LlamaIndex support / vLLM 0.3.2 Support / Byzer-SQL new features / Qwen 1.5 support
- 0.1.39ï¼š Enhance Function Impl / Upgrade SaaS SDK / Add OpenAI-Compatible API Server
- 0.1.38ï¼š Upgrade saas/sparkdask model / add embedding rerank model / agent message store support
- 0.1.37ï¼š Upgrade saas/zhipu model, you can choose glm-4 / embedding-2 for LLM or embedding purpose
- 0.1.36ï¼š Fix data analysis agent bugs which caused by the upgrade of Byzer-Agent
- 0.1.35ï¼š Add Baichuan Saas embedding model
- 0.1.34ï¼š Enhance the Byzer-Agent API and fix some bugs in Byzer-LLM
- 0.1.33ï¼š Fix Response Class bugs/ Add function implementation
- 0.1.32ï¼š StableDiffusion optimization
- 0.1.31ï¼š Stream Chat with token count information / Optimize multi modal model chat
- 0.1.30ï¼š Apply chat template for vLLM backend
- 0.1.29ï¼š Enhance DataAnalysis Agent
- 0.1.28ï¼š Bug fix
- 0.1.27ï¼š Bug fix
- 0.1.26ï¼š Support QianWen Saas/ Support stream chat in QianWenSaas/ Fix some Saas model bugs
- 0.1.24ï¼š Support get meta from model instance and auto setup template
- 0.1.23ï¼š Fintune with python API/ Fix some bugs
- 0.1.22ï¼š Function Calling support/ Response with pydantic class
- 0.1.19ï¼š Fix embedding bugs
- 0.1.18ï¼š Support stream chat/ Support Model Template
- 0.1.17ï¼š None
- 0.1.16ï¼š Enhance the API for byzer-retrieval
- 0.1.14ï¼š add get_tables/get_databases API for byzer-retrieval
- 0.1.13: support shutdown cluster for byzer-retrieval
- 0.1.12: Support Python API (alpha)
- 0.1.5:  Support python wrapper for [byzer-retrieval](https://github.com/allwefantasy/byzer-retrieval)

---

## Installation

Recommend Env:

1. Conda:  python==3.10.11  
2. OS:     ubuntu 22.04
3. Cuda:   12.1.0 (Optional if you only use SaaS model)

```bash
## Make sure you python version is 3.10.11
pip install -r requirements.txt
## Skip this step if you have no Nvidia GPU
pip install vllm==0.3.3
pip install -U byzerllm
ray start --head
```

If your cuda version is 11.8, please check the following link to install vLLM:

https://docs.vllm.ai/en/latest/getting_started/installation.html

The key steps are:

```shell
As of now, vLLMâ€™s binaries are compiled on CUDA 12.1 by default. However, you can install vLLM with CUDA 11.8 by running:

# Install vLLM with CUDA 11.8.
export VLLM_VERSION=0.2.6
export PYTHON_VERSION=310
pip install https://github.com/vllm-project/vllm/releases/download/v${VLLM_VERSION}/vllm-${VLLM_VERSION}+cu118-cp${PYTHON_VERSION}-cp${PYTHON_VERSION}-manylinux1_x86_64.whl

# Re-install PyTorch with CUDA 11.8.
pip uninstall torch -y
pip install torch --upgrade --index-url https://download.pytorch.org/whl/cu118

# Re-install xFormers with CUDA 11.8.
pip uninstall xformers -y
pip install --upgrade xformers --index-url https://download.pytorch.org/whl/cu118
```

### Raw Machine 

> Only tested on Ubuntu 20.04/22.04 CentOS 8.0

If your machine is a raw machine whithout GPU Driver and Cuda installed, you can use the following script to setup the machine:

```shell
git clone https://gitee.com/allwefantasy/byzer-llm
cd byzer-llm/setup-machine
```

Then switch to the ROOT user, and run the following script:

```shell
ROLE=master ./setup-machine.sh
```
This step will create a user called `byzerllm`.

Then switch to the `byzerllm` user, and run the following script:

```shell
ROLE=master ./setup-machine.sh
```

This time the script will automatically install the following software:

1. 
2. Conda
3. Nvidia Driver 535
4. Cuda 12.1.0
5. Ray
6. all the python packages in requirements.txt
7. Byzer-SQL/Byzer-Notebook

If you need to add more worker nodes to the Ray cluster, repeat the above steps on the worker nodes.
Notice that the `ROLE` should be `worker` on the worker nodes.

```shell
ROLE=worker ./setup-machine.sh
```

---

## Quick Start

### Command Line

Deploy a model:

```shell
byzerllm deploy --model_path /home/byzerllm/models/openbuddy-llama2-13b64k-v15 \
--pretrained_model_type custom/auto \
--gpu_gpus_per_worker 4 \
--num_workers 1 \
--model llama2_chat 
```


Then you can chat with the model:

```shell
byzerllm query --model llama2_chat --query "ä½ å¥½"
```

You can undeploy the model like this:

```shell
byzerllm undeploy --model llama2_chat
```

If you need to specify the Ray address, use `--ray_address`.

Here is another case to show you how to deploy SaaS model:

```shell
byzerllm deploy  --pretrained_model_type saas/qianwen \
--cpus_per_worker 0.01 \
--gpus_per_worker 0 \
--num_workers 1 \
--infer_params saas.api_key=xxxxx saas.model=qwen-max \
--model qianwen_short_chat 
```



### Python
```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend

ray.init(address="auto",namespace="default",ignore_reinit_error=True)

llm = ByzerLLM()

llm.setup_gpus_per_worker(4).setup_num_workers(1)
llm.setup_infer_backend(InferBackend.transformers)

llm.deploy(model_path="/home/byzerllm/models/openbuddy-llama2-13b64k-v15",
           pretrained_model_type="custom/llama2",
           udf_name="llama2_chat",infer_params={})



llm_client = ByzerLLM()
llm_client.setup_template("llama2_chat","auto")

v = llm.chat_oai(model="llama2_chat",conversations=[{
    "role":"user",
    "content":"hello",
}])

print(v[0].output)
```

The above code will deploy a llama2 model and then use the model to infer the input text. If you use transformers as the inference backend, you should specify the `pretrained_model_type` manually since the transformers backend can not auto detect the model type.

Byzer-LLM also support `deploy` SaaS model with the same way. This feature provide a unified interface for both open-source model and SaaS model. The following code will deploy a Azure OpenAI model and then use the model to infer the input text.


```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend
ray.init(address="auto",namespace="default",ignore_reinit_error=True)

llm = ByzerLLM()

llm.setup_gpus_per_worker(0).setup_num_workers(10)
llm.setup_infer_backend(InferBackend.transformers)

llm.deploy(pretrained_model_type="saas/azure_openai",
           udf_name="azure_openai",
           infer_params={
            "saas.api_type":"azure",
            "saas.api_key"="xxx"
            "saas.api_base"="xxx"
            "saas.api_version"="2023-07-01-preview"
            "saas.deployment_id"="xxxxxx"
           })


llm_client = ByzerLLM()
llm_client.setup_template("azure_openai","auto")

v = llm.chat_oai(model="azure_openai",conversations=[{
    "role":"user",
    "content":"hello",
}])

print(v[0].output)
```

Notice that the SaaS model does not need GPU, so we set the `setup_gpus_per_worker` to 0, and you can use `setup_num_workers`
to control max concurrency,how ever, the SaaS model has its own max concurrency limit, the `setup_num_workers` only control the max
concurrency accepted by the Byzer-LLM.

## How to connect Models from outside of Ray Cluster

The recommended way is to start a empty Ray worker in your target machine(e.g. Your web server machine):

```shell
ray start --address="xxxxx:6379"  --num-gpus=0 --num-cpus=0 
```

Then you can connect the Models from outside of the Ray cluster:

```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend

## connect the ray cluster by the empty worker we started before
## this code should be run once in your prorgram
ray.init(address="auto",namespace="default",ignore_reinit_error=True)

## new a ByzerLLM instance

llm_client = ByzerLLM()
llm_client.setup_template("llama2_chat","auto")

v = llm.chat_oai(model="llama2_chat",conversations=[{
    "role":"user",
    "content":"hello",
}])

print(v[0].output)
```

Or 


```python
from dataclasses import asdict
import json
import base64

import byzerllm
from byzerllm.utils.client.types import Templates

byzerllm.connect_cluster(address="ray://CLUSTER_IP:10001")

model_name = "zephyr_7b_chat"
llm = byzerllm.ByzerLLM()
llm.setup_template(model=model_name,template=Templates.default())
llm.setup_default_model_name(model_name)

t = llm.chat_oai(conversations=[{
    "role":"user",
    "content":"ä½ å¥½"}])
print(t[0].output)
```


## Embedding Model

The following code is a example of deploying BGE embedding model

```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend
ray.init(address="auto",namespace="default",ignore_reinit_error=True)
llm = ByzerLLM()

llm.setup_gpus_per_worker(0.4).setup_num_workers(2).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path="/home/byzerllm/models/bge-large-zh",
    pretrained_model_type="custom/bge",
    udf_name="emb",
    infer_params={}
)   
```

Then you can convert any text to vector :

```python
t = llm.emb("emb",LLMRequest(instruction="wow"))
t[0].output
#output: [-0.005588463973253965,
 -0.01747054047882557,
 -0.040633779019117355,
...
 -0.010880181565880775,
 -0.01713103987276554,
 0.017675869166851044,
 -0.010260719805955887,
 ...]
```

The Byzer-LLM also support SaaS embedding model. The following code will deploy a Baichuan embedding model and then use the model to infer the input text.

```python
import os
os.environ["RAY_DEDUP_LOGS"] = "0" 

import ray
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.utils.client import ByzerLLM,LLMRequest,LLMResponse,LLMHistoryItem,InferBackend
from byzerllm.utils.client import Templates

ray.init(address="auto",namespace="default",ignore_reinit_error=True)  

llm = ByzerLLM(verbose=True)

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "baichuan_emb"
if llm.is_model_exist(chat_name):
    llm.undeploy(udf_name=chat_name)

llm.deploy(model_path="",
           pretrained_model_type="saas/baichuan",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"",            
            "saas.model":"Baichuan-Text-Embedding"
           })
llm.setup_default_emb_model_name(chat_name)

v = llm.emb(None,LLMRequest(instruction="ä½ å¥½"))
print(v.output)
```

## Embedding Rerank Model

If you need to use embedding rerank model, you can refer to the following usage.
```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend
ray.init(address="auto",namespace="default",ignore_reinit_error=True)
llm = ByzerLLM()

llm.setup_gpus_per_worker(0.4).setup_num_workers(2).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path="/Users/wanghan/data/bge-reranker-base",
    pretrained_model_type="custom/bge_rerank",
    udf_name="emb_rerank",
    infer_params={}
)   
llm.setup_default_emb_model_name("emb_rerank")
```
Then you can get a relevance score by inputting query and passage to the reranker
```python
sentence_pairs_01 = ['query', 'passage']
t1 = llm.emb_rerank(sentence_pairs=sentence_pairs_01)
print(t1[0].output)
#output [['query', 'passage'], 0.4474925994873047]

sentence_pairs_02 = [['what is panda?', 'hi'], ['what is panda?','The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]
t2 = llm.emb_rerank(sentence_pairs=sentence_pairs_02)
print(t2[0].output)
#output [[['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'], 6.1821160316467285], [['what is panda?', 'hi'], -8.154398918151855]]
```

## Quatization

If the backend is `InferBackend.transformers`, here is the baichuan2 example:

```python
llm.setup_gpus_per_worker(2).setup_num_workers(1).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/baichuan2",
    udf_name="baichuan2_13_chat",
    infer_params={"quatization":"4"}
)
```
The available `quatization` values:

1. 4
2. 8
3. true/false

When it's set true, the int4 will be choosed.

If the bakcend is `InferBackend.VLLM`, here is the Yi example:

If you need to deploy model with Quantization, you can set the `infer_params` as the following code:

```python
llm.setup_gpus_per_worker(1).setup_num_workers(1).setup_infer_backend(InferBackend.VLLM)
llm.deploy(
    model_path="/home/winubuntu/models/Yi-6B-Chat-4bits",
    pretrained_model_type="custom/auto",
    udf_name="chat",
    infer_params={"backend.quantization":"AWQ"}
)
```

The parameter `backend.quantization` can be GPTQ/AWQ.


## Supported Models

The supported open-source `pretrained_model_type` are:

1. custom/llama2
2. bark	
3. whisper	
3. chatglm6b
4. custom/chatglm2
5. moss
6. custom/alpha_moss
7. dolly
8. falcon
9. llama
10. custom/starcode
11. custom/visualglm
12. custom/m3e
13. custom/baichuan
14. custom/bge
15. custom/qwen_vl_chat
16. custom/stable_diffusion
17. custom/zephyr

The supported SaaS `pretrained_model_type` are:

1. saas/chatglm	Chatglm130B
2. saas/sparkdesk	æ˜Ÿç«å¤§æ¨¡åž‹
3. saas/baichuan	ç™¾å·å¤§æ¨¡åž‹
4. saas/zhipu	æ™ºè°±å¤§æ¨¡åž‹
5. saas/minimax	MiniMax å¤§æ¨¡åž‹
6. saas/qianfan	æ–‡å¿ƒä¸€è¨€
7. saas/azure_openai	
8. saas/openai

Notice that the derived models from llama/llama2/startcode are also supported. For example, you can use `llama` to load vicuna model.

## vLLM Support

The Byzer-llm also support vLLM as the inference backend. The following code will deploy a vLLM model and then use the model to infer the input text.

```python
import ray
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend

ray.init(address="auto",namespace="default",ignore_reinit_error=True)
llm = ByzerLLM()

llm.setup_gpus_per_worker(2)
llm.setup_num_workers(1)
llm.setup_infer_backend(InferBackend.VLLM)

llm.deploy(
    model_path="/home/byzerllm/models/openbuddy-zephyr-7b-v14.1",
    pretrained_model_type="custom/auto",
    udf_name="zephyr_chat"",
    infer_params={}
)

v = llm.chat_oai(model="zephyr_chat",conversations=[{
    "role":"user",
    "content":"hello",
}])
print(v[0].output)
```

There are some tiny differences between the vLLM and the transformers backend. 

1. The `pretrained_model_type` is fixed to `custom/auto` for vLLM, since the vLLM will auto detect the model type.
2. Use `setup_infer_backend` to specify `InferBackend.VLLM` as the inference backend.

### vLLM troubleshooting
If you use the version of vLLM > 0.2.7 and meets the following error:

```shell
 Error Type: TASK_EXECUTION_EXCEPTION

 self._call_impl(*args, **kwargs)
  File "/home/byzerllm/miniconda3/envs/byzerllm-dev2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/byzerllm/miniconda3/envs/byzerllm-dev2/lib/python3.10/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 96, in forward
    input_mask = ((input_ < self.vocab_start_index) |
TypeError: '<' not supported between instances of 'TensorMetadata' and 'int'
```

You can open the Ray dashboard and find the RayVLLMWorker Actor, go into and check the tasks tab and click the error task to get the error message above.

The solution is to modify the `$CONDA_ENV/site-packages/vllm/model_executor/parallel_utils/communication_op.py` in the vLLM pip package:

Replace the following code:

```python
TensorMetadata = namedtuple("TensorMetadata", ["dtype", "size"])
```

to 

```python
class TensorMetadata:
    def __init__(self, dtype, size):
        self.dtype = dtype
        self.size = size
```



 

### Stream Chat

If the model deployed with the backend vLLM, then it also support `stream chat`ï¼š
the `stream_chat_oai` will return a generator, you can use the generator to get the output text.

```python

llm.setup_default_model_name(chat_model_name) 

t = llm.stream_chat_oai(conversations=[{
    "role":"user",
    "content":"Hello, how are you?"
}])

for line in t:
   print(line+"\n")
```

## DeepSpeed Support

The Byzer-llm also support DeepSpeed as the inference backend. The following code will deploy a DeepSpeed model and then use the model to infer the input text.

```python
import ray
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend

ray.init(address="auto",namespace="default",ignore_reinit_error=True)
llm = ByzerLLM()

llm.setup_gpus_per_worker(4)
llm.setup_num_workers(1)
llm.setup_infer_backend(InferBackend.DeepSpeed)

llm.deploy(
    model_path="/home/byzerllm/models/openbuddy-llama-13b-v5-fp16",
    pretrained_model_type="custom/auto",
    udf_name="llama_chat"",
    infer_params={}
)

llm.chat("llama_chat",LLMRequest(instruction="hello world"))[0].output
```

The code above is totally the same as the code for vLLM, except that the `InferBackend` is `InferBackend.DeepSpeed`.

## llama_cpp Support

```bash
byzerllm deploy --pretrained_model_type custom/auto \
--infer_backend llama_cpp \
--cpus_per_worker 0.001 \
--gpus_per_worker 0 \
--num_workers 1 \
--infer_params verbose=true \
--model_path /Users/allwefantasy/Downloads/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf \
--model llama_3_chat
```


## Byzer-LLM OpenAI-Compatible RESTful API server

You can use the following code to start a ByzerLLm OpenAI-Compatible RESTful API server:

```shell
ray start --address="xxxxx:6379"  --num-gpus=0 --num-cpus=0 
python -m byzerllm.utils.client.entrypoints.openai.api_server
```

By default, the server will listen on port 8000, you can use the following code to test the API:

```python
from openai import OpenAI
client = OpenAI(
    base_url="http://127.0.0.1:8000/v1",
    api_key="xxxx"
)

chat_completion = client.chat.completions.create(    
    model="wenxin_chat",
    messages=[{"role": "user", "content": "å†™ä¸€ä¸ªæŽ’åºç®—æ³•"}],
    stream=False
)

print(chat_completion.choices[0].message.content)
```

Stream chat:

```python

from openai import OpenAI
client = OpenAI(
    base_url="http://127.0.0.1:8000/v1",
    api_key="simple"
)

chat_completion = client.chat.completions.create(    
    model="wenxin_chat",
    messages=[{"role": "user", "content": "å†™ä¸€ä¸ªæŽ’åºç®—æ³•"}],
    stream=True
)

for chunk in chat_completion:    
    print(chunk.choices[0].delta.content or "", end="")
```

## Function Calling

Here is a simple example for function calling based on QWen 72B

Deploy Model:


```python
import ray
ray.init(address="auto",namespace="default") 
llm = ByzerLLM()

model_location="/home/byzerllm/models/Qwen-72B-Chat"

llm.setup_gpus_per_worker(8).setup_num_workers(1).setup_infer_backend(InferBackend.VLLM)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/auto",
    udf_name=chat_model_name,
    infer_params={}
)

llm.setup_default_model_name("chat")
# from 0.1.24 
# llm.setup_auto("chat")
meta = llm.get_meta()
llm.setup_max_model_length("chat",meta.get("max_model_len",32000))
lm.setup_template("chat",Templates.qwen()) 
```

Try to create some Python functions:

```python

from typing import List,Dict,Any,Annotated
import pydantic 
import datetime
from dateutil.relativedelta import relativedelta

def compute_date_range(count:Annotated[int,"æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»åž‹"],
                       unit:Annotated[str,"æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»åž‹",{"enum":["day","week","month","year"]}])->List[str]:
    '''
    è®¡ç®—æ—¥æœŸèŒƒå›´

    Args:
        count: æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»åž‹
        unit: æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»åž‹ï¼Œå¯é€‰å€¼ä¸º day,week,month,year
    '''        
    now = datetime.datetime.now()
    now_str = now.strftime("%Y-%m-%d %H:%M:%S")
    if unit == "day":
        return [(now - relativedelta(days=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    elif unit == "week":
        return [(now - relativedelta(weeks=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    elif unit == "month":
        return [(now - relativedelta(months=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    elif unit == "year":
        return [(now - relativedelta(years=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    return ["",""]

def compute_now()->str:
    '''
    è®¡ç®—å½“å‰æ—¶é—´
    '''
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
```

Here we provide two functions:

1. compute_date_range: compute the date range based on the count and unit
2. compute_now: get the current date

We will use the model to call these tools according to the user's question.

```python
t = llm.chat_oai([{
    "content":'''è®¡ç®—å½“å‰æ—¶é—´''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

t[0].values

## output: ['2023-12-18 17:30:49']
```

```python
t = llm.chat_oai([{
    "content":'''æœ€è¿‘ä¸‰ä¸ªæœˆè¶‹åŠ¿''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

t[0].values

## output: [['2023-09-18 17:31:21', '2023-12-18 17:31:21']]
```

```python
t = llm.chat_oai([{
    "content":'''æœ€è¿‘ä¸‰å¤©''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

t[0].values

## output: [['2023-12-15 17:23:38', '2023-12-18 17:23:38']]
```

```python
t = llm.chat_oai([{
    "content":'''ä½ åƒé¥­äº†ä¹ˆï¼Ÿ''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

if t[0].values:
    print(t[0].values[0])
else:
    print(t[0].response.output)   

## output: 'æ‚¨å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½è¯­è¨€æ¨¡åž‹ï¼Œæš‚æ—¶æ— æ³•åƒé¥­ã€‚'
```

You can check the default prompt template function in `from byzerllm.utils import function_calling_format`.
If the model is not work well with the default function, you can setup your custom function:

```python
def custom_function_calling_format(prompt:str,tools:List[Callable],tool_choice:Callable)->str:
.....


llm.setup_function_calling_format_func("chat",custom_function_calling_format)
```

## Respond with pydantic class

When you chat with LLM, you can specify the reponse class, 

```python
import pydantic 

class Story(pydantic.BaseModel):
    '''
    æ•…äº‹
    '''

    title: str = pydantic.Field(description="æ•…äº‹çš„æ ‡é¢˜")
    body: str = pydantic.Field(description="æ•…äº‹ä¸»ä½“")



t = llm.chat_oai([
{
    "content":f'''è¯·ç»™æˆ‘è®²ä¸ªæ•…äº‹ï¼Œåˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ ‡é¢˜ï¼Œä¸€ä¸ªæ•…äº‹ä¸»ä½“''',
    "role":"user"
},
],response_class=Story)

t[0].value

## output: Story(title='å‹‡æ•¢çš„å°å…”å­', body='åœ¨ä¸€ä¸ªç¾Žä¸½çš„æ£®æž—é‡Œï¼Œä½ç€ä¸€åªå¯çˆ±çš„å°å…”å­ã€‚å°å…”å­éžå¸¸å‹‡æ•¢ï¼Œæœ‰ä¸€å¤©ï¼Œæ£®æž—é‡Œçš„åŠ¨ç‰©ä»¬éƒ½è¢«å¤§ç°ç‹¼å“åäº†ã€‚åªæœ‰å°å…”å­ç«™å‡ºæ¥ï¼Œç”¨æ™ºæ…§å’Œå‹‡æ°”æ‰“è´¥äº†å¤§ç°ç‹¼ï¼Œä¿æŠ¤äº†æ‰€æœ‰çš„åŠ¨ç‰©ã€‚ä»Žæ­¤ï¼Œå°å…”å­æˆä¸ºäº†æ£®æž—é‡Œçš„è‹±é›„ã€‚')
```

The above code will ask the LLM to generate the Story class directly. However, sometimes we hope the LLM 
generate text first, then extract the structure from the text, you can set `response_after_chat=True` to 
enable this behavior. However, this will bring some performance penalty(additional inference).

```python
t = llm.chat_oai([
{
    "content":f'''è¯·ç»™æˆ‘è®²ä¸ªæ•…äº‹ï¼Œåˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ ‡é¢˜ï¼Œä¸€ä¸ªæ•…äº‹ä¸»ä½“''',
    "role":"user"
},
],response_class=Story,response_after_chat=True)

t[0].value
## output: Story(title='æœˆå…‰ä¸‹çš„å®ˆæŠ¤è€…', body='åœ¨ä¸€ä¸ªé¥è¿œçš„å¤è€æ‘åº„é‡Œï¼Œä½ç€ä¸€ä½åå«é˜¿æ˜Žçš„å¹´è½»äººã€‚é˜¿æ˜Žæ˜¯ä¸ªå­¤å„¿ï¼Œä»Žå°åœ¨æ‘é‡Œé•¿å¤§ï¼Œä»¥ç§ç”°ä¸ºç”Ÿã€‚ä»–å–„è‰¯ã€å‹¤åŠ³ï¼Œæ·±å—æ‘æ°‘ä»¬å–œçˆ±ã€‚\n\næ‘å­é‡Œæœ‰ä¸ªä¼ è¯´ï¼Œæ¯å½“æ»¡æœˆæ—¶åˆ†ï¼Œæœˆäº®å¥³ç¥žä¼šåœ¨æ‘å­åŽå±±çš„å¤æ ‘ä¸‹å‡ºçŽ°ï¼Œèµç¦ç»™é‚£äº›å–„è‰¯çš„äººä»¬ã€‚ç„¶è€Œï¼Œåªæœ‰æœ€çº¯æ´çš„å¿ƒæ‰èƒ½çœ‹åˆ°å¥¹ã€‚å› æ­¤ï¼Œæ¯å¹´çš„è¿™ä¸ªæ—¶å€™ï¼Œé˜¿æ˜Žéƒ½ä¼šç‹¬è‡ªä¸€äººå‰å¾€åŽå±±ï¼Œå¸Œæœ›èƒ½å¾—åˆ°å¥³ç¥žçš„ç¥ç¦ã€‚\n\nè¿™ä¸€å¹´ï¼Œæ‘å­é­å—äº†ä¸¥é‡çš„æ—±ç¾ï¼Œåº„ç¨¼æž¯é»„ï¼Œäººä»¬ç”Ÿæ´»å›°è‹¦ã€‚é˜¿æ˜Žå†³å®šå‘æœˆäº®å¥³ç¥žç¥ˆæ±‚é™é›¨ï¼Œæ‹¯æ•‘æ‘å­ã€‚ä»–åœ¨æœˆå…‰ä¸‹è™”è¯šåœ°ç¥ˆç¥·ï¼Œå¸Œæœ›å¥³ç¥žèƒ½å¬åˆ°ä»–çš„å‘¼å”¤ã€‚\n\nå°±åœ¨è¿™ä¸ªæ—¶åˆ»ï¼Œæœˆäº®å¥³ç¥žå‡ºçŽ°äº†ã€‚å¥¹è¢«é˜¿æ˜Žçš„å–„è‰¯å’Œæ‰§ç€æ‰€æ„ŸåŠ¨ï¼Œç­”åº”äº†ä»–çš„è¯·æ±‚ã€‚ç¬¬äºŒå¤©æ—©æ™¨ï¼Œå¤©ç©ºä¹Œäº‘å¯†å¸ƒï¼Œå¤§é›¨å€¾ç›†è€Œä¸‹ï¼Œä¹…æ—±çš„åœŸåœ°å¾—åˆ°äº†æ»‹æ¶¦ï¼Œåº„ç¨¼é‡æ–°ç„•å‘ç”Ÿæœºã€‚\n\nä»Žæ­¤ä»¥åŽï¼Œæ¯å¹´çš„æ»¡æœˆä¹‹å¤œï¼Œé˜¿æ˜Žéƒ½ä¼šåŽ»åŽå±±ç­‰å¾…æœˆäº®å¥³ç¥žçš„å‡ºçŽ°ï¼Œä»–æˆä¸ºäº†æ‘æ°‘å¿ƒä¸­çš„å®ˆæŠ¤è€…ï¼Œç”¨ä»–çš„å–„è‰¯å’Œæ‰§ç€ï¼Œå®ˆæŠ¤ç€æ•´ä¸ªæ‘åº„ã€‚è€Œä»–ä¹Ÿç»ˆäºŽæ˜Žç™½ï¼ŒçœŸæ­£çš„å®ˆæŠ¤è€…ï¼Œå¹¶éžéœ€è¦è¶…å‡¡çš„åŠ›é‡ï¼Œåªéœ€è¦ä¸€é¢—å……æ»¡çˆ±ä¸Žå–„è‰¯çš„å¿ƒã€‚')
```

You can check the default prompt template function in `from byzerllm.utils import response_class_format,response_class_format_after_chat`.
If the model is not work well with the default function, you can setup your custom function:

```python
def custom_response_class_format(prompt:str,cls:pydantic.BaseModel)->str:
.....


llm.setup_response_class_format_func("chat",custom_response_class_format)
```

## Function Implementation

The Byzer-llm also support function implementation. You can define a empty function, and combine the doc in the function/the user's quesion to guide the LLM to implement the function. 

Here is a simple example:

```python
from byzerllm.utils.client import code_utils,message_utils
from typing import List,Union,Optional
import pydantic

class Time(pydantic.BaseModel):
    time: str = pydantic.Field(...,description="æ—¶é—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")


@llm.impl()
def calculate_current_time()->Time:
    '''
    è®¡ç®—å½“å‰æ—¶é—´
    '''
    pass 


calculate_current_time()
#output: Time(time='2024-01-28')
```

By default, the function implementation will be cached, 

```python
start = time.monotonic()
calculate_current_time()
print(f"first time cost: {time.monotonic()-start}")

start = time.monotonic()
calculate_current_time()
print(f"second time cost: {time.monotonic()-start}")

# output:
# first time cost: 6.067266260739416
# second time cost: 4.347506910562515e-05
```

you can use `llm.clear_impl_cache()` to clear the cache. 

Here is the example for function implementation with parameters:

```python
from byzerllm.utils.client import code_utils,message_utils
from typing import List,Union,Optional,Annotated
import pydantic
from datetime import datetime

class Time(pydantic.BaseModel):
    time: str = pydantic.Field(...,description="æ—¶é—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")


@llm.impl()
def add_one_day(current_day:Annotated[datetime,"å½“å‰æ—¥æœŸï¼Œç±»åž‹æ˜¯datatime.datetime"])->Time:
    '''
    ç»™ä¼ å…¥çš„æ—¥æœŸåŠ ä¸€å¤©ï¼Œå¾—åˆ°æ˜Žå¤©çš„æ—¶é—´
    '''
    pass 


add_one_day(datetime.now())
# output:Time(time='2024-01-29')
```

With instruction:

```python
from byzerllm.utils.client import code_utils,message_utils
from typing import List,Union,Optional
import pydantic

class TimeRange(pydantic.BaseModel):
    '''
    æ—¶é—´åŒºé—´
    æ ¼å¼éœ€è¦å¦‚ä¸‹ï¼š yyyy-MM-dd
    '''  
    
    start: str = pydantic.Field(...,description="å¼€å§‹æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")
    end: str = pydantic.Field(...,description="æˆªæ­¢æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")

@llm.impl(instruction="åŽ»å¹´ä¸‰æœˆåˆ°ä¸ƒæœˆ")
def calculate_time_range()->TimeRange:
    '''
    è®¡ç®—æ—¶é—´åŒºé—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd. 
    '''
    pass 

calculate_time_range()
# output: TimeRange(start='2023-03-01', end='2023-07-31')
```

If you want to replace instruction with the user's question, you can use the following code:

```python
from byzerllm.utils.client import code_utils,message_utils
from typing import List,Union,Optional
import pydantic

class TimeRange(pydantic.BaseModel):
    '''
    æ—¶é—´åŒºé—´
    æ ¼å¼éœ€è¦å¦‚ä¸‹ï¼š yyyy-MM-dd
    '''  
    
    start: str = pydantic.Field(...,description="å¼€å§‹æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")
    end: str = pydantic.Field(...,description="æˆªæ­¢æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")

def calculate_time_range()->TimeRange:
    '''
    è®¡ç®—æ—¶é—´åŒºé—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd. 
    '''
    pass 


llm.impl(instruction="åŽ»å¹´ä¸‰æœˆåˆ°ä¸ƒæœˆ")(calculate_time_range)()
```

You can use `verbose=True` to get more information about the function implementation:

```python
@llm.impl()
def add_one_day(current_day:Annotated[datetime,"å½“å‰æ—¥æœŸï¼Œç±»åž‹æ˜¯datatime.datetime"])->Time:
    '''
    ç»™ä¼ å…¥çš„æ—¥æœŸåŠ ä¸€å¤©ï¼Œå¾—åˆ°æ˜Žå¤©çš„æ—¶é—´
    '''
    pass 
```

You can also use the basic chat_oai function to implement the function:


```python
class TimeRange(pydantic.BaseModel):
    '''
    æ—¶é—´åŒºé—´
    æ ¼å¼éœ€è¦å¦‚ä¸‹ï¼š yyyy-MM-dd
    '''  
    
    start: str = pydantic.Field(...,description="å¼€å§‹æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")
    end: str = pydantic.Field(...,description="æˆªæ­¢æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")


def calculate_time_range():
    '''
    è®¡ç®—æ—¶é—´åŒºé—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd. 
    '''
    pass 
    
t = llm.chat_oai([{
    "content":"åŽ»å¹´ä¸‰æœˆåˆ°ä¸ƒæœˆ",
    "role":"user"    
}],impl_func=calculate_time_range,response_class=TimeRange,execute_impl_func=True)
```

The above code , we define a function called `calculate_time_range`, and the function is empty, then we discribe the function in the doc string, and define the response class `TimeRange`, to make sure the return value is a `TimeRange` instance. Since the function should be used to resolve the user's question, so the implementation of the function should be related to the user's question. Instead try to implement a common use function, we can just implement a function which can only resolve the user's current question.

After the execution, you can get the output like this:

```python
t[0].value
# start='2023-03-01' end='2023-07-31'
```

If the value is None or not correct, you can get the error message:

```python
t[0].metadata.get("resason","")
```

If your function has parameters, you can pass the parameters to the function by `impl_func_params`:

```python
t = llm.chat_oai([{
    "content":"xxxxx",
    "role":"user"    
}],
impl_func=calculate_time_range,
impl_func_params={},
response_class=TimeRange,execute_impl_func=True)
```

If you want to replace the default prompt template function, here is a example:

```python
import pydantic
from typing import List,Optional,Union,Callable
from byzerllm.utils import serialize_function_to_json

def function_impl_format2(prompt:str,func:Optional[Union[Callable,str]],
                             cls:Union[pydantic.BaseModel,str])->str:
    
    tool_choice_ser = serialize_function_to_json(func)    
    _cls = ""
    if isinstance(cls, str):
        _cls = cls
    else:
        _cls = cls.schema_json(ensure_ascii=False)
    
    msg = f''''ç”Ÿæˆä¸€ä¸ªpythonå‡½æ•°ï¼Œç»™å‡ºè¯¦ç»†çš„æ€è€ƒé€»è¾‘ï¼Œå¯¹æœ€åŽç”Ÿæˆçš„å‡½æ•°ä¸è¦è¿›è¡Œç¤ºä¾‹è¯´æ˜Žã€‚

ç”Ÿæˆçš„å‡½æ•°çš„åå­—ä»¥åŠå‚æ•°éœ€è¦æ»¡è¶³å¦‚ä¸‹çº¦æŸï¼š

\```json
{tool_choice_ser}
\```

ç”Ÿæˆçš„å‡½æ•°çš„è¿”å›žå€¼å¿…é¡»æ˜¯ Json æ ¼å¼ã€‚

ä¸‹é¢æ˜¯ä½¿ç”¨ OpenAPI 3.1. è§„èŒƒæè¿°äº†ä½ éœ€å¦‚ä½•è¿›è¡Œjsonæ ¼å¼çš„ç”Ÿæˆã€‚

\```json
{_cls}
\```

æ ¹æ®ç”¨çš„æˆ·é—®é¢˜,{func.__doc__}ã€‚ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š{prompt}

è¯·ä½ å®žçŽ°è¿™ä¸ªå‡½æ•°ã€‚
''' 
    
    return msg

llm.setup_impl_func_format_func(chat_model_name,function_impl_format2)
```

The default prompt template function is `function_impl_format`, you can check the source code in `from byzerllm.utils import function_impl_format`.


## LLM-Friendly Function/DataClass

If you want to improve the performance of Function Calling or Response Class, you should make your Function(Tool) and Data Class is LLM-Friendly.  

Let's take a look at the following python code:

```python
def compute_date_range(count:int, unit:str)->List[str]:                   
    now = datetime.datetime.now()
    ....
```

This code is not LLM-Friendly Function since it's difficult to know the usage of this funciton and 
what's the meaning of the input parameters.

The LLM just like human, it's hard to let the LLM know when or how to invoke this function. Especially the parameter `unit`
actually is enum value but the LLM no way to get this message.

So, in order to make the LLM knows more about this function in Byzer-LLM, you should 
follow some requirments:

1. Adding pythonic function comment 
2. Use annotated to provide type and comment for every parameter, if the parameter is a enum, then provide enum values.

Here is the LLM-Friendly fuction definision.

```python
def compute_date_range(count:Annotated[int,"æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»åž‹"],
                       unit:Annotated[str,"æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»åž‹",{"enum":["day","week","month","year"]}])->List[str]:
    '''
    è®¡ç®—æ—¥æœŸèŒƒå›´

    Args:
        count: æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»åž‹
        unit: æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»åž‹ï¼Œå¯é€‰å€¼ä¸º day,week,month,year
    '''        
    now = datetime.datetime.now()
    ....
```

If the LLM make something wrong to your function (e.g. provide the bad parameters), try to optimize the function comment 
and the parameter Annotated comment.

## Model Meta

The Byzer-llm also support get the model meta information. The following code will get the meta information of model instance called `chat`:

```python

```python
llm.get_meta(model="chat")

#output:
# {'model_deploy_type': 'proprietary',
#  'backend': 'ray/vllm',
#  'max_model_len': 32768,
#  'architectures': ['QWenLMHeadModel']}
```

## Chat Template

The different models have different chat templates, the Byzer-LLM have provide some chat templates for the models. You can use the following code to setup the chat template:

```python
from byzerllm.utils.client import Templates
llm.setup_template("chat",Templates.qwen()) 
```

However, we also support the `tokeninzer.apply_chat_template`, you can use the following code to apply the chat template:

```python
llm.setup_template("chat","auto") 
```

If the model is not work well with the `tokeninzer.apply_chat_template`, this function will raise an exception. In this case, you can use the `llm.setup_template` to setup the chat template manually.

You can also use the `llm.get_meta` to check if the model support the `apply_chat_template`:

```python
llm.get_meta(model="chat")
```

The output:

```json
{'model_deploy_type': 'proprietary',
 'backend': 'ray/vllm',
 'support_stream': True,
 'support_chat_template': True,
 'max_model_len': 4096,
 'architectures': ['LlamaForCausalLM']}
```

Notice that this feature will cause additional RPC call, so it will bring some performance penalty.

## Prompt Function && Prompt Class

Prompt Function is a function implemented by text instead of code. The Prompt function will auto bind the function parameters to the 
function's doc string.

Prompt Class is a class which contains the prompt function.

Here is a simple example for prompt function:

```python
@llm.prompt(render="simple")
def generate_answer(context:str,question:str)->str:
    '''
    Answer the question based on only the following context:
    {context}
    Question: {question}
    Answer:
    '''
    pass

context='''
Byzeräº§å“æ ˆä»Žåº•å±‚å­˜å‚¨åˆ°å¤§æ¨¡åž‹ç®¡ç†å’Œservingå†åˆ°åº”ç”¨å¼€å‘æ¡†æž¶ï¼š
1. Byzer-Retrieval, ä¸€ä¸ªæ”¯æŒå‘é‡+æœç´¢çš„æ··åˆæ£€ç´¢æ•°æ®åº“ã€‚
2. Byzer-LLM, å¯ä»¥è¡”æŽ¥SaaS/å¼€æºæ¨¡åž‹ï¼Œèƒ½éƒ¨ç½²ï¼Œå¯ä»¥ç»Ÿä¸€å…¥å£ã€‚
3. Byzer-Agent ,ä¸€å¥—åˆ†å¸ƒå¼ Agent æ¡†æž¶ï¼Œåšä¸ºåº”ç”¨å¼€å‘æ¡†æž¶
4. ByzerPerf, ä¸€å¥—æ€§èƒ½åžåè¯„æµ‹æ¡†æž¶
5. ByzerEvaluation, ä¸€å¥—å¤§æ¨¡åž‹æ•ˆæžœè¯„æµ‹æ¡†æž¶ ï¼ˆæœªå¼€æºï¼‰
6. Byzer-SQLï¼Œ ä¸€å¥—å…¨SQLæ–¹è¨€ï¼Œæ”¯æŒETLï¼Œæ•°æ®åˆ†æžç­‰å·¥ä½œï¼Œå¹¶ä¸”æ”¯æŒå¤§æ¨¡åž‹çš„ç®¡ç†å’Œè°ƒç”¨ï¼ˆmodel as UDFï¼‰,æ–¹ä¾¿æ•°æ®é¢„å¤„ç†ã€‚
'''
print(generate_answer(context=context,question="Byzer SQLæ˜¯ä»€ä¹ˆï¼Ÿ"))
## output: Byzer SQL æ˜¯ä¸€ä¸ªå…¨SQLæ–¹è¨€ï¼Œæ”¯æŒETLã€æ•°æ®åˆ†æžç­‰å·¥ä½œï¼Œå¹¶ä¸”æ”¯æŒå¤§æ¨¡åž‹çš„ç®¡ç†å’Œè°ƒç”¨ï¼ˆmodel as UDFï¼‰ï¼Œæ–¹ä¾¿æ•°æ®é¢„å¤„ç†ã€‚
```

Now we try to convert the prompt function to prompt class:

```python
import ray
from byzerllm.utils.client import ByzerLLM
import byzerllm

ray.init(address="auto",namespace="default",ignore_reinit_error=True)  

class RAG():
    def __init__(self):        
        self.llm = ByzerLLM()
        self.llm.setup_template(model="sparkdesk_chat",template="auto")
        self.llm.setup_default_model_name("sparkdesk_chat")        
    
    @byzerllm.prompt(lambda self: self.llm,render="simple")
    def generate_answer(self,context:str,question:str)->str:
        '''
        Answer the question based on only the following context:
        {context}
        Question: {question}
        Answer:
        '''
        pass

t = RAG()
print(t.generate_answer(context=context,question="Byzer SQLæ˜¯ä»€ä¹ˆï¼Ÿ"))    
```

The first parameter of the `byzerllm.prompt` is the `llm` instance, the following type can be accepted:

1. lambda function: the lambda function will accept the `self` as the parameter, and then you can access the llm instance by self
2. string: the string is the model name, we will create the llm instance by the model name automatically.
3. llm: the llm instance.

Notice that the llm instance we mentioned above is the `ByzerLLM` instance.

If you only want to get the prompt which is bind the parameters instead of execute the prompt, you can use the following code:

```python
import ray
from byzerllm.utils.client import ByzerLLM
import byzerllm

ray.init(address="auto",namespace="default",ignore_reinit_error=True)  

class RAG():
    def __init__(self):        
        self.llm = ByzerLLM()
        self.llm.setup_template(model="sparkdesk_chat",template="auto")
        self.llm.setup_default_model_name("sparkdesk_chat")        
    
    @byzerllm.prompt(render="simple")
    def generate_answer(self,context:str,question:str)->str:
        '''
        Answer the question based on only the following context:
        {context}
        Question: {question}
        Answer:
        '''
        pass

t = RAG()
print(t.generate_answer(context=context,question="Byzer SQLæ˜¯ä»€ä¹ˆï¼Ÿ"))
```

The difference between the two code is that the `byzerllm.prompt whether have delived the `llm` instance to the prompt function.

Here is the output:

```text

Answer the question based on only the following context:

Byzeräº§å“æ ˆä»Žåº•å±‚å­˜å‚¨åˆ°å¤§æ¨¡åž‹ç®¡ç†å’Œservingå†åˆ°åº”ç”¨å¼€å‘æ¡†æž¶ï¼š
1. Byzer-Retrieval, ä¸€ä¸ªæ”¯æŒå‘é‡+æœç´¢çš„æ··åˆæ£€ç´¢æ•°æ®åº“ã€‚
2. Byzer-LLM, å¯ä»¥è¡”æŽ¥SaaS/å¼€æºæ¨¡åž‹ï¼Œèƒ½éƒ¨ç½²ï¼Œå¯ä»¥ç»Ÿä¸€å…¥å£ã€‚
3. Byzer-Agent ,ä¸€å¥—åˆ†å¸ƒå¼ Agent æ¡†æž¶ï¼Œåšä¸ºåº”ç”¨å¼€å‘æ¡†æž¶
4. ByzerPerf, ä¸€å¥—æ€§èƒ½åžåè¯„æµ‹æ¡†æž¶
5. ByzerEvaluation, ä¸€å¥—å¤§æ¨¡åž‹æ•ˆæžœè¯„æµ‹æ¡†æž¶ ï¼ˆæœªå¼€æºï¼‰
6. Byzer-SQLï¼Œ ä¸€å¥—å…¨SQLæ–¹è¨€ï¼Œæ”¯æŒETLï¼Œæ•°æ®åˆ†æžç­‰å·¥ä½œï¼Œå¹¶ä¸”æ”¯æŒå¤§æ¨¡åž‹çš„ç®¡ç†å’Œè°ƒç”¨ï¼ˆmodel as UDFï¼‰,æ–¹ä¾¿æ•°æ®é¢„å¤„ç†ã€‚

Question: Byzer SQLæ˜¯ä»€ä¹ˆï¼Ÿ
Answer:
```

Prompt function support two kinds of return value:

1. str
2. pydanitc.BaseModel

Here is the example for pydanitc.BaseModel:

```python

import ray
import functools
import inspect
import byzerllm
import pydantic

ray.init(address="auto",namespace="default",ignore_reinit_error=True)  

class ByzerProductDesc(pydantic.BaseModel):
    byzer_retrieval: str
    byzer_llm: str
    byzer_agent: str
    byzer_perf: str
    byzer_evaluation: str
    byzer_sql: str

class RAG():
    def __init__(self):        
        self.llm = ByzerLLM()
        self.llm.setup_template(model="sparkdesk_chat",template="auto")
        self.llm.setup_default_model_name("sparkdesk_chat")        
    
    @byzerllm.prompt(lambda self: self.llm,render="simple")
    def generate_answer(self,context:str,question:str)->ByzerProductDesc:
        '''
        Answer the question based on only the following context:
        {context}
        Question: {question}
        Answer:
        '''
        pass

t = RAG()

byzer_product = t.generate_answer(context=context,question="Byzer äº§å“åˆ—è¡¨")
print(byzer_product.byzer_sql)
## output: ä¸€å¥—å…¨SQLæ–¹è¨€ï¼Œæ”¯æŒETLï¼Œæ•°æ®åˆ†æžç­‰å·¥ä½œï¼Œå¹¶ä¸”æ”¯æŒå¤§æ¨¡åž‹çš„
```

As we know, the prompt function is a function implemented by text instead of code, the lucky thing is that we can `program`
in the text, we introduce the jinjia2 to support this feature. Here is the example:

```python
import ray
import functools
import inspect
import byzerllm
import pydantic
from byzerllm.utils.client import ByzerLLM

ray.init(address="auto",namespace="default",ignore_reinit_error=True)  

data = {
    'name': 'Jane Doe',
    'task_count': 3,
    'tasks': [
        {'name': 'Submit report', 'due_date': '2024-03-10'},
        {'name': 'Finish project', 'due_date': '2024-03-15'},
        {'name': 'Reply to emails', 'due_date': '2024-03-08'}
    ]
}


class RAG():
    def __init__(self):        
        self.llm = ByzerLLM()
        self.llm.setup_template(model="sparkdesk_chat",template="auto")
        self.llm.setup_default_model_name("sparkdesk_chat")        
    
    @byzerllm.prompt(lambda self:self.llm,render="jinja2")
    def generate_answer(self,name,task_count,tasks)->str:
        '''
        Hello {{ name }},

        This is a reminder that you have {{ task_count }} pending tasks:
        {% for task in tasks %}
        - Task: {{ task.name }} | Due: {{ task.due_date }}
        {% endfor %}

        Best regards,
        Your Reminder System
        '''
        pass

t = RAG()

response = t.generate_answer(**data)
print(response)

## output:Hello! Is there anything else I can assist you with?
```

If you want to handle the parameters in the prompt function using Python code instead of Jinjia2, you can use the following code:

```python
import byzerllm

byzerllm.connect_cluster()

data = {
    'name': 'Jane Doe',
    'task_count': 3,
    'tasks': [
        {'name': 'Submit report', 'due_date': '2024-03-10'},
        {'name': 'Finish project', 'due_date': '2024-03-15'},
        {'name': 'Reply to emails', 'due_date': '2024-03-08'}
    ]
}


class RAG():
    def __init__(self):        
        self.llm = byzerllm.ByzerLLM()
        self.llm.setup_template(model="sparkdesk_chat",template="auto")
        self.llm.setup_default_model_name("sparkdesk_chat")        
    
    @byzerllm.prompt(lambda self:self.llm,render="jinja2")
    def generate_answer(self,name,task_count,tasks)->str:
        '''
        Hello {{ name }},

        This is a reminder that you have {{ task_count }} pending tasks:
            
        {{ tasks }}

        Best regards,
        Your Reminder System
        '''
        
        tasks_str = "\n".join([f"- Task: {task['name']} | Due: { task['due_date'] }" for task in tasks])
        return {"tasks": tasks_str}

t = RAG()

response = t.generate_answer(**data)
print(response)
```

Make sure the return value is a dict, and the key is the parameter name in the prompt function. 
The return value will override the parameter value and then use the new value to render the prompt function.



## LLM Default Generation Parameters

The Byzer-llm also support setup the default generation parameters for the model. The following code will setup the default generation parameters for the model instance called `chat`:

```python
llm.setup_extra_generation_params("chat",{
    "generation.stop_token_ids":[7]
})
```

In this case, the `generation.stop_token_ids` will be set to `[7]` for the model instance `chat`. Every time you call the `chat` model, the `generation.stop_token_ids` will be set to `[7]` automatically.

## Pin Model Worker Mapping

Byzer-LLM have multi-type requests to the model instance.

1. embedding
2. tokenizer
3. apply_chat_template
4. meta
5. complete/chat

And in Byzer-LLM you can start one model instance which have multi workers,e.g.

```python
llm.setup_gpus_per_worker(2).setup_num_workers(4).setup_infer_backend(InferBackend.VLLM)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/auto",
    udf_name="chat",
    infer_params={"backend.gpu_memory_utilization":0.8,
                    "backend.enforce_eager":False,
                    "backend.trust_remote_code":True,
                    "backend.max_model_len":1024*4,
                    "backend.quantization":"gptq",
                    }
)
```

Here you deploy one model instance with 4 workers, and each worker have 2 gpus, each worker is a vLLM instance.
The ByzerLLM will auto route the request to the worker, and the worker is selected by the LRU algorithm.

If you want to the request like embedding,tokenizer,apply_chat_template,meta to be processed by the same worker, you can use the following code:

```python
llm.setup_pin_model_worker_mapping({
                "embedding":0,
                "tokenizer":0,
                "apply_chat_template":0,
                "meta":0,
            } 
```

this will make sure the request like embedding,tokenizer,apply_chat_template,meta to be processed by the worker 0.

## Model Worker Load Balance

In Byzer-LLM you can start one model instance which have multi workers,e.g.

```python
llm.setup_gpus_per_worker(2).setup_num_workers(4).setup_infer_backend(InferBackend.VLLM)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/auto",
    udf_name="chat",
    infer_params={"backend.gpu_memory_utilization":0.8,
                    "backend.enforce_eager":False,
                    "backend.trust_remote_code":True,
                    "backend.max_model_len":1024*4,
                    "backend.quantization":"gptq",
                    }
)
```

Here you deploy one model instance with 4 workers, each worker have 2 gpus, and each worker is a vLLM instance.
By default, the ByzerLLM will auto route the request to the worker, and the worker is selected by the LRU algorithm.
You can change this behavior by using the following code:

```python
llm.setup_gpus_per_worker(8).setup_num_workers(1).setup_infer_backend(InferBackend.VLLM)
llm.setup_worker_concurrency(999)
llm.setup_load_balance_way("round_robin")
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/auto",
    udf_name=chat_model_name,
    infer_params={"backend.gpu_memory_utilization":0.8,
                    "backend.enforce_eager":False,
                    "backend.trust_remote_code":True,
                    "backend.max_model_len":1024*4,
                    "backend.quantization":"gptq",
                    }
)
```

Here you can use the `setup_worker_concurrency` to setup the worker concurrency, and use the `setup_load_balance_way` to setup the load balance way. For now, the Byzer-LLM support the following load balance way:

1. round_robin
2. lru


## Multi Modal 

The Byzer-llm also support multi modal. The following code will deploy a multi modal model and then use the model to infer the input text.

```python
import ray
from byzerllm.utils.client import ByzerLLM,InferBackend

ray.init(address="auto",namespace="default")   

llm = ByzerLLM()
chat_model_name = "qwen_vl_chat"
model_location = "/home/byzerllm/models/Qwen-VL-Chat"

llm.setup_gpus_per_worker(1).setup_num_workers(1).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/qwen_vl_chat",
    udf_name=chat_model_name,
    infer_params={}
)    
```

Then you can use the model to chat:

```python
import base64
image_path = "/home/byzerllm/projects/jupyter-workspace/1.jpg"
with open(image_path, "rb") as f:
    image_content = base64.b64encode(f.read()).decode("utf-8")

t = llm.chat_oai(conversations=[{
    "role": "user",
    "content": "è¿™æ˜¯ä»€ä¹ˆ"
}],model=chat_model_name,llm_config={"image":image_content})

t[0].output

# '{"response": "å›¾ä¸­æ˜¯ä¸€åå¥³å­åœ¨æ²™æ»©ä¸Šå’Œç‹—çŽ©è€ï¼Œæ—è¾¹çš„ç‹—æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ï¼Œå®ƒååœ¨æ²™æ»©ä¸Šï¼Œé¢å¯¹ç€ä¸€åèº«ç©¿æ ¼å­è¡¬è¡«çš„å¥³å­ã€‚å¥³å­çš„è…¿æœ‰äº›æ®‹ç–¾ï¼Œä½†æ˜¯å¥¹ä¾ç„¶åšæŒååœ¨æ²™æ»©ä¸Šå’Œç‹—çŽ©è€ã€‚å¥¹çš„å³æ‰‹æ‹¿ç€ä¸€ä¸ªå°çŽ©å…·ï¼Œè¿™ä¸ªçŽ©å…·ä¸Šé¢æœ‰ä¸¤è¡Œé»‘è‰²å­—æ¯ï¼Œå…·ä½“æ˜¯ä»€ä¹ˆå†…å®¹çœ‹ä¸æ¸…æ¥šã€‚å¥¹æ‰“ç®—æŠŠçŽ©å…·æ‰”ç»™æ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚", "history": [{"role": "user", "content": "Picture 1: <img>/tmp/byzerllm/visualglm/images/23eb4cea-cb6e-4f55-8adf-3179ca92ab42.jpg</img>\\nè¿™æ˜¯ä»€ä¹ˆ"}, {"role": "assistant", "content": "å›¾ä¸­æ˜¯ä¸€åå¥³å­åœ¨æ²™æ»©ä¸Šå’Œç‹—çŽ©è€ï¼Œæ—è¾¹çš„ç‹—æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ï¼Œå®ƒååœ¨æ²™æ»©ä¸Šï¼Œé¢å¯¹ç€ä¸€åèº«ç©¿æ ¼å­è¡¬è¡«çš„å¥³å­ã€‚å¥³å­çš„è…¿æœ‰äº›æ®‹ç–¾ï¼Œä½†æ˜¯å¥¹ä¾ç„¶åšæŒååœ¨æ²™æ»©ä¸Šå’Œç‹—çŽ©è€ã€‚å¥¹çš„å³æ‰‹æ‹¿ç€ä¸€ä¸ªå°çŽ©å…·ï¼Œè¿™ä¸ªçŽ©å…·ä¸Šé¢æœ‰ä¸¤è¡Œé»‘è‰²å­—æ¯ï¼Œå…·ä½“æ˜¯ä»€ä¹ˆå†…å®¹çœ‹ä¸æ¸…æ¥šã€‚å¥¹æ‰“ç®—æŠŠçŽ©å…·æ‰”ç»™æ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚"}]}'
```

You can chat multi rounds with the following code:

```python
import json
history = json.loads(t[0].output)["history"]

llm.chat_oai(conversations=history+[{
    "role": "user",
    "content": "èƒ½åœˆå‡ºç‹—ä¹ˆï¼Ÿ"
}],model=chat_model_name,llm_config={"image":image_content})

# [LLMResponse(output='{"response": "<ref>ç‹—</ref><box>(221,425),(511,889)</box>", "history": [{"role"
```

Get the history from the previous chat, then add the hisotry to new conversation, then chat again.

## StableDiffusion

The Byzer-llm also support StableDiffusion as the inference backend. The following code will deploy a StableDiffusion model and then use the model to infer the input text.

```python
import ray
from byzerllm.utils.client import ByzerLLM,InferBackend

ray.init(address="auto",namespace="default")   

llm = ByzerLLM()
chat_model_name = "sd_chat"
model_location = "/home/byzerllm/models/stable-diffusion-v1-5"

llm.setup_gpus_per_worker(2).setup_num_workers(1).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/stable_diffusion",
    udf_name=chat_model_name,
    infer_params={}
)

def show_image(content):
    from IPython.display import display, Image
    import base64             
    img = Image(base64.b64decode(content))
    display(img)    
    
```

Then you can chat with the model:

```python
import json
t = llm.chat_oai(
    conversations=[
        {
            "role":"user",
            "content":"ç”»ä¸€åªçŒ«"
        }
    ],model=chat_model_name,llm_config={"gen.batch_size":3}
)

cats = json.loads(t[0].output)
for res in cats:
    show_image(res["img64"])
```

The output:

![](./images/cat2.png)

The parameters:

| å‚æ•°                        | å«ä¹‰                                                         | é»˜è®¤å€¼   |
| --------------------------- | ------------------------------------------------------------ | -------- |
| Instruction                 | prompt                                                       | éžç©º     |
| generation.negative_prompt  | åå‘çš„prompt                                                 | ""       |
| generation.sampler_name     | è°ƒåº¦å(unpic, euler_a,euler,ddim,ddpm,deis,dpm2,dpm2-a,dpm++_2m,dpm++_2m_karras,heun,heun_karras,lms,pndm:w) | euler_a  |
| generation.sampling_steps   | ç”Ÿæˆçš„æ­¥éª¤æ•°                                                 | 25       |
| generation.batch_size       | ä¸€æ¬¡ç”Ÿæˆå‡ å¼                                                  | 1        |
| generation.batch_count      | ç”Ÿæˆå‡ æ¬¡                                                     | 1        |
| generation.cfg_scale        | éšæœºæˆ–è´´åˆç¨‹åº¦å€¼,å€¼è¶Šå°ç”Ÿæˆçš„å›¾ç‰‡ç¦»ä½ çš„Tagsæè¿°çš„å†…å®¹å·®è·è¶Šå¤§ | 7.5      |
| generation.seed             | éšæœºç§å­                                                     | -1       |
| generation.width            | å›¾ç‰‡å®½åº¦                                                     | 768      |
| generation.height           | å›¾ç‰‡é«˜åº¦                                                     | 768      |
| generation.enable_hires     | å¼€å¯é«˜åˆ†è¾¨çŽ‡ä¿®å¤åŠŸèƒ½(å’Œä¸‹é¢ä¸¤ä¸ªä¸€ç»„)                         | false    |
| generation.upscaler_mode    | æ”¾å¤§ç®—æ³•(bilinear, bilinear-antialiased,bicubic,bicubic-antialiased,nearest,nearest-exact) | bilinear |
| generation.scale_slider     | æ”¾å¤§æ¯”ä¾‹                                                     | 1.5      |
| generation.enable_multidiff | å›¾ç‰‡åˆ†å‰²å¤„ç†(å‡å°‘æ˜¾å­˜é”€è€—)(å’Œä¸‹é¢3ä¸ªä¸€ç»„)                    | false    |
| generation.views_batch_size | åˆ†æ‰¹å¤„ç†è§„æ¨¡                                                 | 4        |
| generation.window_size      | åˆ‡å‰²å¤§å°ï¼Œå®½ï¼Œé«˜                                             | 64       |
| generation.stride           | æ­¥é•¿                                                         | 16       |
| generation.init_image       | åˆå§‹åŒ–å›¾ç‰‡ï¼ŒåŸºäºŽè¿™ä¸ªå›¾ç‰‡å¤„ç†(å¿…é¡»ä¼ è¾“base64åŠ å¯†çš„å›¾ç‰‡) (å’Œä¸‹é¢çš„ä¸€ç»„) | None     |
| generation.strength         | é‡ç»˜å¹…åº¦: å›¾åƒæ¨¡ä»¿è‡ªç”±åº¦ï¼Œè¶Šé«˜è¶Šè‡ªç”±å‘æŒ¥ï¼Œè¶Šä½Žå’Œå‚è€ƒå›¾åƒè¶ŠæŽ¥è¿‘ï¼Œé€šå¸¸å°äºŽ0.3åŸºæœ¬å°±æ˜¯åŠ æ»¤é•œ | 0.5      |



## SQL Support

In addition to the Python API, Byzer-llm also support SQL API. In order to use the SQL API, you should install Byzer-SQL language first.

Try to install the Byzer-SQL language with the following command:

```bash
git clone https://gitee.com/allwefantasy/byzer-llm
cd byzer-llm/setup-machine
sudo -i 
ROLE=master ./setup-machine.sh
```

After the installation, you can visit the Byzer Console at http://localhost:9002. 

In the Byzer Console, you can run the following SQL to deploy a llama2 model which have the same effect as the Python code above.

```sql
!byzerllm setup single;
!byzerllm setup "num_gpus=4";
!byzerllm setup "maxConcurrency=1";
!byzerllm setup "infer_backend=transformers";

run command as LLM.`` where 
action="infer"
and pretrainedModelType="custom/llama2"
and localModelDir="/home/byzerllm/models/openbuddy-llama-13b-v5-fp16"
and reconnect="false"
and udfName="llama2_chat"
and modelTable="command";

```

Then you can invoke the model with UDF `llama2_chat`:

```sql

select 
llama2_chat(llm_param(map(
              "user_role","User",
              "assistant_role","Assistant",
              "system_msg",'You are a helpful assistant. Think it over and answer the user question correctly.',
              "instruction",llm_prompt('
Please remenber my name: {0}              
',array("Zhu William"))

))) as q 
as q1;
```

Once you deploy the model with `run command as LLM`, then you can ues the model as a SQL function. This feature is very useful for data scientists who want to use LLM in their data analysis or data engineers who want to use LLM in their data pipeline.

---

### QWen

If you use QWen in ByzerLLM, you should sepcify the following parameters mannualy:

1. the role mapping 
2. the stop_token_ids
3. trim the stop tokens from the output

However, we provide a template for this, try to the following code:

```python
from byzerllm.utils.client import Templates

### Here,we setup the template for qwen
llm.setup_template("chat",Templates.qwen())

t = llm.chat_oai(conversations=[{
    "role":"user",
    "content":"ä½ å¥½,ç»™æˆ‘è®²ä¸ª100å­—çš„ç¬‘è¯å§?"
}])
print(t)
```

---
## SaaS Models

Since the different SaaS models have different parameters, here we provide some templates for the SaaS models to help you deploy the SaaS models.


### baichuan/ç™¾å·

```python

import ray
from byzerllm.utils.client import ByzerLLM

ray.init(address="auto",namespace="default",ignore_reinit_error=True)   

llm = ByzerLLM(verbose=True)

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "baichuan_chat2"
if llm.is_model_exist(chat_name):
    llm.undeploy(udf_name=chat_name)

llm.deploy(model_path="",
           pretrained_model_type="saas/baichuan",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxxxxx",                        
            "saas.model":"Baichuan2-Turbo"
           })

llm.chat_oai(model=chat_name,conversations=[{
    "role":"user",
    "content":"ä½ å¥½",
}])           
```
There are some enum values for the `saas.model`:

1. Baichuan2-Turbo
2. Baichuan-Text-Embedding

### qianwen/é€šä¹‰åƒé—®

```python
from byzerllm.utils.client import ByzerLLM
llm = ByzerLLM()

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "qianwen_chat"

llm.deploy(model_path="",
           pretrained_model_type="saas/qianwen",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxxxxx",            
            "saas.model":"qwen-turbo"
           })

## here you can use `stream_chat_oai`
v = llm.stream_chat_oai(model=chat_name,conversations=[{
    "role":"user",
    "content":"ä½ å¥½ï¼Œä½ æ˜¯è°",
}],llm_config={"gen.incremental_output":False})

for t in v:
    print(t,flush=True)           
```

There are some enum values for the `saas.model`:

1. qwen-turbo
2. qwen-max

### yi_vl_plus/01ä¸‡ç‰©å¤šæ¨¡æ€

Deploy:

```shell
byzerllm deploy  --pretrained_model_type saas/official_openai --infer_params saas.api_key=xxxxx saas.model=yi-vl-plus saas.base_url=https://api.lingyiwanwu.com/v1 --model yi_vl_chat
```    

Chat:
    
```python
from dataclasses import asdict
import json
import base64

image_url = base64.b64encode(open("/home/winubuntu/projects/jupyter-workspace/H/jupyter-workspace/1.jpg","rb").read()).decode()

v = llm.chat_oai(model="yi_vl_chat",conversations=[{
    "role":"user",
    "content":json.dumps([{        
        "text":"è¯¦ç»†æè¿°å›¾ç‰‡é‡Œéƒ½æœ‰å•¥"
    },{        
        "image":f'data:image/jpg,{image_url}'
    }],ensure_ascii=False)
}])

v
```



### qianwen_vl/é€šä¹‰åƒé—®å¤šæ¨¡æ€

```python
import os
os.environ["RAY_DEDUP_LOGS"] = "0" 

import ray
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.utils.client import ByzerLLM,LLMRequest,LLMResponse,LLMHistoryItem,InferBackend
from byzerllm.utils.client import Templates

ray.init(address="auto",namespace="default",ignore_reinit_error=True)  

llm = ByzerLLM(verbose=True)

llm.setup_cpus_per_worker(0.001).setup_num_workers(1).setup_gpus_per_worker(0)


chat_name = "qianwen_vl_chat"
if llm.is_model_exist(chat_name):
    llm.undeploy(udf_name=chat_name)

llm.deploy(model_path="",
           pretrained_model_type="saas/qianwen_vl",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxxx" ,
            "saas.model":"qwen-vl-plus"           
           })
```

There are some enum values for the `saas.model`:

1. qwen-vl-plus
2. qwen-vl-max

You can call the model like this:

```python
from dataclasses import asdict
import json
import base64

import byzerllm

byzerllm.connect_cluster()

model_name = "qianwen_vl_chat"
llm = byzerllm.ByzerLLM()
llm.setup_template(model=model_name,template="auto")
llm.setup_default_model_name(model_name)

path = "/home/winubuntu/projects/jupyter-workspace/H/jupyter-workspace/serving/t.png"

with open(path, 'rb') as image_file:
    image = base64.b64encode(image_file.read()).decode('utf-8')

t = llm.chat_oai(conversations=[{
    "role":"user",
    "content":json.dumps([{
        "image":image,
        "text":"å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆ"
    }],ensure_ascii=False)
}])
print(t[0].output)
```


### azure openai

```sql

!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/azure_openai"
and `saas.api_type`="azure"
and `saas.api_key`="xxx"
and `saas.api_base`="xxx"
and `saas.api_version`="2023-07-01-preview"
and `saas.deployment_id`="xxxxx"
and udfName="azure_openai"
and modelTable="command";
```

### openai

```sql

import ray

from byzerllm.utils.client import ByzerLLM

ray.init(address="auto",namespace="default",ignore_reinit_error=True)  

llm = ByzerLLM()

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "openai_chat"

llm.deploy(model_path="",
           pretrained_model_type="saas/official_openai",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxx",            
            "saas.model":"gpt-3.5-turbo-1106"
           })
```

If you need to use proxy, you can setup the proxy with the following code:

```python
llm.deploy(model_path="",
           pretrained_model_type="saas/official_openai",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxx",            
            "saas.model":"gpt-3.5-turbo-1106"
            "saas.base_url": "http://my.test.server.example.com:8083",
            "saas.proxies":"http://my.test.proxy.example.com"
            "saas.local_address":"0.0.0.0"
           })
```


### zhipu/æ™ºè°±

```python
import ray

from byzerllm.utils.client import ByzerLLM

ray.init(address="auto",namespace="default",ignore_reinit_error=True)  

llm = ByzerLLM()

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "zhipu_chat"

llm.deploy(model_path="",
           pretrained_model_type="saas/zhipu",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxx",            
            "saas.model":"glm-4"
           })
```

There are some enum values for the `saas.model`:

1. glm-4
2. embedding-2

### minimax

```sql

!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/minimax"
and `saas.api_key`="xxxxxxxxxxxxxxxxxx"
and `saas.group_id`="xxxxxxxxxxxxxxxx"
and `saas.model`="abab5.5-chat"
and `saas.api_url`="https://api.minimax.chat/v1/text/chatcompletion_pro"
and udfName="minimax_saas"
and modelTable="command";

```

### sparkdesk/æ˜Ÿç«

#### Command Line

```bash
byzerllm deploy --pretrained_model_type saas/sparkdesk \
--cpus_per_worker 0.001 \
--gpus_per_worker 0 \
--num_workers 1 \
--infer_params saas.appid=xxxx  saas.api_key=xxxxx saas.api_secret=xxxxxx  saas.gpt_url="wss://spark-api.xf-yun.com/v3.5/chat" \
--model sparkdesk_chat
```

validate the model:

```bash
byzerllm query --model sparkdesk_chat --query 'ä½ å¥½'
```

un-deploy the model:

```bash
byzerllm undeploy --model sparkdesk_chat
```

#### Python

```python
import ray

from byzerllm.utils.client import ByzerLLM

ray.init(address="auto",namespace="default",ignore_reinit_error=True)

llm = ByzerLLM()

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "sparkdesk_saas"

if llm.is_model_exist(chat_name):
  llm.undeploy(udf_name=chat_name)

llm.deploy(model_path="",
           pretrained_model_type="saas/sparkdesk",
           udf_name=chat_name,
           infer_params={
             "saas.appid":"xxxxxxx",
             "saas.api_key":"xxxxxxx",
             "saas.api_secret":"xxxxxxx",
             "saas.gpt_url":"wss://spark-api.xf-yun.com/v3.1/chat",
             "saas.domain":"generalv3"
           })

v = llm.chat_oai(model=chat_name,conversations=[{
  "role":"user",
  "content":"your prompt content",
}])
```
sparkdesk V1.5 request URLï¼Œassociated domain parameter is generalï¼š  
`wss://spark-api.xf-yun.com/v1.1/chat`  
sparkdesk V2 request URLï¼Œassociated domain parameter is generalv2ï¼š  
`wss://spark-api.xf-yun.com/v2.1/chat`  
sparkdesk V3 request URLï¼Œassociated domain parameter is generalv3 (Function Call feature is now supported.)ï¼š  
`wss://spark-api.xf-yun.com/v3.1/chat`  

#### Byzer-SQL

```sql
!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/sparkdesk"
and `saas.appid`="xxxxxxxxxxxxxxxxxx"
and `saas.api_key`="xxxxxxxxxxxxxxxx"
and `saas.api_secret`="xxxx"
and `gpt_url`="ws://spark-api.xf-yun.com/v1.1/chat"
and udfName="sparkdesk_saas"
and modelTable="command";
```

### AmazonBedrock

```python
import ray

from byzerllm.utils.client import ByzerLLM, Templates

ray.init(address="auto",namespace="default",ignore_reinit_error=True)

llm = ByzerLLM()

chat_name = "aws_bedrock_llama2_70b_chat"

llm.setup_num_workers(1).setup_gpus_per_worker(0)

if llm.is_model_exist(chat_name):
  llm.undeploy(udf_name=chat_name)

llm.deploy(model_path="",
           pretrained_model_type="saas/aws_bedrock",
           udf_name=chat_name,
           infer_params={
               "saas.aws_access_key": "your access key",
               "saas.aws_secret_key": "your secret key",
               "saas.region_name": "your region name",
               "saas.model_api_version": "model api version",
               "saas.model": "meta.llama2-70b-chat-v1"
           })

v = llm.chat_oai(model=chat_name,conversations=[{
  "role":"user",
  "content":"your prompt content",
}])
```

There are some enum values for the `saas.model`:

1. meta.llama2-70b-chat-v1
2. meta.llama2-13b-chat-v1
3. anthropic.claude-3-sonnet-20240229-v1:0
4. anthropic.claude-3-haiku-20240307-v1:0

### Claude

```bash
byzerllm deploy --pretrained_model_type saas/claude \
--cpus_per_worker 0.001 \
--gpus_per_worker 0 \
--num_workers 1 \
--infer_params saas.api_key=xxxxx saas.model=claude-3-haiku-20240307 \
--model haiku_chat
```

Here is the list of the claude models:

```
Claude 3 Opus	claude-3-opus-20240229
Claude 3 Sonnet	claude-3-sonnet-20240229
Claude 3 Haiku	claude-3-haiku-20240307
```
### Gemini

```bash
byzerllm deploy --pretrained_model_type saas/gemini \
--cpus_per_worker 0.001 \
--gpus_per_worker 0 \
--num_workers 1 \
--infer_params saas.api_key=xxxxx saas.model=gemini-pro \
--model gemini_chat
```

### Moonshot/Kimi

You can use the saas/offical_openai to deploy the Kimi model.

```bash
byzerllm deploy --pretrained_model_type saas/official_openai \
--cpus_per_worker 0.001 \
--gpus_per_worker 0 \
--num_workers 2 \
--infer_params saas.api_key=xxxxx saas.base_url="https://api.moonshot.cn/v1" saas.model=moonshot-v1-8k \
--model kimi_8k_chat
```

### volcano_tts

Deploy:

```bash
byzerllm deploy --pretrained_model_type saas/volcengine \
--cpus_per_worker 0.001 \
--gpus_per_worker 0 \
--num_workers 1 \
--infer_params saas.api_key=xxxxx saas.app_id=xxxx saas.model=volcano_tts \
--model volcano_tts
```

Usage:

```python
import byzerllm
import base64
import json

byzerllm.connect_cluster()

llm = byzerllm.ByzerLLM()
llm.setup_default_model_name("volcano_tts")


t = llm.chat_oai(conversations=[{
    "role":"user",
    "content": json.dumps({
        "input":"ä½ å¥½ï¼Œå¤§å¸…å“¥",
        "voice": "BV705_streaming",
        "response_format": "mp3"
    },ensure_ascii=False)
}])

with open("voice.mp3","wb") as f:
    f.write(base64.b64decode(t[0].output))
```

Or stream mode:

```python
import byzerllm
import base64
import json

byzerllm.connect_cluster()

llm = byzerllm.ByzerLLM()
llm.setup_default_model_name("volcano_tts")

t = llm.stream_chat_oai(conversations=[{
    "role":"user",
    "content": json.dumps({
        "input":"ä½ å¥½ï¼Œå¤§å¸…å“¥",
        "voice": "BV705_streaming",
        "response_format": "mp3"
    },ensure_ascii=False)
}])

with open("voice.mp3","ab") as f:
    for s in t:        
        f.write(s[0])
```

Voice List: https://www.volcengine.com/docs/6561/97465



### azure_tts

Deploy:

```bash
byzerllm deploy --pretrained_model_type saas/azure \
--cpus_per_worker 0.001 \
--gpus_per_worker 0 \
--num_workers 1 \
--infer_params saas.api_key=${MODEL_AZURE_TTS_TOKEN} saas.service_region=eastus \
--model azure_tts
```


Python:

```python
import byzerllm
import base64
import json

byzerllm.connect_cluster()

llm = byzerllm.ByzerLLM()
llm.setup_default_model_name("azure_tts")


t = llm.chat_oai(conversations=[{
    "role":"user",
    "content": json.dumps({
        "input":"ä½ å¥½ï¼Œazure_tts",
        "voice": "zh-CN-XiaoxiaoNeural",
        "response_format": "mp3"
    },ensure_ascii=False)
}])

with open("voice.mp3","wb") as f:
    f.write(base64.b64decode(t[0].output))
```

Or stream mode:

```python
import byzerllm
import base64
import json

byzerllm.connect_cluster()

llm = byzerllm.ByzerLLM()
llm.setup_default_model_name("azure_tts")

t = llm.stream_chat_oai(conversations=[{
    "role":"user",
    "content": json.dumps({
        "input":"ä½ å¥½ï¼Œæµå¼ azure_tts",
        "voice": "zh-CN-XiaoxiaoNeural",
        "response_format": "mp3"
    },ensure_ascii=False)
}])

with open("voice.mp3","ab") as f:
    for s in t:        
        f.write(s[0])
```

Language list: https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=stt
Voice List: https://speech.microsoft.com/portal/voicegallery

### openai_image_generation

Deploy:

```bash
byzerllm deploy --pretrained_model_type saas/openai \
--cpus_per_worker 0.001 \
--gpus_per_worker 0 \
--num_workers 1 \
--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=dall-e-3 \
--model openai_image_gen
```

Python Usage:

```python
import byzerllm
import base64
import json

byzerllm.connect_cluster()

llm = byzerllm.ByzerLLM()
llm.setup_default_model_name("openai_image_gen")


t = llm.chat_oai(conversations=[{
    "role":"user",
    "content": json.dumps({
        "input":"a white siamese cat",
        "size": "1024x1024",
        "quality": "standard"
    },ensure_ascii=False)
}])

with open("output1.jpg","wb") as f:
    f.write(base64.b64decode(t[0].output))
```



### openai_tts

Deploy:

```bash
byzerllm deploy --pretrained_model_type saas/openai \
--cpus_per_worker 0.001 \
--gpus_per_worker 0 \
--num_workers 1 \
--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=tts-1 \
--model openai_tts
```

Python:

```python
import byzerllm
import base64
import json

byzerllm.connect_cluster()

llm = byzerllm.ByzerLLM()
llm.setup_default_model_name("open_tts")


t = llm.chat_oai(conversations=[{
    "role":"user",
    "content": json.dumps({
        "input":"hello, open_tts",
        "voice": "alloy",
        "response_format": "mp3"
    },ensure_ascii=False)
}])

with open("voice.mp3","wb") as f:
    f.write(base64.b64decode(t[0].output))
```

Or stream mode:

```python
import byzerllm
import base64
import json

byzerllm.connect_cluster()

llm = byzerllm.ByzerLLM()
llm.setup_default_model_name("openai_tts")

t = llm.stream_chat_oai(conversations=[{
    "role":"user",
    "content": json.dumps({
        "input":"hello openai_tts",
        "voice": "alloy",
        "response_format": "mp3"
    },ensure_ascii=False)
}])

with open("voice.mp3","ab") as f:
    for s in t:        
        f.write(s[0])
```

Voice List: https://platform.openai.com/docs/guides/text-to-speech


---

## Pretrain

This section will introduce how to pretrain a LLM model with Byzer-llm.  However, for now, the pretrain feature is more mature in Byzer-SQL, so we will introduce the pretrain feature in Byzer-SQL.

```sql
-- Deepspeed Config
set ds_config='''
{
  "gradient_accumulation_steps": 1,
  "train_micro_batch_size_per_gpu": 1,
  "prescale_gradients": false,
  "zero_allow_untested_optimizer": true,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 1e-8,
      "eps": 1.0e-8,
      "betas": [
        0.9,
        0.95
      ],
      "weight_decay": 0.1
    }
  },
  "tensorboard": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 3,
    "offload_optimizer": {
         "device": "cpu"         
     },           
    "offload_param": {
         "device": "cpu"
    },
    "contiguous_gradients": true,
    "allgather_bucket_size": 1e8,
    "reduce_bucket_size": 1e8,
    "overlap_comm": true,
    "reduce_scatter": true
  },
  "steps_per_print": 16,
  "gradient_clipping": 1.0,
  "wall_clock_breakdown": true,
  "bf16": {
    "enabled": true
  }
}
''';

-- load data
load text.`file:///home/byzerllm/data/raw_data/*`
where wholetext="true" as trainData;

select value as text,file from trainData  as newTrainData;

-- split the data into 12 partitions
run newTrainData as TableRepartition.`` where partitionNum="12" and partitionCols="file" 
as finalTrainData;


-- setup env, we use 12 gpus to pretrain the model
!byzerllm setup sfft;
!byzerllm setup "num_gpus=12";

-- specify the pretrain model type and the pretrained model path
run command as LLM.`` where 
and localPathPrefix="/home/byzerllm/models/sfft/jobs"
and pretrainedModelType="sfft/llama2"
-- original model is from
and localModelDir="/home/byzerllm/models/Llama-2-7b-chat-hf"
-- and localDataDir="/home/byzerllm/data/raw_data"

-- we use async mode to pretrain the model, since the pretrain process will take several days or weeks
-- Ray Dashboard will show the tensorboard address, and then you can monitor the loss
and detached="true"
and keepPartitionNum="true"

-- use deepspeed config, this is optional
and deepspeedConfig='''${ds_config}'''


-- the pretrain data is from finalTrainData table
and inputTable="finalTrainData"
and outputTable="llama2_cn"
and model="command"
-- some hyper parameters
and `sfft.int.max_length`="128"
and `sfft.bool.setup_nccl_socket_ifname_by_ip`="true"
;
```

Since the deepspeed checkpoint is not compatible with the huggingface checkpoint, we need to convert the deepspeed checkpoint to the huggingface checkpoint. The following code will convert the deepspeed checkpoint to the huggingface checkpoint.

```sql
!byzerllm setup single;

run command as LLM.`` where 
action="convert"
and pretrainedModelType="deepspeed/llama3b"
and modelNameOrPath="/home/byzerllm/models/base_model"
and checkpointDir="/home/byzerllm/data/checkpoints"
and tag="Epoch-1"
and savePath="/home/byzerllm/models/my_3b_test2";
```


Now you can deploy the converted model :

```sql
-- éƒ¨ç½²hugginface æ¨¡åž‹
!byzerllm setup single;

set node="master";
!byzerllm setup "num_gpus=2";
!byzerllm setup "workerMaxConcurrency=1";

run command as LLM.`` where 
action="infer"
and pretrainedModelType="custom/auto"
and localModelDir="/home/byzerllm/models/my_3b_test2"
and reconnect="false"
and udfName="my_3b_chat"
and modelTable="command";
```

## Finetune

```sql
-- load data, we use the dummy data for finetune
-- data format supported by Byzer-SQLï¼šhttps://docs.byzer.org/#/byzer-lang/zh-cn/byzer-llm/model-sft

load json.`/tmp/upload/dummy_data.jsonl` where
inferSchema="true"
as sft_data;

-- Fintune Llama2
!byzerllm setup sft;
!byzerllm setup "num_gpus=4";

run command as LLM.`` where 
and localPathPrefix="/home/byzerllm/models/sft/jobs"

-- æŒ‡å®šæ¨¡åž‹ç±»åž‹
and pretrainedModelType="sft/llama2"

-- æŒ‡å®šæ¨¡åž‹
and localModelDir="/home/byzerllm/models/Llama-2-7b-chat-hf"
and model="command"

-- æŒ‡å®šå¾®è°ƒæ•°æ®è¡¨
and inputTable="sft_data"

-- è¾“å‡ºæ–°æ¨¡åž‹è¡¨
and outputTable="llama2_300"

-- å¾®è°ƒå‚æ•°
and  detached="true"
and `sft.int.max_seq_length`="512";
```

You can check the finetune actor in the Ray Dashboard, the name of the actor is `sft-william-xxxxx`.

After the finetune actor is finished, you can get the model path, so you can deploy the finetuned model.


Here is the log of the finetune actor:

```
Loading data: /home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_data/data.jsonl3
2
there are 33 data in dataset
*** starting training ***
{'train_runtime': 19.0203, 'train_samples_per_second': 1.735, 'train_steps_per_second': 0.105, 'train_loss': 3.0778136253356934, 'epoch': 0.97}35

***** train metrics *****36  
epoch                    =       0.9737  
train_loss               =     3.077838  
train_runtime            = 0:00:19.0239  
train_samples_per_second =      1.73540  
train_steps_per_second   =      0.10541

[sft-william] Copy /home/byzerllm/models/Llama-2-7b-chat-hf to /home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_model/final/pretrained_model4243              
[sft-william] Train Actor is already finished. You can check the model in: /home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_model/final   
```

You can download the finetuned model from the path `/home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_model/final`, or copy the model to all other node in the Ray cluster.

Try to deploy the finetuned model:

```sql
!byzerllm setup single;
run command as LLM.`` where 
action="infer"
and localPathPrefix="/home/byzerllm/models/infer/jobs"
and localModelDir="/home/byzerllm/models/sft/jobs/sft-william-llama2-alpaca-data-ccb8fb55-382c-49fb-af04-5cbb3966c4e6/finetune_model/final"
and pretrainedModelType="custom/llama2"
and udfName="fintune_llama2_chat"
and modelTable="command";
```

Byzer-LLM use QLora to finetune the model, you can merge the finetuned model with the original model with the following code:

```sql
-- åˆå¹¶lora model + base model

!byzerllm setup single;

run command as LLM.`` where 
action="convert"
and pretrainedModelType="deepspeed/llama"
and model_dir="/home/byzerllm/models/sft/jobs/sft-william-20230912-21-50-10-2529bf9f-493e-40a3-b20f-0369bd01d75d/finetune_model/final/pretrained_model"
and checkpoint_dir="/home/byzerllm/models/sft/jobs/sft-william-20230912-21-50-10-2529bf9f-493e-40a3-b20f-0369bd01d75d/finetune_model/final"
and savePath="/home/byzerllm/models/sft/jobs/sft-william-20230912-21-50-10-2529bf9f-493e-40a3-b20f-0369bd01d75d/finetune_model/merge";

```

## Third-party Libraries

### Llama_index

Llama_index can use byzer-llm to access the LLM model, use Byzer-retrieval as the RAG storage.
Try to use the following code to get service_context and storage_context.

```python
## init the ByzerLLM and ByzerRetrieval
code_search_path=["/home/winubuntu/softwares/byzer-retrieval-lib/"]
env_vars = {"JAVA_HOME": "/home/winubuntu/softwares/jdk-21",
            "PATH":"/home/winubuntu/softwares/jdk-21/bin:/home/winubuntu/.cargo/bin:/usr/local/cuda/bin:/home/winubuntu/softwares/byzer-lang-all-in-one-linux-amd64-3.1.1-2.3.2/jdk8/bin:/home/winubuntu/miniconda3/envs/byzerllm-dev/bin:/home/winubuntu/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin"}
import os
import ray
from byzerllm.apps.llama_index import get_service_context,get_storage_context
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.utils.client import ByzerLLM,Templates

ray.init(address="auto",namespace="default",ignore_reinit_error=True,
                 job_config=ray.job_config.JobConfig(code_search_path=code_search_path,
                                                      runtime_env={"env_vars": env_vars})
                 )  
model_name = "qianwen_chat"
llm = ByzerLLM()
llm.setup_template(model=model_name,template="auto")
llm.setup_default_emb_model_name("emb")
llm.setup_default_model_name(model_name)
llm.setup_extra_generation_params(model_name,extra_generation_params={
    "temperature":0.01,
    "top_p":0.99
})
retrieval = ByzerRetrieval()
retrieval.launch_gateway() 

## get the service_context and storage_context for Llama_index
service_context = get_service_context(llm)
storage_context = get_storage_context(llm,retrieval,chunk_collection="default",namespace="default")
```


## Articles

1. [ä¸€å£æ°”é›†æˆé‚£äº›ä¸ªå¤§æ¨¡åž‹ä½ ä¹Ÿè¯•è¯•](https://www.51xpage.com/ai/yi-kou-qi-ji-cheng-na-xie-ge-da-mo-xing-ni-ye-shi-shi-unknown-unknown-man-man-xue-ai006/)
2. [Byzer-LLM å¿«é€Ÿä½“éªŒæ™ºè°± GLM-4](https://mp.weixin.qq.com/s/Zhzn_C9-dKP4Nq49h8yUxw)
3. [å‡½æ•°å®žçŽ°è¶Šé€šç”¨è¶Šå¥½ï¼Ÿæ¥çœ‹çœ‹ Byzer-LLM çš„ Function Implementation å¸¦æ¥çš„ç¼–ç¨‹æ€æƒ³å¤§å˜åŒ–](https://mp.weixin.qq.com/s/_Sx0eC0WqC2M4K1JY9f49Q)
4. [Byzer-LLM ä¹‹ QWen-VL-Chat/StableDiffusionå¤šæ¨¡æ€è¯»å›¾ï¼Œç”Ÿå›¾](https://mp.weixin.qq.com/s/x4g66QvocE5dUlnL1yF9Dw)
5. [åŸºäºŽByzer-Agent æ¡†æž¶å¼€å‘æ™ºèƒ½æ•°æ®åˆ†æžå·¥å…·](https://mp.weixin.qq.com/s/BcoHUEXF24wTjArc7mwNaw)
6. [Byzer-LLM æ”¯æŒåŒæ—¶å¼€æºå’ŒSaaSç‰ˆé€šä¹‰åƒé—®](https://mp.weixin.qq.com/s/VvzMUV654D7IO0He47nv3A)
7. [ç»™å¼€æºå¤§æ¨¡åž‹å¸¦æ¥Function Callingã€ Respond With Class](https://mp.weixin.qq.com/s/GTVCYUhR_atYMX9ymp0eCg)









##File: /Users/allwefantasy/projects/byzer-llm/README-CN.md
![logo.jpg](https://raw.gitcode.com/allwefantasy11/byzer-llm/attachment/uploads/f5751555-1419-470c-8a33-dfcdc238789d/logo.jpg 'logo.jpg')

<h3 align="center">
ç®€å•ã€é«˜æ•ˆä¸”ä½Žæˆæœ¬çš„é¢„è®­ç»ƒã€å¾®è°ƒä¸ŽæœåŠ¡ï¼Œæƒ åŠå¤§ä¼—
</h3>

<p align="center">
| <a href="./README.md"><b>English</b></a> | <a href="./README-CN.md"><b>ä¸­æ–‡</b></a> |
</p>

---

æœ€æ–°ä¿¡æ¯ðŸ”¥

- [2024/01] Release Byzer-LLM 0.1.39
- [2023/12] Release Byzer-LLM 0.1.30

---

Byzer-LLM åŸºäºŽ Ray æŠ€æœ¯æž„å»ºï¼Œæ˜¯ä¸€æ¬¾è¦†ç›–å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰å®Œæ•´ç”Ÿå‘½å‘¨æœŸçš„è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€å¾®è°ƒã€éƒ¨ç½²åŠæŽ¨ç†æœåŠ¡ç­‰é˜¶æ®µã€‚

Byzer-LLM çš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºŽï¼š

1. å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼šæ”¯æŒé¢„è®­ç»ƒã€å¾®è°ƒã€éƒ¨ç½²å’ŒæŽ¨ç†æœåŠ¡å…¨æµç¨‹
2. å…¼å®¹ Python/SQL API æŽ¥å£
3. åŸºäºŽ Ray æž¶æž„è®¾è®¡ï¼Œä¾¿äºŽè½»æ¾æ‰©å±•

---

* [ç‰ˆæœ¬è®°å½•](#ç‰ˆæœ¬è®°å½•) 
* [å®‰è£…æŒ‡å—](#å®‰è£…æŒ‡å—) 
* [å¿«é€Ÿå…¥é—¨](#å¿«é€Ÿå…¥é—¨) 
* [å¦‚ä½•è¿žæŽ¥æ¥è‡ª Ray é›†ç¾¤å¤–éƒ¨çš„æ¨¡åž‹](#å¦‚ä½•è¿žæŽ¥æ¥è‡ª-Ray-é›†ç¾¤å¤–éƒ¨çš„æ¨¡åž‹)
* åµŒå…¥/é‡æŽ’åº
    * [åµŒå…¥æ¨¡åž‹](#åµŒå…¥æ¨¡åž‹)
    * [åµŒå…¥é‡æŽ’åºæ¨¡åž‹](#åµŒå…¥é‡æŽ’åºæ¨¡åž‹)
* [é‡åŒ–](#é‡åŒ–) 
* [æ”¯æŒçš„æ¨¡åž‹](#æ”¯æŒçš„æ¨¡åž‹) 
* æœåŠ¡ç«¯
    * åŽç«¯
        * [æ”¯æŒ vLLM ](#æ”¯æŒ-vLLM) 
        * [æ”¯æŒ DeepSpeed](#æ”¯æŒ-DeepSpeed) 
    * [Byzer-LLM å…¼å®¹ OpenAI çš„ RESTful API æœåŠ¡](#å…¼å®¹-OpenAI-RESTful-API-æœåŠ¡)
* å¤§è¯­è¨€æ¨¡åž‹ä¸Ž Python
    * [å‡½æ•°è°ƒç”¨](#å‡½æ•°è°ƒç”¨) 
    * [ä½¿ç”¨ pydantic ç±»å“åº”](#å“åº”æ—¶ä½¿ç”¨-pydantic-ç±») 
    * [å‡½æ•°å®žçŽ°](#å‡½æ•°å®žçŽ°åŠŸèƒ½) 
    * [å¯¹ LLM å‹å¥½çš„å‡½æ•°/æ•°æ®ç±»](#å¤§è¯­è¨€æ¨¡åž‹å‹å¥½åž‹å‡½æ•°æ•°æ®ç±») 
* æ¨¡åž‹é…ç½®   
    * [æ¨¡åž‹å…ƒä¿¡æ¯](#æ¨¡åž‹å…ƒä¿¡æ¯) 
    * [èŠå¤©æ¨¡æ¿](#å¯¹è¯æ¨¡æ¿) 
    * [LLM é»˜è®¤å‚æ•°](#LLM-é»˜è®¤å‚æ•°) 
* [SaaS æ¨¡åž‹](#SaaS-æ¨¡åž‹) 
    * [é€šä¹‰åƒé—®](#é€šä¹‰åƒé—®qianwen) 
    * [ç™¾å·](#ç™¾å·baichuan)
    * [azure openai](#azure-openai)
    * [openai](#openai)
    * [æ™ºè°±](#æ™ºè°±zhipu)
    * [æ˜Ÿç«](#æ˜Ÿç«sparkdesk)         
    * [AmazonBedrock](#AmazonBedrock)
* [å¤šæ¨¡æ€](#å¤šæ¨¡æ€) 
* [StableDiffusion](#StableDiffusion)
* [SQL æ”¯æŒ](#SQL-æ”¯æŒ) 
* [é¢„è®­ç»ƒ](#é¢„è®­ç»ƒ)
* [å¾®è°ƒ](#å¾®è°ƒ)
* [æ–‡ç« ](#æ–‡ç« )
* [è´¡çŒ®æŒ‡å—](#è´¡çŒ®æŒ‡å—)

---

## ç‰ˆæœ¬è®°å½•
- 0.1.39ï¼šæå‡å‡½æ•°åŠŸèƒ½å®žçŽ° / æ›´æ–° SaaS å¼€å‘è€…å¥—ä»¶ï¼ˆSDKï¼‰ / é›†æˆ OpenAI å…¼å®¹ API æœåŠ¡
- 0.1.38ï¼šå‡çº§ saas/sparkdask æ¨¡åž‹ç»„ä»¶ / å¼•å…¥åµŒå…¥å¼é‡æŽ’åºæ¨¡åž‹ / å®žçŽ°ä»£ç†æ¶ˆæ¯å­˜å‚¨æ”¯æŒ
- 0.1.37ï¼šå¯¹ saas/zhipu æ¨¡åž‹è¿›è¡Œæ›´æ–°ï¼Œæ‚¨å¯ä»¥é€‰ç”¨ glm-4 æˆ– embedding-2 ç”¨äºŽå¤§è¯­è¨€æ¨¡åž‹æˆ–è€…åµŒå…¥åº”ç”¨åœºæ™¯
- 0.1.36ï¼šä¿®æ­£ç”± Byzer-Agent æ›´æ–°æ‰€å¯¼è‡´çš„æ•°æ®åˆ†æžä»£ç†æ¨¡å—çš„æ•…éšœ
- 0.1.35ï¼šæ–°å¢žç™¾å· SaaS åµŒå…¥å¼æ¨¡åž‹
- 0.1.34ï¼šè¿›ä¸€æ­¥å¼ºåŒ– Byzer-Agent API åŠŸèƒ½å¹¶ä¿®å¤ Byzer-LLM å†…éƒ¨çš„éƒ¨åˆ†é—®é¢˜
- 0.1.33ï¼šè§£å†³å“åº”ç±»å†…éƒ¨é”™è¯¯ / æ–°å¢žå¤šé¡¹å‡½æ•°å®žçŽ°
- 0.1.32ï¼šå¯¹ StableDiffusion è¿›è¡Œæ€§èƒ½ä¼˜åŒ–
- 0.1.31ï¼šå¯ç”¨åŒ…å«ä»¤ç‰Œè®¡æ•°ä¿¡æ¯çš„å®žæ—¶èŠå¤©åŠŸèƒ½ / å¯¹å¤šæ¨¡æ€æ¨¡åž‹èŠå¤©ä½“éªŒè¿›è¡Œäº†ä¼˜åŒ–
- 0.1.30ï¼šåœ¨ vLLM åŽå°åº”ç”¨èŠå¤©æ¨¡æ¿åŠŸèƒ½
- 0.1.29ï¼šæå‡äº† DataAnalysis ä»£ç†çš„åŠŸèƒ½è¡¨çŽ°
- 00.1.28ï¼šä¿®å¤è‹¥å¹²å·²çŸ¥ bug
- 0.1.27ï¼šä¿®å¤è‹¥å¹²å·²çŸ¥ bug
- 0.1.26ï¼šæ”¯æŒ QianWen SaaS å¹³å° / å®žçŽ°å®žæ—¶èŠå¤©åŠŸèƒ½åœ¨ QianWenSaas ä¸­çš„åº”ç”¨ / è§£å†³éƒ¨åˆ† SaaS æ¨¡åž‹å­˜åœ¨çš„é—®é¢˜
- 0.1.24ï¼šæ”¯æŒä»Žæ¨¡åž‹å®žä¾‹ç›´æŽ¥æå–å…ƒæ•°æ®å¹¶è‡ªåŠ¨é…ç½®æ¨¡æ¿
- 0.1.23ï¼šé€šè¿‡ Python API è¿›è¡Œæ¨¡åž‹å¾®è°ƒ / è§£å†³äº†ä¸€äº›çŽ°æœ‰é—®é¢˜
- 0.1.22ï¼šå¢žæ·»äº†å‡½æ•°è°ƒç”¨æ”¯æŒ / å“åº”ç»“æž„é‡‡ç”¨ pydantic ç±»åž‹å®šä¹‰
- 0.1.19ï¼šä¿®å¤äº†åµŒå…¥ç›¸å…³é—®é¢˜
- 0.1.18ï¼šå®žçŽ°äº†æµå¼èŠå¤©åŠŸèƒ½ / åŠ å…¥äº†æ¨¡åž‹æ¨¡æ¿æ”¯æŒ
- 0.1.17ï¼šæ­¤ç‰ˆæœ¬æœªæœ‰å®žè´¨æ€§æ›´æ–°å†…å®¹
- 0.1.16ï¼šå¢žå¼ºäº†é’ˆå¯¹ byzer-retrieval çš„ API åŠŸèƒ½
- 0.1.14ï¼šä¸º byzer-retrieval æ·»åŠ äº†èŽ·å–è¡¨æ ¼(get_tables)å’Œæ•°æ®åº“(get_databases)çš„ API æŽ¥å£
- 0.1.13ï¼šæ”¯æŒ byzer-retrieval èƒ½å¤Ÿå…³é—­é›†ç¾¤æ“ä½œ
- 0.1.12ï¼šåˆæ­¥æ”¯æŒ Python APIï¼ˆå°šå¤„äºŽ alpha æµ‹è¯•é˜¶æ®µï¼‰
- 0.1.5ï¼šæ”¯æŒ Python å°è£…å½¢å¼çš„ [byzer-retrieval](https://github.com/allwefantasy/byzer-retrieval)

---


## å®‰è£…æŒ‡å—

æŽ¨èé…ç½®çŽ¯å¢ƒ:

1. Conda:  python==3.10.11  
2. OS:     ubuntu 22.04
3. Cuda:   12.1.0 (å¯é€‰ï¼Œä»…åœ¨æ‚¨ä½¿ç”¨SaaSæ¨¡åž‹æ—¶ä½¿ç”¨)

```bash
## Make sure you python version is 3.10.11
pip install -r requirements.txt
## Skip this step if you have no Nvidia GPU
pip install vllm==0.2.6
pip install -U byzerllm
ray start --head
```

è‹¥ä½ çš„ CUDA ç‰ˆæœ¬ä¸º 11.8ï¼Œè¯·å‚ç…§ä»¥ä¸‹é“¾æŽ¥æ¥å®‰è£… vLLMï¼š
https://docs.vllm.ai/en/latest/getting_started/installation.html

å®‰è£…è¿‡ç¨‹ä¸­éœ€å…³æ³¨çš„å…³é”®çŽ¯èŠ‚å¦‚ä¸‹ï¼š


```shell
As of now, vLLMâ€™s binaries are compiled on CUDA 12.1 by default. However, you can install vLLM with CUDA 11.8 by running:

# Install vLLM with CUDA 11.8.
export VLLM_VERSION=0.2.6
export PYTHON_VERSION=310
pip install https://github.com/vllm-project/vllm/releases/download/v${VLLM_VERSION}/vllm-${VLLM_VERSION}+cu118-cp${PYTHON_VERSION}-cp${PYTHON_VERSION}-manylinux1_x86_64.whl

# Re-install PyTorch with CUDA 11.8.
pip uninstall torch -y
pip install torch --upgrade --index-url https://download.pytorch.org/whl/cu118

# Re-install xFormers with CUDA 11.8.
pip uninstall xformers -y
pip install --upgrade xformers --index-url https://download.pytorch.org/whl/cu118
```

### åŽŸå§‹æœºå™¨é…ç½®æŒ‡å—

> æœ¬æ–¹æ¡ˆå·²é’ˆå¯¹ Ubuntu 20.04/22.04 ç‰ˆæœ¬å’Œ CentOS 8.0 æ“ä½œç³»ç»Ÿå®Œæˆæµ‹è¯•

è‹¥æ‚¨æ‰‹å¤´çš„è®¡ç®—æœºå°šå¤„äºŽåˆå§‹çŠ¶æ€ï¼Œå³æœªå®‰è£… GPU é©±åŠ¨å’Œ CUDA çŽ¯å¢ƒï¼Œå¯æŒ‰ä»¥ä¸‹æä¾›çš„è„šæœ¬æ­¥éª¤è½»æ¾å®Œæˆæœºå™¨é…ç½®ï¼š

```shell
git clone https://gitee.com/allwefantasy/byzer-llm
cd byzer-llm/setup-machine
```

æŽ¥ä¸‹æ¥ï¼Œè¯·åˆ‡æ¢è‡³ **ROOT**ï¼Œå¹¶æ‰§è¡Œä»¥ä¸‹å‡†å¤‡å¥½çš„è‡ªåŠ¨åŒ–é…ç½®è„šæœ¬ï¼š

```shell
ROLE=master ./setup-machine.sh
```
ç´§æŽ¥ç€ï¼Œç³»ç»Ÿå°†ä¸ºæ‚¨æ–°å»ºä¸€ä¸ªåä¸º `byzerllm` çš„ç”¨æˆ·è´¦æˆ·ã€‚

éšåŽï¼Œè¯·åˆ‡æ¢è‡³è¿™ä¸ªæ–°å»ºçš„ `byzerllm` ç”¨æˆ·èº«ä»½ï¼Œå¹¶æ‰§è¡Œä»¥ä¸‹é…ç½®è„šæœ¬ï¼š

```shell
ROLE=master ./setup-machine.sh
```

è„šæœ¬ä¼šè‡ªåŠ¨ä¸ºæ‚¨å®‰è£…ä»¥ä¸‹å„é¡¹è½¯ä»¶ï¼š

1. 
2. Conda
3. Nvidia Driver 535
4. Cuda 12.1.0
5. Ray 
6. requirements.txt æ–‡ä»¶å†…æ‰€åˆ—ä¸¾çš„æ‰€æœ‰ Python ç¬¬ä¸‰æ–¹åº“
7. Byzer-SQL/Byzer-Notebook å¤§æ•°æ®å¤„ç†ä¸Žåˆ†æžå·¥å…·

è‹¥æ‚¨éœ€è¦å‘ Ray é›†ç¾¤æ‰©å±•æ›´å¤šå·¥ä½œèŠ‚ç‚¹ï¼Œåªéœ€åœ¨æ–°å¢žçš„å·¥ä½œèŠ‚ç‚¹ä¸Šé‡å¤ä»¥ä¸Šå®‰è£…æ­¥éª¤ã€‚
è¯·æ³¨æ„ï¼Œåœ¨å·¥ä½œèŠ‚ç‚¹ä¸Šï¼Œ`ROLE` åº”ä¸º `worker`ã€‚

```shell
ROLE=worker ./setup-machine.sh
```

---

## å¿«é€Ÿå…¥é—¨

```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend

ray.init(address="auto",namespace="default",ignore_reinit_error=True)

llm = ByzerLLM()

llm.setup_gpus_per_worker(4).setup_num_workers(1)
llm.setup_infer_backend(InferBackend.transformers)

llm.deploy(model_path="/home/byzerllm/models/openbuddy-llama2-13b64k-v15",
           pretrained_model_type="custom/llama2",
           udf_name="llama2_chat",infer_params={})



llm_client = ByzerLLM()
llm_client.setup_template("llama2_chat","auto")

v = llm.chat_oai(model="llama2_chat",conversations=[{
    "role":"user",
    "content":"hello",
}])

print(v[0].output)
```

ä¸Šè¿°ä»£ç å°†ä¼šåŠ è½½å¹¶éƒ¨ç½²ä¸€ä¸ªåä¸º llama2 çš„æ¨¡åž‹ï¼Œç„¶åŽåˆ©ç”¨è¯¥æ¨¡åž‹å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡ŒæŽ¨ç†åˆ†æžã€‚å¦‚æžœä½ é€‰æ‹© transformers ä½œä¸ºæŽ¨ç†åŽç«¯å¼•æ“Žï¼Œéœ€è¦æ³¨æ„çš„æ˜¯éœ€è¦æ‰‹åŠ¨è®¾å®š `pretrained_model_type` å‚æ•°ï¼Œå› ä¸º transformers æœ¬èº«ä¸å…·å¤‡è‡ªåŠ¨æ£€æµ‹æ¨¡åž‹ç±»åž‹çš„æœºèƒ½ã€‚

Byzer-LLM åŒæ ·æ”¯æŒä»¥ç›¸åŒæ–¹å¼è°ƒç”¨å¹¶éƒ¨ç½²äº‘ç«¯ï¼ˆSaaSï¼‰æ¨¡åž‹ã€‚è¿™ä¸€åŠŸèƒ½ä¸ºå¼€æºæ¨¡åž‹å’Œäº‘æœåŠ¡æ¨¡åž‹æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ“ä½œç•Œé¢ã€‚æŽ¥ä¸‹æ¥çš„ç¤ºä¾‹ä»£ç å°†å±•ç¤ºå¦‚ä½•éƒ¨ç½²æ¥è‡ª Azure OpenAI çš„æ¨¡åž‹ï¼Œå¹¶åœ¨å…¶åŽåˆ©ç”¨è¿™ä¸ªæ¨¡åž‹å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡ŒæŽ¨ç†å¤„ç†ã€‚


```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend
ray.init(address="auto",namespace="default",ignore_reinit_error=True)

llm = ByzerLLM()

llm.setup_gpus_per_worker(0).setup_num_workers(10)
llm.setup_infer_backend(InferBackend.transformers)

llm.deploy(pretrained_model_type="saas/azure_openai",
           udf_name="azure_openai",
           infer_params={
            "saas.api_type":"azure",
            "saas.api_key"="xxx"
            "saas.api_base"="xxx"
            "saas.api_version"="2023-07-01-preview"
            "saas.deployment_id"="xxxxxx"
           })


llm_client = ByzerLLM()
llm_client.setup_template("azure_openai","auto")

v = llm.chat_oai(model="azure_openai",conversations=[{
    "role":"user",
    "content":"hello",
}])

print(v[0].output)
```

è¯·æ³¨æ„ï¼Œé‰´äºŽ SaaS æ¨¡åž‹æ— éœ€ä¾èµ– GPUï¼Œæˆ‘ä»¬æŠŠ `setup_gpus_per_worker` å‚æ•°è®¾ä¸º 0ã€‚å¦å¤–ï¼Œä½ å¯ä»¥å€ŸåŠ© `setup_num_workers` å‚æ•°æ¥è°ƒæ•´æœ€å¤§å¹¶å‘æ‰§è¡Œæ•°ï¼Œç„¶è€Œè¦æ³¨æ„çš„æ˜¯ï¼ŒSaaS æ¨¡åž‹è‡ªå¸¦å…¶å¹¶å‘è¯·æ±‚çš„ä¸Šé™ï¼Œå› æ­¤ `setup_num_workers` å‚æ•°æ‰€æŽ§åˆ¶çš„æ˜¯ Byzer-LLM æŽ¥å—çš„æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°ï¼Œè€Œéžç»å¯¹çš„å¹¶å‘æ‰§è¡Œä¸Šé™ï¼Œå®žé™…å¹¶å‘æ‰§è¡Œæ•°ä»éœ€å‚ç…§ SaaS æ¨¡åž‹è‡ªèº«çš„å¹¶å‘é™åˆ¶ã€‚

## å¦‚ä½•è¿žæŽ¥æ¥è‡ª Ray é›†ç¾¤å¤–éƒ¨çš„æ¨¡åž‹

å»ºè®®çš„æœ€ä½³å®žè·µæ˜¯åœ¨æ‚¨çš„ç›®æ ‡è®¾å¤‡ï¼ˆä¾‹å¦‚ Web æœåŠ¡å™¨ï¼‰ä¸Šå¯åŠ¨ä¸€ä¸ªé—²ç½®çš„ Ray å·¥ä½œèŠ‚ç‚¹ï¼š

```shell
ray start --address="xxxxx:6379"  --num-gpus=0 --num-cpus=0 
```

è¿™æ ·ä¸€æ¥ï¼Œæ‚¨ä¾¿å¯ä»¥ä»Ž Ray é›†ç¾¤å¤–éƒ¨é¡ºåˆ©å¯¹æŽ¥æ‰€éœ€æ¨¡åž‹ï¼š

```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend

## connect the ray cluster by the empty worker we started before
## this code should be run once in your prorgram
ray.init(address="auto",namespace="default",ignore_reinit_error=True)

## new a ByzerLLM instance

llm_client = ByzerLLM()
llm_client.setup_template("llama2_chat","auto")

v = llm.chat_oai(model="llama2_chat",conversations=[{
    "role":"user",
    "content":"hello",
}])

print(v[0].output)
```


## åµŒå…¥æ¨¡åž‹

ä»¥ä¸‹å±•ç¤ºçš„ä»£ç ç‰‡æ®µæ˜¯ä¸€ä¸ªå…³äºŽéƒ¨ç½² BGE åµŒå…¥æ¨¡åž‹çš„å®žé™…æ¡ˆä¾‹

```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend
ray.init(address="auto",namespace="default",ignore_reinit_error=True)
llm = ByzerLLM()

llm.setup_gpus_per_worker(0.4).setup_num_workers(2).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path="/home/byzerllm/models/bge-large-zh",
    pretrained_model_type="custom/bge",
    udf_name="emb",
    infer_params={}
)   
```

è¿™æ ·ï¼Œæ‚¨å°±èƒ½å¤Ÿå°†ä»»æ„ä¸€æ®µæ–‡æœ¬æˆåŠŸè½¬åŒ–ä¸ºå‘é‡è¡¨ç¤ºï¼š

```python
t = llm.emb("emb",LLMRequest(instruction="wow"))
t[0].output
#output: [-0.005588463973253965,
 -0.01747054047882557,
 -0.040633779019117355,
...
 -0.010880181565880775,
 -0.01713103987276554,
 0.017675869166851044,
 -0.010260719805955887,
 ...]
```

Byzer-LLM è¿˜æ”¯æŒäº‘ç«¯ï¼ˆSaaSï¼‰åµŒå…¥æ¨¡åž‹æœåŠ¡ã€‚ä¸‹é¢è¿™æ®µä»£ç æ¼”ç¤ºäº†å¦‚ä½•éƒ¨ç½²ä¸€ä¸ªç™¾å·æä¾›çš„åµŒå…¥æ¨¡åž‹ï¼Œå¹¶åˆ©ç”¨è¯¥æ¨¡åž‹å¯¹è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–å¤„ç†ã€‚

```python
import os
os.environ["RAY_DEDUP_LOGS"] = "0" 

import ray
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.utils.client import ByzerLLM,LLMRequest,LLMResponse,LLMHistoryItem,InferBackend
from byzerllm.utils.client import Templates

ray.init(address="auto",namespace="default",ignore_reinit_error=True)  

llm = ByzerLLM(verbose=True)

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "baichuan_emb"
if llm.is_model_exist(chat_name):
    llm.undeploy(udf_name=chat_name)

llm.deploy(model_path="",
           pretrained_model_type="saas/baichuan",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"",            
            "saas.model":"Baichuan-Text-Embedding"
           })
llm.setup_default_emb_model_name(chat_name)

v = llm.emb(None,LLMRequest(instruction="ä½ å¥½"))
print(v.output)
```

## åµŒå…¥é‡æŽ’åºæ¨¡åž‹

è‹¥æ‚¨æ‰“ç®—åˆ©ç”¨åµŒå…¥é‡æŽ’åºæ¨¡åž‹è¿›è¡Œä¼˜åŒ–ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹å…·ä½“åº”ç”¨ç¤ºä¾‹ã€‚

```python
import ray
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend
ray.init(address="auto",namespace="default",ignore_reinit_error=True)
llm = ByzerLLM()

llm.setup_gpus_per_worker(0.4).setup_num_workers(2).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path="/Users/wanghan/data/bge-reranker-base",
    pretrained_model_type="custom/bge_rerank",
    udf_name="emb_rerank",
    infer_params={}
)   
llm.setup_default_emb_model_name("emb_rerank")
```
æŽ¥ä¸‹æ¥ï¼Œæ‚¨å¯ä»¥é€šè¿‡å°†æŸ¥è¯¢æ–‡æœ¬å’Œå¾…è¯„ä¼°æ–‡æœ¬é€å…¥é‡æŽ’åºæ¨¡åž‹ï¼Œå¾—åˆ°å®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§å¾—åˆ†ã€‚

```python
sentence_pairs_01 = ['query', 'passage']
t1 = llm.emb_rerank(sentence_pairs=sentence_pairs_01)
print(t1[0].output)
#output [['query', 'passage'], 0.4474925994873047]

sentence_pairs_02 = [['what is panda?', 'hi'], ['what is panda?','The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]
t2 = llm.emb_rerank(sentence_pairs=sentence_pairs_02)
print(t2[0].output)
#output [[['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'], 6.1821160316467285], [['what is panda?', 'hi'], -8.154398918151855]]
```

## é‡åŒ–

å½“åŽç«¯é‡‡ç”¨ `InferBackend.transformers` æ—¶ï¼Œè¿™é‡Œå±•ç¤ºçš„æ˜¯ä¸€ä¸ªå…³äºŽâ€œç™¾å·2â€æ¨¡åž‹çš„å®žä¾‹åº”ç”¨ã€‚

```python
llm.setup_gpus_per_worker(2).setup_num_workers(1).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/baichuan2",
    udf_name="baichuan2_13_chat",
    infer_params={"quatization":"4"}
)
```
ç›®å‰æ”¯æŒçš„ `quantization`ï¼ˆé‡åŒ–ï¼‰é€‰é¡¹åŒ…æ‹¬ï¼š

1. 4
2. 8
3. true/false

è‹¥å°†è¯¥å‚æ•°è®¾ä¸º trueï¼Œç³»ç»Ÿå°†é‡‡ç”¨ int4 é‡åŒ–çº§åˆ«ã€‚

é’ˆå¯¹åŽç«¯ä¸º `InferBackend.VLLM` çš„æƒ…å†µï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨â€œæ˜“â€æ¨¡åž‹çš„ç¤ºä¾‹ï¼š

è‹¥éœ€è¦éƒ¨ç½²ç»è¿‡é‡åŒ–åŽ‹ç¼©çš„æ¨¡åž‹ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹ä»£ç æ ·å¼è®¾ç½® `infer_params` å‚æ•°ï¼š

```python
llm.setup_gpus_per_worker(1).setup_num_workers(1).setup_infer_backend(InferBackend.VLLM)
llm.deploy(
    model_path="/home/winubuntu/models/Yi-6B-Chat-4bits",
    pretrained_model_type="custom/auto",
    udf_name="chat",
    infer_params={"backend.quantization":"AWQ"}
)
```

`backend.quantization` å‚æ•°å¯ä»¥é€‰ç”¨ GPTQ æˆ– AWQ ä¸¤ç§é‡åŒ–æ–¹æ³•ã€‚


## æ”¯æŒçš„æ¨¡åž‹åˆ—è¡¨

æ”¯æŒçš„å¼€æº `pretrained_model_type` åŒ…æ‹¬ï¼š

1. custom/llama2
2. bark	
3. whisper	
3. chatglm6b
4. custom/chatglm2
5. moss
6. custom/alpha_moss
7. dolly
8. falcon
9. llama
10. custom/starcode
11. custom/visualglm
12. custom/m3e
13. custom/baichuan
14. custom/bge
15. custom/qwen_vl_chat
16. custom/stable_diffusion
17. custom/zephyr

æ”¯æŒçš„ SaaS `pretrained_model_type` å¦‚ä¸‹ï¼š

1. saas/chatglm	Chatglm130B
2. saas/sparkdesk	æ˜Ÿç«å¤§æ¨¡åž‹
3. saas/baichuan	ç™¾å·å¤§æ¨¡åž‹
4. saas/zhipu	æ™ºè°±å¤§æ¨¡åž‹
5. saas/minimax	MiniMax å¤§æ¨¡åž‹
6. saas/qianfan	æ–‡å¿ƒä¸€è¨€
7. saas/azure_openai	
8. saas/openai

è¯·æ³¨æ„ï¼Œæºè‡ª lama/llama2/starcode çš„è¡ç”Ÿæ¨¡åž‹ä¹ŸåŒæ ·å—åˆ°æ”¯æŒã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `llama` åŠ è½½ vicuna æ¨¡åž‹ã€‚

## æ”¯æŒ vLLM

Byzer-LLM åŒæ ·å…·å¤‡æ”¯æŒå°† vLLM ä½œä¸ºæŽ¨ç†åŽç«¯çš„èƒ½åŠ›ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥ä¾æ®ä»¥ä¸‹ä»£ç èŒƒä¾‹ï¼Œéƒ¨ç½²ä¸€ä¸ª vLLMï¼ˆè™šæ‹Ÿå¤§è§„æ¨¡è¯­è¨€æ¨¡åž‹ï¼‰ï¼Œå¹¶å€Ÿæ­¤æ¨¡åž‹å¯¹ç»™å®šæ–‡æœ¬è¿›è¡Œæ™ºèƒ½æŽ¨ç†å¤„ç†ã€‚

```python
import ray
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend

ray.init(address="auto",namespace="default",ignore_reinit_error=True)
llm = ByzerLLM()

llm.setup_gpus_per_worker(2)
llm.setup_num_workers(1)
llm.setup_infer_backend(InferBackend.VLLM)

llm.deploy(
    model_path="/home/byzerllm/models/openbuddy-zephyr-7b-v14.1",
    pretrained_model_type="custom/auto",
    udf_name="zephyr_chat"",
    infer_params={}
)

v = llm.chat_oai(model="zephyr_chat",conversations=[{
    "role":"user",
    "content":"hello",
}])
print(v[0].output)
```

vLLM ä¸Ž transformers åŽç«¯åœ¨ä½¿ç”¨ä¸Šæœ‰ä¸€äº›å¾®å°çš„ä¸åŒç‚¹ï¼š

1. åœ¨ vLLM ä¸­ï¼Œ`pretrained_model_type` å‚æ•°å›ºå®šä¸º `custom/auto`ï¼Œè¿™æ˜¯å› ä¸º vLLM è‡ªå¸¦æ¨¡åž‹ç±»åž‹è‡ªåŠ¨æ£€æµ‹åŠŸèƒ½ã€‚
2. è‹¥è¦æŒ‡å®šæŽ¨ç†åŽç«¯ä¸º vLLMï¼Œè¯·å°† `setup_infer_backend` å‚æ•°è®¾ç½®ä¸º `InferBackend.VLLM`ã€‚
 

### æµå¼å¯¹è¯

è‹¥æ¨¡åž‹é‡‡ç”¨äº† vLLM åŽç«¯è¿›è¡Œéƒ¨ç½²ï¼Œå®ƒè¿˜å°†æ”¯æŒâ€œæµå¼å¯¹è¯â€ç‰¹æ€§ï¼š

è°ƒç”¨ `stream_chat_oai` æ–¹æ³•å¯ä»¥èŽ·å¾—ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œè¿›è€Œé€æ¡æ‹‰å–æ¨¡åž‹ç”Ÿæˆçš„å›žå¤æ–‡æœ¬ã€‚

```python

llm.setup_default_model_name(chat_model_name) 

t = llm.stream_chat_oai(conversations=[{
    "role":"user",
    "content":"Hello, how are you?"
}])

for line in t:
   print(line+"\n")
```

## æ”¯æŒ DeepSpeed

Byzer-LLM è¿˜æ”¯æŒå°† DeepSpeed ä½œä¸ºæ¨¡åž‹æŽ¨ç†çš„åŽç«¯æŠ€æœ¯ã€‚ä»¥ä¸‹ä»£ç ç‰‡æ®µå°†å±•ç¤ºå¦‚ä½•éƒ¨ç½² DeepSpeed ä¼˜åŒ–çš„æ¨¡åž‹ï¼Œå¹¶åˆ©ç”¨è¯¥æ¨¡åž‹å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡ŒæŽ¨ç†åˆ†æžï¼š

```python
import ray
from byzerllm.utils.retrieval import ByzerRetrieval
from byzerllm.utils.client import ByzerLLM,LLMRequest,InferBackend

ray.init(address="auto",namespace="default",ignore_reinit_error=True)
llm = ByzerLLM()

llm.setup_gpus_per_worker(4)
llm.setup_num_workers(1)
llm.setup_infer_backend(InferBackend.DeepSpeed)

llm.deploy(
    model_path="/home/byzerllm/models/openbuddy-llama-13b-v5-fp16",
    pretrained_model_type="custom/auto",
    udf_name="llama_chat"",
    infer_params={}
)

llm.chat("llama_chat",LLMRequest(instruction="hello world"))[0].output
```

ä¸Šè¿°ä»£ç ä¸Žç”¨äºŽ vLLM çš„ä»£ç åŸºæœ¬ä¸€è‡´ï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äºŽ `InferBackend` è®¾ç½®æˆäº† `InferBackend.DeepSpeed`ã€‚

## å…¼å®¹ OpenAI RESTful API æœåŠ¡

é€šè¿‡æ‰§è¡Œä¸‹åˆ—ä»£ç ç‰‡æ®µï¼Œå³å¯å¯åŠ¨ä¸€ä¸ªèƒ½å¤Ÿä¸Ž OpenAI å¯¹æŽ¥çš„ ByzerLLm å¤§è¯­è¨€æ¨¡åž‹ RESTful API æœåŠ¡å™¨ï¼š

```shell
ray start --address="xxxxx:6379"  --num-gpus=0 --num-cpus=0 
python -m byzerllm.utils.client.entrypoints.openai.api_server
```

é»˜è®¤æƒ…å†µä¸‹ï¼ŒæœåŠ¡å™¨è¿è¡Œæ—¶ä¼šåœ¨`8000`ç«¯å£ç­‰å¾…è¯·æ±‚ã€‚æ‚¨å¯ä»¥é‡‡ç”¨å¦‚ä¸‹ä»£ç ç‰‡æ®µæ¥éªŒè¯å¹¶æµ‹è¯•è¯¥ API åŠŸèƒ½ï¼š

```python
from openai import OpenAI
client = OpenAI(
    base_url="http://127.0.0.1:8000/v1",
    api_key="xxxx"
)

chat_completion = client.chat.completions.create(    
    model="wenxin_chat",
    messages=[{"role": "user", "content": "å†™ä¸€ä¸ªæŽ’åºç®—æ³•"}],
    stream=False
)

print(chat_completion.choices[0].message.content)
```

## æµå¼å¯¹è¯

```python

from openai import OpenAI
client = OpenAI(
    base_url="http://127.0.0.1:8000/v1",
    api_key="simple"
)

chat_completion = client.chat.completions.create(    
    model="wenxin_chat",
    messages=[{"role": "user", "content": "å†™ä¸€ä¸ªæŽ’åºç®—æ³•"}],
    stream=True
)

for chunk in chat_completion:    
    print(chunk.choices[0].delta.content or "", end="")
```

## å‡½æ•°è°ƒç”¨

è¿™æœ‰ä¸€ä¸ªåˆ©ç”¨ QWen 72B æ¨¡åž‹è¿›è¡Œå‡½æ•°è°ƒç”¨çš„åŸºç¡€ç¤ºä¾‹ã€‚

éƒ¨ç½²æ¨¡åž‹çš„æ­¥éª¤æ¼”ç¤ºï¼š

```python
import ray
ray.init(address="auto",namespace="default") 
llm = ByzerLLM()

model_location="/home/byzerllm/models/Qwen-72B-Chat"

llm.setup_gpus_per_worker(8).setup_num_workers(1).setup_infer_backend(InferBackend.VLLM)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/auto",
    udf_name=chat_model_name,
    infer_params={}
)

llm.setup_default_model_name("chat")
# from 0.1.24 
# llm.setup_auto("chat")
meta = llm.get_meta()
llm.setup_max_model_length("chat",meta.get("max_model_len",32000))
lm.setup_template("chat",Templates.qwen()) 
```

è®©æˆ‘ä»¬ä¸€èµ·å°è¯•ç¼–å†™å‡ ä¸ªPythonå‡½æ•°ï¼Œå…ˆæ¥ä½“éªŒå¦‚ä½•ä½¿ç”¨QWen 72Bæ¨¡åž‹ç”Ÿæˆå›žå¤ï¼š

```python

from typing import List,Dict,Any,Annotated
import pydantic 
import datetime
from dateutil.relativedelta import relativedelta

def compute_date_range(count:Annotated[int,"æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»åž‹"],
                       unit:Annotated[str,"æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»åž‹",{"enum":["day","week","month","year"]}])->List[str]:
    '''
    è®¡ç®—æ—¥æœŸèŒƒå›´

    Args:
        count: æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»åž‹
        unit: æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»åž‹ï¼Œå¯é€‰å€¼ä¸º day,week,month,year
    '''        
    now = datetime.datetime.now()
    now_str = now.strftime("%Y-%m-%d %H:%M:%S")
    if unit == "day":
        return [(now - relativedelta(days=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    elif unit == "week":
        return [(now - relativedelta(weeks=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    elif unit == "month":
        return [(now - relativedelta(months=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    elif unit == "year":
        return [(now - relativedelta(years=count)).strftime("%Y-%m-%d %H:%M:%S"),now_str]
    return ["",""]

def compute_now()->str:
    '''
    è®¡ç®—å½“å‰æ—¶é—´
    '''
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
```

è¿™é‡Œæˆ‘ä»¬ç»™å‡ºäº†ä¸¤ä¸ªä¾¿æ·çš„å‡½æ•°ï¼š

1. compute_date_rangeï¼šæ ¹æ®ç”¨æˆ·æŒ‡å®šçš„æ•°é‡ï¼ˆå¦‚å¤©æ•°ã€å‘¨æ•°ç­‰ï¼‰å’Œå•ä½æ¥è®¡ç®—ä¸€ä¸ªèµ·æ­¢æ—¥æœŸçš„åŒºé—´ã€‚
2. compute_nowï¼šèŽ·å–å½“å‰çš„æ—¥æœŸä¿¡æ¯ã€‚

å½“é¢å¯¹ç”¨æˆ·çš„å…·ä½“é—®é¢˜æ—¶ï¼Œæˆ‘ä»¬ä¼šåˆ©ç”¨æ¨¡åž‹è°ƒç”¨è¿™ä¸¤ä¸ªåŠŸèƒ½å·¥å…·ã€‚

```python
t = llm.chat_oai([{
    "content":'''è®¡ç®—å½“å‰æ—¶é—´''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

t[0].values

## output: ['2023-12-18 17:30:49']
```

```python
t = llm.chat_oai([{
    "content":'''æœ€è¿‘ä¸‰ä¸ªæœˆè¶‹åŠ¿''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

t[0].values

## output: [['2023-09-18 17:31:21', '2023-12-18 17:31:21']]
```

```python
t = llm.chat_oai([{
    "content":'''æœ€è¿‘ä¸‰å¤©''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

t[0].values

## output: [['2023-12-15 17:23:38', '2023-12-18 17:23:38']]
```

```python
t = llm.chat_oai([{
    "content":'''ä½ åƒé¥­äº†ä¹ˆï¼Ÿ''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

if t[0].values:
    print(t[0].values[0])
else:
    print(t[0].response.output)   

## output: 'æ‚¨å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½è¯­è¨€æ¨¡åž‹ï¼Œæš‚æ—¶æ— æ³•åƒé¥­ã€‚'
```

æ‚¨å¯ä»¥æ£€æŸ¥ `from byzerllm.utils import function_calling_format` ä¸­çš„é»˜è®¤æç¤ºæ¨¡æ¿å‡½æ•°ã€‚å¦‚æžœæ¨¡åž‹ä½¿ç”¨é»˜è®¤å‡½æ•°æ•ˆæžœä¸ä½³ï¼Œæ‚¨å¯ä»¥è®¾ç½®è‡ªå®šä¹‰å‡½æ•°ï¼š

```python
def custom_function_calling_format(prompt:str,tools:List[Callable],tool_choice:Callable)->str:
.....


llm.setup_function_calling_format_func("chat",custom_function_calling_format)
```

## å“åº”æ—¶ä½¿ç”¨ Pydantic ç±»

åœ¨ä¸Žå¤§è¯­è¨€æ¨¡åž‹äº¤è°ˆæ—¶ï¼Œä½ å¯ä»¥è‡ªå®šä¹‰è®¾ç½®ä¸€ä¸ªç±»ä¼¼â€œå“åº”ç±»â€ï¼ˆResponse Classï¼‰çš„ç»“æž„ï¼Œä»¥æ­¤æ¥è§„èŒƒå’ŒæŽ§åˆ¶æ¨¡åž‹ç»™å‡ºå›žç­”çš„æ•°æ®æ ¼å¼å’Œç»“æž„

```python
import pydantic 

class Story(pydantic.BaseModel):
    '''
    æ•…äº‹
    '''

    title: str = pydantic.Field(description="æ•…äº‹çš„æ ‡é¢˜")
    body: str = pydantic.Field(description="æ•…äº‹ä¸»ä½“")



t = llm.chat_oai([
{
    "content":f'''è¯·ç»™æˆ‘è®²ä¸ªæ•…äº‹ï¼Œåˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ ‡é¢˜ï¼Œä¸€ä¸ªæ•…äº‹ä¸»ä½“''',
    "role":"user"
},
],response_class=Story)

t[0].value

## output: Story(title='å‹‡æ•¢çš„å°å…”å­', body='åœ¨ä¸€ä¸ªç¾Žä¸½çš„æ£®æž—é‡Œï¼Œä½ç€ä¸€åªå¯çˆ±çš„å°å…”å­ã€‚å°å…”å­éžå¸¸å‹‡æ•¢ï¼Œæœ‰ä¸€å¤©ï¼Œæ£®æž—é‡Œçš„åŠ¨ç‰©ä»¬éƒ½è¢«å¤§ç°ç‹¼å“åäº†ã€‚åªæœ‰å°å…”å­ç«™å‡ºæ¥ï¼Œç”¨æ™ºæ…§å’Œå‹‡æ°”æ‰“è´¥äº†å¤§ç°ç‹¼ï¼Œä¿æŠ¤äº†æ‰€æœ‰çš„åŠ¨ç‰©ã€‚ä»Žæ­¤ï¼Œå°å…”å­æˆä¸ºäº†æ£®æž—é‡Œçš„è‹±é›„ã€‚')
```

ä¸Šè¿°ä»£ç ä¼šè®© LLM ç›´æŽ¥ç”Ÿæˆ Story ç±»çš„å¯¹è±¡ã€‚ä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ› LLM å…ˆç”Ÿæˆæ–‡æœ¬ï¼Œå†ä»Žæ–‡æœ¬ä¸­æå–ç»“æž„ä¿¡æ¯ï¼Œè¿™æ—¶å¯ä»¥é€šè¿‡è®¾ç½® `response_after_chat=True` æ¥å¯ç”¨è¿™ä¸€è¡Œä¸ºã€‚ä¸è¿‡ï¼Œè¯·æ³¨æ„ï¼Œè¿™æ ·åšä¼šå¯¼è‡´ä¸€å®šçš„æ€§èƒ½æŸè€—ï¼ˆé¢å¤–çš„æŽ¨ç†è®¡ç®—ï¼‰ã€‚

```python
t = llm.chat_oai([
{
    "content":f'''è¯·ç»™æˆ‘è®²ä¸ªæ•…äº‹ï¼Œåˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ ‡é¢˜ï¼Œä¸€ä¸ªæ•…äº‹ä¸»ä½“''',
    "role":"user"
},
],response_class=Story,response_after_chat=True)

t[0].value
## output: Story(title='æœˆå…‰ä¸‹çš„å®ˆæŠ¤è€…', body='åœ¨ä¸€ä¸ªé¥è¿œçš„å¤è€æ‘åº„é‡Œï¼Œä½ç€ä¸€ä½åå«é˜¿æ˜Žçš„å¹´è½»äººã€‚é˜¿æ˜Žæ˜¯ä¸ªå­¤å„¿ï¼Œä»Žå°åœ¨æ‘é‡Œé•¿å¤§ï¼Œä»¥ç§ç”°ä¸ºç”Ÿã€‚ä»–å–„è‰¯ã€å‹¤åŠ³ï¼Œæ·±å—æ‘æ°‘ä»¬å–œçˆ±ã€‚\n\næ‘å­é‡Œæœ‰ä¸ªä¼ è¯´ï¼Œæ¯å½“æ»¡æœˆæ—¶åˆ†ï¼Œæœˆäº®å¥³ç¥žä¼šåœ¨æ‘å­åŽå±±çš„å¤æ ‘ä¸‹å‡ºçŽ°ï¼Œèµç¦ç»™é‚£äº›å–„è‰¯çš„äººä»¬ã€‚ç„¶è€Œï¼Œåªæœ‰æœ€çº¯æ´çš„å¿ƒæ‰èƒ½çœ‹åˆ°å¥¹ã€‚å› æ­¤ï¼Œæ¯å¹´çš„è¿™ä¸ªæ—¶å€™ï¼Œé˜¿æ˜Žéƒ½ä¼šç‹¬è‡ªä¸€äººå‰å¾€åŽå±±ï¼Œå¸Œæœ›èƒ½å¾—åˆ°å¥³ç¥žçš„ç¥ç¦ã€‚\n\nè¿™ä¸€å¹´ï¼Œæ‘å­é­å—äº†ä¸¥é‡çš„æ—±ç¾ï¼Œåº„ç¨¼æž¯é»„ï¼Œäººä»¬ç”Ÿæ´»å›°è‹¦ã€‚é˜¿æ˜Žå†³å®šå‘æœˆäº®å¥³ç¥žç¥ˆæ±‚é™é›¨ï¼Œæ‹¯æ•‘æ‘å­ã€‚ä»–åœ¨æœˆå…‰ä¸‹è™”è¯šåœ°ç¥ˆç¥·ï¼Œå¸Œæœ›å¥³ç¥žèƒ½å¬åˆ°ä»–çš„å‘¼å”¤ã€‚\n\nå°±åœ¨è¿™ä¸ªæ—¶åˆ»ï¼Œæœˆäº®å¥³ç¥žå‡ºçŽ°äº†ã€‚å¥¹è¢«é˜¿æ˜Žçš„å–„è‰¯å’Œæ‰§ç€æ‰€æ„ŸåŠ¨ï¼Œç­”åº”äº†ä»–çš„è¯·æ±‚ã€‚ç¬¬äºŒå¤©æ—©æ™¨ï¼Œå¤©ç©ºä¹Œäº‘å¯†å¸ƒï¼Œå¤§é›¨å€¾ç›†è€Œä¸‹ï¼Œä¹…æ—±çš„åœŸåœ°å¾—åˆ°äº†æ»‹æ¶¦ï¼Œåº„ç¨¼é‡æ–°ç„•å‘ç”Ÿæœºã€‚\n\nä»Žæ­¤ä»¥åŽï¼Œæ¯å¹´çš„æ»¡æœˆä¹‹å¤œï¼Œé˜¿æ˜Žéƒ½ä¼šåŽ»åŽå±±ç­‰å¾…æœˆäº®å¥³ç¥žçš„å‡ºçŽ°ï¼Œä»–æˆä¸ºäº†æ‘æ°‘å¿ƒä¸­çš„å®ˆæŠ¤è€…ï¼Œç”¨ä»–çš„å–„è‰¯å’Œæ‰§ç€ï¼Œå®ˆæŠ¤ç€æ•´ä¸ªæ‘åº„ã€‚è€Œä»–ä¹Ÿç»ˆäºŽæ˜Žç™½ï¼ŒçœŸæ­£çš„å®ˆæŠ¤è€…ï¼Œå¹¶éžéœ€è¦è¶…å‡¡çš„åŠ›é‡ï¼Œåªéœ€è¦ä¸€é¢—å……æ»¡çˆ±ä¸Žå–„è‰¯çš„å¿ƒã€‚')
```

ä½ å¯ä»¥åœ¨ byzerllm.utils æ¨¡å—ä¸­é€šè¿‡ import è¯­å¥å¼•å…¥é»˜è®¤çš„æç¤ºæ¨¡æ¿å‡½æ•°ï¼Œ`from byzerllm.utils import response_class_format,response_class_format_after_chat`ã€‚

å¦‚æžœæ¨¡åž‹ä½¿ç”¨é»˜è®¤å‡½æ•°çš„æ•ˆæžœä¸å°½å¦‚äººæ„ï¼Œä½ å¯ä»¥è®¾ç½®è‡ªå®šä¹‰å‡½æ•°æ¥ä¼˜åŒ–å®ƒï¼š

```python
def custom_response_class_format(prompt:str,cls:pydantic.BaseModel)->str:
.....


llm.setup_response_class_format_func("chat",custom_response_class_format)
```

## å‡½æ•°å®žçŽ°åŠŸèƒ½

Byzer-LLM è¿˜æ”¯æŒå‡½æ•°å®žçŽ°åŠŸèƒ½ã€‚æ‚¨å¯ä»¥å®šä¹‰ä¸€ä¸ªç©ºå‡½æ•°ï¼Œå¹¶ç»“åˆå‡½æ•°å†…çš„æ–‡æ¡£è¯´æ˜Ž/ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œæ¥å¼•å¯¼å¤§è¯­è¨€æ¨¡åž‹(LLM)åŽ»å®žçŽ°è¿™ä¸ªå‡½æ•°çš„åŠŸèƒ½ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼š

```python
from byzerllm.utils.client import code_utils,message_utils
from typing import List,Union,Optional
import pydantic

class Time(pydantic.BaseModel):
    time: str = pydantic.Field(...,description="æ—¶é—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")


@llm.impl()
def calculate_current_time()->Time:
    '''
    è®¡ç®—å½“å‰æ—¶é—´
    '''
    pass 


calculate_current_time()
#output: Time(time='2024-01-28')
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œç³»ç»Ÿä¼šæŠŠå‡½æ•°å†…éƒ¨çš„è®¡ç®—è¿‡ç¨‹ï¼ˆå³å‡½æ•°å®žçŽ°ï¼‰ç¼“å­˜èµ·æ¥ï¼Œè¿™æ ·å½“ä¸‹æ¬¡è°ƒç”¨ç›¸åŒå‡½æ•°æ—¶å°±èƒ½è¿…é€Ÿæ‰§è¡Œï¼Œæ— éœ€é‡æ–°è®¡ç®—ã€‚

```python
start = time.monotonic()
calculate_current_time()
print(f"first time cost: {time.monotonic()-start}")

start = time.monotonic()
calculate_current_time()
print(f"second time cost: {time.monotonic()-start}")

# output:
# first time cost: 6.067266260739416
# second time cost: 4.347506910562515e-05
```

è‹¥è¦æ¸…é™¤ç¼“å­˜ï¼Œå¯ä»¥é€šè¿‡è¿è¡Œ `llm.clear_impl_cache()` æ–¹æ³•æ¥å®žçŽ°è¿™ä¸€ç›®çš„ã€‚

æŽ¥ä¸‹æ¥æ˜¯ä¸€ä¸ªå±•ç¤ºå¦‚ä½•é’ˆå¯¹å¸¦å‚æ•°çš„å‡½æ•°æ‰§è¡Œç»“æžœè¿›è¡Œç¼“å­˜å¤„ç†çš„ç¤ºä¾‹ï¼š

```python
from byzerllm.utils.client import code_utils,message_utils
from typing import List,Union,Optional,Annotated
import pydantic
from datetime import datetime

class Time(pydantic.BaseModel):
    time: str = pydantic.Field(...,description="æ—¶é—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")


@llm.impl()
def add_one_day(current_day:Annotated[datetime,"å½“å‰æ—¥æœŸï¼Œç±»åž‹æ˜¯datatime.datetime"])->Time:
    '''
    ç»™ä¼ å…¥çš„æ—¥æœŸåŠ ä¸€å¤©ï¼Œå¾—åˆ°æ˜Žå¤©çš„æ—¶é—´
    '''
    pass 


add_one_day(datetime.now())
# output:Time(time='2024-01-29')
```

æ“ä½œæŒ‡å¼•ï¼š

```python
from byzerllm.utils.client import code_utils,message_utils
from typing import List,Union,Optional
import pydantic

class TimeRange(pydantic.BaseModel):
    '''
    æ—¶é—´åŒºé—´
    æ ¼å¼éœ€è¦å¦‚ä¸‹ï¼š yyyy-MM-dd
    '''  
    
    start: str = pydantic.Field(...,description="å¼€å§‹æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")
    end: str = pydantic.Field(...,description="æˆªæ­¢æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")

@llm.impl(instruction="åŽ»å¹´ä¸‰æœˆåˆ°ä¸ƒæœˆ")
def calculate_time_range()->TimeRange:
    '''
    è®¡ç®—æ—¶é—´åŒºé—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd. 
    '''
    pass 

calculate_time_range()
# output: TimeRange(start='2023-03-01', end='2023-07-31')
```

è‹¥æƒ³å°†ç”¨æˆ·çš„æŸ¥è¯¢é—®é¢˜ç”¨äºŽæ›¿ä»£åŽŸå…ˆç”¨æ¥æ¸…é™¤ç¼“å­˜çš„æŒ‡ä»¤ï¼Œå¯ä»¥é‡‡ç”¨å¦‚ä¸‹ä»£ç å®žçŽ°è¿™ä¸€åŠŸèƒ½ï¼š

```python
from byzerllm.utils.client import code_utils,message_utils
from typing import List,Union,Optional
import pydantic

class TimeRange(pydantic.BaseModel):
    '''
    æ—¶é—´åŒºé—´
    æ ¼å¼éœ€è¦å¦‚ä¸‹ï¼š yyyy-MM-dd
    '''  
    
    start: str = pydantic.Field(...,description="å¼€å§‹æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")
    end: str = pydantic.Field(...,description="æˆªæ­¢æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")

def calculate_time_range()->TimeRange:
    '''
    è®¡ç®—æ—¶é—´åŒºé—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd. 
    '''
    pass 


llm.impl(instruction="åŽ»å¹´ä¸‰æœˆåˆ°ä¸ƒæœˆ")(calculate_time_range)()
```

è‹¥æƒ³æ·±å…¥äº†è§£å‡½æ•°å®žçŽ°çš„è¯¦ç»†æƒ…å†µï¼Œå¯åœ¨è°ƒç”¨æ—¶åŠ ä¸Š `verbose=True` å‚æ•°ï¼Œç³»ç»Ÿå°†ä¸ºä½ æä¾›æ›´å¤šç›¸å…³ä¿¡æ¯ï¼š

```python
@llm.impl()
def add_one_day(current_day:Annotated[datetime,"å½“å‰æ—¥æœŸï¼Œç±»åž‹æ˜¯datatime.datetime"])->Time:
    '''
    ç»™ä¼ å…¥çš„æ—¥æœŸåŠ ä¸€å¤©ï¼Œå¾—åˆ°æ˜Žå¤©çš„æ—¶é—´
    '''
    pass 
```

ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨åŸºç¡€çš„ chat_oai å‡½æ•°æ¥å®žçŽ°å‡½æ•°ï¼š

```python
class TimeRange(pydantic.BaseModel):
    '''
    æ—¶é—´åŒºé—´
    æ ¼å¼éœ€è¦å¦‚ä¸‹ï¼š yyyy-MM-dd
    '''  
    
    start: str = pydantic.Field(...,description="å¼€å§‹æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")
    end: str = pydantic.Field(...,description="æˆªæ­¢æ—¶é—´.æ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd")


def calculate_time_range():
    '''
    è®¡ç®—æ—¶é—´åŒºé—´ï¼Œæ—¶é—´æ ¼å¼ä¸º yyyy-MM-dd. 
    '''
    pass 
    
t = llm.chat_oai([{
    "content":"åŽ»å¹´ä¸‰æœˆåˆ°ä¸ƒæœˆ",
    "role":"user"    
}],impl_func=calculate_time_range,response_class=TimeRange,execute_impl_func=True)
```

ä¸Šè¿°ä»£ç ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªåä¸º `calculate_time_range` çš„å‡½æ•°ï¼Œè¯¥å‡½æ•°ç›®å‰ä¸ºç©ºã€‚æŽ¥ç€æˆ‘ä»¬åœ¨æ–‡æ¡£å­—ç¬¦ä¸²ä¸­è¯¦ç»†æè¿°äº†å‡½æ•°çš„åŠŸèƒ½ï¼Œå¹¶å®šä¹‰äº†å“åº”ç±» `TimeRange`ï¼Œç¡®ä¿å‡½æ•°è¿”å›žä¸€ä¸ª `TimeRange` å®žä¾‹ã€‚ç”±äºŽè¯¥å‡½æ•°åº”æœåŠ¡äºŽè§£ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œæ‰€ä»¥å®ƒçš„å®žçŽ°åº”å½“ä¸Žç”¨æˆ·çš„å…·ä½“é—®é¢˜ç´§å¯†ç›¸å…³ã€‚æˆ‘ä»¬ä¸æ˜¯è¦åŽ»å®žçŽ°ä¸€ä¸ªé€šç”¨çš„å‡½æ•°ï¼Œè€Œæ˜¯å®žçŽ°ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ç”¨æˆ·å½“å‰é—®é¢˜è¿›è¡Œè§£ç­”çš„å‡½æ•°ã€‚

æ‰§è¡ŒåŽï¼Œä½ ä¼šå¾—åˆ°å¦‚ä¸‹æ‰€ç¤ºçš„è¾“å‡ºç»“æžœï¼š

```python
t[0].value
# start='2023-03-01' end='2023-07-31'
```

å¦‚æžœè¿”å›žçš„å€¼æ˜¯ None æˆ–ä¸æ­£ç¡®ï¼Œç³»ç»Ÿå°†ä¼šç»™å‡ºé”™è¯¯æç¤ºä¿¡æ¯ï¼š
```python
t[0].metadata.get("resason","")
```

å¦‚æžœä½ å®šä¹‰çš„å‡½æ•°å¸¦æœ‰å‚æ•°ï¼Œå¯ä»¥é€šè¿‡ `impl_func_params` å‚æ•°ä¼ é€’ç»™è¯¥å‡½æ•°ï¼š

```python
t = llm.chat_oai([{
    "content":"xxxxx",
    "role":"user"    
}],
impl_func=calculate_time_range,
impl_func_params={},
response_class=TimeRange,execute_impl_func=True)
```

å¦‚æžœä½ æƒ³è¦æ›¿æ¢é»˜è®¤çš„æç¤ºæ¨¡æ¿å‡½æ•°ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªç¤ºä¾‹ï¼š

```python
import pydantic
from typing import List,Optional,Union,Callable
from byzerllm.utils import serialize_function_to_json

def function_impl_format2(prompt:str,func:Optional[Union[Callable,str]],
                             cls:Union[pydantic.BaseModel,str])->str:
    
    tool_choice_ser = serialize_function_to_json(func)    
    _cls = ""
    if isinstance(cls, str):
        _cls = cls
    else:
        _cls = cls.schema_json(ensure_ascii=False)
    
    msg = f''''ç”Ÿæˆä¸€ä¸ªpythonå‡½æ•°ï¼Œç»™å‡ºè¯¦ç»†çš„æ€è€ƒé€»è¾‘ï¼Œå¯¹æœ€åŽç”Ÿæˆçš„å‡½æ•°ä¸è¦è¿›è¡Œç¤ºä¾‹è¯´æ˜Žã€‚

ç”Ÿæˆçš„å‡½æ•°çš„åå­—ä»¥åŠå‚æ•°éœ€è¦æ»¡è¶³å¦‚ä¸‹çº¦æŸï¼š

\```json
{tool_choice_ser}
\```

ç”Ÿæˆçš„å‡½æ•°çš„è¿”å›žå€¼å¿…é¡»æ˜¯ Json æ ¼å¼ã€‚

ä¸‹é¢æ˜¯ä½¿ç”¨ OpenAPI 3.1. è§„èŒƒæè¿°äº†ä½ éœ€å¦‚ä½•è¿›è¡Œjsonæ ¼å¼çš„ç”Ÿæˆã€‚

\```json
{_cls}
\```

æ ¹æ®ç”¨çš„æˆ·é—®é¢˜,{func.__doc__}ã€‚ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š{prompt}

è¯·ä½ å®žçŽ°è¿™ä¸ªå‡½æ•°ã€‚
''' 
    
    return msg

llm.setup_impl_func_format_func(chat_model_name,function_impl_format2)
```
é»˜è®¤çš„æç¤ºæ¨¡æ¿å‡½æ•°æ˜¯ `function_impl_format`ï¼Œä½ å¯ä»¥åœ¨ `from byzerllm.utils import function_impl_format` è¿™æ®µä»£ç ä¸­æŸ¥çœ‹å…¶æºä»£ç ã€‚

## å¤§è¯­è¨€æ¨¡åž‹å‹å¥½åž‹å‡½æ•°/æ•°æ®ç±»

è‹¥è¦æå‡å‡½æ•°è°ƒç”¨æˆ–å“åº”ç±»çš„æ€§èƒ½è¡¨çŽ°ï¼Œåº”å½“ç¡®ä¿ä½ çš„å‡½æ•°ï¼ˆå·¥å…·ï¼‰å’Œæ•°æ®ç±»å¯¹å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰å‹å¥½ã€‚

æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹ä¸€æ®µ Python ä»£ç ç¤ºä¾‹ï¼š

```python
def compute_date_range(count:int, unit:str)->List[str]:                   
    now = datetime.datetime.now()
    ....
```

è¿™æ®µä»£ç å¹¶éžå¯¹å¤§è¯­è¨€æ¨¡åž‹å‹å¥½ï¼Œå› ä¸ºå®ƒéš¾ä»¥è®©äººæˆ–LLMç†è§£å‡½æ•°çš„å…·ä½“ç”¨é€”ä»¥åŠè¾“å…¥å‚æ•°çš„å«ä¹‰ã€‚

å¤§è¯­è¨€æ¨¡åž‹å°±å¦‚åŒäººç±»ä¸€æ ·ï¼Œå¾ˆéš¾è®©å®ƒçŸ¥é“ä½•æ—¶æˆ–å¦‚ä½•è°ƒç”¨è¿™ä¸ªå‡½æ•°ã€‚ç‰¹åˆ«æ˜¯å‚æ•°`unit`å®žé™…ä¸Šæ˜¯ä¸€ä¸ªæžšä¸¾å€¼ï¼Œä½†æ˜¯å¤§è¯­è¨€æ¨¡åž‹æ— æ³•èŽ·çŸ¥è¿™ä¸€ä¿¡æ¯ã€‚

å› æ­¤ï¼Œä¸ºäº†è®©å¤§è¯­è¨€æ¨¡åž‹æ›´å¥½åœ°ç†è§£ Byzer-LLM ä¸­çš„è¿™ä¸ªå‡½æ•°ï¼Œä½ åº”è¯¥éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š

1. æ·»åŠ ç¬¦åˆ Python è§„èŒƒçš„å‡½æ•°æ³¨é‡Š
2. ä½¿ç”¨ç±»åž‹æ³¨è§£ä¸ºæ¯ä¸ªå‚æ•°æä¾›ç±»åž‹å’Œæ³¨é‡Šï¼Œå¦‚æžœå‚æ•°æ˜¯ä¸€ä¸ªæžšä¸¾å€¼ï¼Œè¿˜éœ€è¦æä¾›æžšä¸¾çš„æ‰€æœ‰å¯èƒ½å–å€¼ã€‚

ä¸‹é¢æ˜¯æ”¹è¿›åŽçš„å¯¹å¤§è¯­è¨€æ¨¡åž‹å‹å¥½çš„å‡½æ•°å®šä¹‰ç¤ºä¾‹

```python
def compute_date_range(count:Annotated[int,"æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»åž‹"],
                       unit:Annotated[str,"æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»åž‹",{"enum":["day","week","month","year"]}])->List[str]:
    '''
    è®¡ç®—æ—¥æœŸèŒƒå›´

    Args:
        count: æ—¶é—´è·¨åº¦ï¼Œæ•°å€¼ç±»åž‹
        unit: æ—¶é—´å•ä½ï¼Œå­—ç¬¦ä¸²ç±»åž‹ï¼Œå¯é€‰å€¼ä¸º day,week,month,year
    '''        
    now = datetime.datetime.now()
    ....
```

å¦‚æžœå¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰åœ¨è°ƒç”¨ä½ çš„å‡½æ•°æ—¶å‡ºçŽ°é—®é¢˜ï¼ˆä¾‹å¦‚æä¾›äº†é”™è¯¯çš„å‚æ•°ï¼‰ï¼Œè¯•ç€ä¼˜åŒ–å‡½æ•°æ³¨é‡Šå’Œå‚æ•°çš„ç±»åž‹æ³¨è§£æ³¨é‡Šï¼Œä»¥å¸®åŠ©LLMæ›´å¥½åœ°ç†è§£å‡½æ•°çš„æ­£ç¡®ç”¨æ³•å’Œå‚æ•°å«ä¹‰ã€‚

## æ¨¡åž‹å…ƒä¿¡æ¯

Byzer-LLM åŒæ ·æ”¯æŒèŽ·å–æ¨¡åž‹å®žä¾‹çš„å…ƒä¿¡æ¯ã€‚ä¸‹é¢çš„ä»£ç å°†èŽ·å–åä¸º `chat` çš„æ¨¡åž‹å®žä¾‹çš„å…ƒä¿¡æ¯ï¼š

```python
llm.get_meta(model="chat")

#output:
# {'model_deploy_type': 'proprietary',
#  'backend': 'ray/vllm',
#  'max_model_len': 32768,
#  'architectures': ['QWenLMHeadModel']}
```

## å¯¹è¯æ¨¡æ¿

ä¸åŒçš„æ¨¡åž‹æ‹¥æœ‰å„è‡ªçš„å¯¹è¯æ¨¡æ¿ï¼ŒByzer-LLM ä¸ºå„ä¸ªæ¨¡åž‹æä¾›äº†ä¸€äº›é¢„è®¾çš„å¯¹è¯æ¨¡æ¿ã€‚ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç æ¥è®¾ç½®å¯¹è¯æ¨¡æ¿ï¼š

```python
from byzerllm.utils.client import Templates
llm.setup_template("chat",Templates.qwen()) 
```

ä¸ä»…å¦‚æ­¤ï¼Œæˆ‘ä»¬è¿˜æ”¯æŒä½¿ç”¨ `tokenizer.apply_chat_template` æ–¹æ³•ï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç åº”ç”¨å¯¹è¯æ¨¡æ¿ï¼š

```python
llm.setup_template("chat","auto") 
```

è¦æ˜¯æ¨¡åž‹è·Ÿ `tokenizer.apply_chat_template` è¿™ä¸ªå°å·¥å…·çŽ©ä¸è½¬ï¼Œå®ƒä¼šå‘å‡ºä¿¡å·â€”â€”ä¹Ÿå°±æ˜¯æŠ›å‡ºä¸€ä¸ªå¼‚å¸¸ã€‚è¿™æ—¶å€™ï¼Œä½ å®Œå…¨å¯ä»¥äº²è‡ªå‡ºæ‰‹ï¼Œç”¨ `llm.setup_template` è¿™ä¸ªæ‹›å¼æ¥æ‰‹åŠ¨æ‰“é€ èŠå¤©æ¨¡æ¿ã€‚

æ­¤å¤–ï¼Œä½ è¿˜èƒ½ç”¨ `llm.get_meta` è¿™ä¸ªæŽ¢æµ‹å™¨åŽ»çž§çž§ï¼Œçœ‹çœ‹å’±å®¶çš„æ¨¡åž‹åˆ°åº•æ”¯ä¸æ”¯æŒ `apply_chat_template` è¿™é¡¹æŠ€èƒ½ï¼š

```python
llm.get_meta(model="chat")
```

è¾“å‡ºï¼š

```json
{'model_deploy_type': 'proprietary',
 'backend': 'ray/vllm',
 'support_stream': True,
 'support_chat_template': True,
 'max_model_len': 4096,
 'architectures': ['LlamaForCausalLM']}
```

æ³¨æ„ï¼Œè¿™é¡¹ç‰¹æ€§ä¼šè§¦å‘é¢å¤–çš„RPCè°ƒç”¨ï¼Œå› æ­¤ä¼šé€ æˆä¸€å®šçš„æ€§èƒ½æŸå¤±ã€‚

## LLM é»˜è®¤å‚æ•°

Byzer-LLM åŒæ ·æ”¯æŒä¸ºæ¨¡åž‹è®¾ç½®é»˜è®¤ç”Ÿæˆå‚æ•°ã€‚ä»¥ä¸‹ä»£ç å°†ä¸ºåä¸º `chat` çš„æ¨¡åž‹å®žä¾‹è®¾ç½®é»˜è®¤ç”Ÿæˆå‚æ•°ï¼š

```python
llm.setup_extra_generation_params("chat",{
    "generation.stop_token_ids":[7]
})
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå¯¹äºŽæ¨¡åž‹å®žä¾‹ `chat`ï¼Œæˆ‘ä»¬å°†æŠŠ `generation.stop_token_ids` å‚æ•°è®¾ç½®ä¸ºæ•°ç»„ `[7]`ã€‚è¿™æ„å‘³ç€æ¯æ¬¡è°ƒç”¨ `chat` æ¨¡åž‹æ‰§è¡Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ä½¿ç”¨è¿™ä¸ªé¢„è®¾å€¼ï¼Œå³å°†åœæ­¢ç”Ÿæˆåºåˆ—çš„æ ‡è¯†ç¬¦`generation.stop_token_ids`è®¾ä¸º `[7]` çš„ç‰¹æ®Šæ ‡è®°ã€‚å½“æ¨¡åž‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é‡åˆ°è¯¥åœç”¨è¯IDæ—¶ï¼Œå°±ä¼šåœæ­¢ç”Ÿæˆæ–°çš„æ–‡æœ¬ç‰‡æ®µã€‚

## å¤šæ¨¡æ€

Byzer å¤§è¯­è¨€æ¨¡åž‹ï¼ˆByzer-LLMï¼‰åŒæ ·å…·å¤‡å¤„ç†å¤šæ¨¡æ€æ•°æ®çš„èƒ½åŠ›ã€‚æŽ¥ä¸‹æ¥å±•ç¤ºçš„ä»£ç ç‰‡æ®µå°†ä¼šéƒ¨ç½²ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡åž‹ï¼ŒéšåŽè¿ç”¨æ­¤æ¨¡åž‹å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œæ™ºèƒ½æŽ¨æ–­ã€‚

```python
import ray
from byzerllm.utils.client import ByzerLLM,InferBackend

ray.init(address="auto",namespace="default")   

llm = ByzerLLM()
chat_model_name = "qwen_vl_chat"
model_location = "/home/byzerllm/models/Qwen-VL-Chat"

llm.setup_gpus_per_worker(1).setup_num_workers(1).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/qwen_vl_chat",
    udf_name=chat_model_name,
    infer_params={}
)    
```

éšåŽï¼Œä½ å¯ä»¥å€ŸåŠ©è¿™ä¸ªæ¨¡åž‹è¿›è¡Œå®žæ—¶å¯¹è¯äº¤äº’ï¼š

```python
import base64
image_path = "/home/byzerllm/projects/jupyter-workspace/1.jpg"
with open(image_path, "rb") as f:
    image_content = base64.b64encode(f.read()).decode("utf-8")

t = llm.chat_oai(conversations=[{
    "role": "user",
    "content": "è¿™æ˜¯ä»€ä¹ˆ"
}],model=chat_model_name,llm_config={"image":image_content})

t[0].output

# '{"response": "å›¾ä¸­æ˜¯ä¸€åå¥³å­åœ¨æ²™æ»©ä¸Šå’Œç‹—çŽ©è€ï¼Œæ—è¾¹çš„ç‹—æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ï¼Œå®ƒååœ¨æ²™æ»©ä¸Šï¼Œé¢å¯¹ç€ä¸€åèº«ç©¿æ ¼å­è¡¬è¡«çš„å¥³å­ã€‚å¥³å­çš„è…¿æœ‰äº›æ®‹ç–¾ï¼Œä½†æ˜¯å¥¹ä¾ç„¶åšæŒååœ¨æ²™æ»©ä¸Šå’Œç‹—çŽ©è€ã€‚å¥¹çš„å³æ‰‹æ‹¿ç€ä¸€ä¸ªå°çŽ©å…·ï¼Œè¿™ä¸ªçŽ©å…·ä¸Šé¢æœ‰ä¸¤è¡Œé»‘è‰²å­—æ¯ï¼Œå…·ä½“æ˜¯ä»€ä¹ˆå†…å®¹çœ‹ä¸æ¸…æ¥šã€‚å¥¹æ‰“ç®—æŠŠçŽ©å…·æ‰”ç»™æ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚", "history": [{"role": "user", "content": "Picture 1: <img>/tmp/byzerllm/visualglm/images/23eb4cea-cb6e-4f55-8adf-3179ca92ab42.jpg</img>\\nè¿™æ˜¯ä»€ä¹ˆ"}, {"role": "assistant", "content": "å›¾ä¸­æ˜¯ä¸€åå¥³å­åœ¨æ²™æ»©ä¸Šå’Œç‹—çŽ©è€ï¼Œæ—è¾¹çš„ç‹—æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ï¼Œå®ƒååœ¨æ²™æ»©ä¸Šï¼Œé¢å¯¹ç€ä¸€åèº«ç©¿æ ¼å­è¡¬è¡«çš„å¥³å­ã€‚å¥³å­çš„è…¿æœ‰äº›æ®‹ç–¾ï¼Œä½†æ˜¯å¥¹ä¾ç„¶åšæŒååœ¨æ²™æ»©ä¸Šå’Œç‹—çŽ©è€ã€‚å¥¹çš„å³æ‰‹æ‹¿ç€ä¸€ä¸ªå°çŽ©å…·ï¼Œè¿™ä¸ªçŽ©å…·ä¸Šé¢æœ‰ä¸¤è¡Œé»‘è‰²å­—æ¯ï¼Œå…·ä½“æ˜¯ä»€ä¹ˆå†…å®¹çœ‹ä¸æ¸…æ¥šã€‚å¥¹æ‰“ç®—æŠŠçŽ©å…·æ‰”ç»™æ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚"}]}'
```

æŽ¥ä¸‹æ¥è¿™æ®µä»£ç å¯ä»¥å¸®åŠ©ä½ è¿žç»­ä¸æ–­åœ°ä¸Žæ¨¡åž‹è¿›è¡Œå¤šå›žåˆçš„å¯¹è¯äº¤æµï¼š

```python
import json
history = json.loads(t[0].output)["history"]

llm.chat_oai(conversations=history+[{
    "role": "user",
    "content": "èƒ½åœˆå‡ºç‹—ä¹ˆï¼Ÿ"
}],model=chat_model_name,llm_config={"image":image_content})

# [LLMResponse(output='{"response": "<ref>ç‹—</ref><box>(221,425),(511,889)</box>", "history": [{"role"
```

é¦–å…ˆï¼Œæå–ä¸Šæ¬¡å¯¹è¯çš„èŠå¤©è®°å½•ï¼Œç„¶åŽå°†è¿™éƒ¨åˆ†åŽ†å²å†…å®¹èžå…¥æ–°çš„å¯¹è¯çŽ¯èŠ‚ï¼Œè¿›è€Œç»§ç»­å¼€å±•æ–°çš„å¯¹è¯äº¤æµã€‚

## StableDiffusion

Tyzer å¤§è¯­è¨€æ¨¡åž‹ï¼ˆByzer-LLMï¼‰åŒæ ·æ”¯æŒé›†æˆ StableDiffusion æŠ€æœ¯ä½œä¸ºå…¶åº•å±‚æŽ¨ç†æ¡†æž¶ã€‚æŽ¥ä¸‹æ¥çš„ä»£ç å°†éƒ¨ç½²ä¸€ä¸ªåŸºäºŽ StableDiffusion çš„æ¨¡åž‹ï¼Œå¹¶å€ŸåŠ©æ­¤æ¨¡åž‹å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œæ·±åº¦ç†è§£å’Œè§†è§‰ç”Ÿæˆç­‰æ–¹é¢çš„æ™ºèƒ½æŽ¨æ–­ã€‚

```python
import ray
from byzerllm.utils.client import ByzerLLM,InferBackend

ray.init(address="auto",namespace="default")   

llm = ByzerLLM()
chat_model_name = "sd_chat"
model_location = "/home/byzerllm/models/stable-diffusion-v1-5"

llm.setup_gpus_per_worker(2).setup_num_workers(1).setup_infer_backend(InferBackend.Transformers)
llm.deploy(
    model_path=model_location,
    pretrained_model_type="custom/stable_diffusion",
    udf_name=chat_model_name,
    infer_params={}
)

def show_image(content):
    from IPython.display import display, Image
    import base64             
    img = Image(base64.b64decode(content))
    display(img)    
    
```

ç„¶åŽå°±å¯ä»¥é€šè¿‡è¿™ä¸ªæ¨¡åž‹è¿›è¡Œå¯¹è¯ï¼š

```python
import json
t = llm.chat_oai(
    conversations=[
        {
            "role":"user",
            "content":"ç”»ä¸€åªçŒ«"
        }
    ],model=chat_model_name,llm_config={"gen.batch_size":3}
)

cats = json.loads(t[0].output)
for res in cats:
    show_image(res["img64"])
```

è¾“å‡ºï¼š

![](./images/cat2.png)

The parameters:
å‚æ•°é…ç½®ï¼š

| å‚æ•°                        | å«ä¹‰                                                         | é»˜è®¤å€¼   |
| --------------------------- | ------------------------------------------------------------ | -------- |
| Instruction                 | prompt                                                       | éžç©º     |
| generation.negative_prompt  | åå‘çš„prompt                                                 | ""       |
| generation.sampler_name     | è°ƒåº¦å(unpic, euler_a,euler,ddim,ddpm,deis,dpm2,dpm2-a,dpm++_2m,dpm++_2m_karras,heun,heun_karras,lms,pndm:w) | euler_a  |
| generation.sampling_steps   | ç”Ÿæˆçš„æ­¥éª¤æ•°                                                 | 25       |
| generation.batch_size       | ä¸€æ¬¡ç”Ÿæˆå‡ å¼                                                  | 1        |
| generation.batch_count      | ç”Ÿæˆå‡ æ¬¡                                                     | 1        |
| generation.cfg_scale        | éšæœºæˆ–è´´åˆç¨‹åº¦å€¼,å€¼è¶Šå°ç”Ÿæˆçš„å›¾ç‰‡ç¦»ä½ çš„Tagsæè¿°çš„å†…å®¹å·®è·è¶Šå¤§ | 7.5      |
| generation.seed             | éšæœºç§å­                                                     | -1       |
| generation.width            | å›¾ç‰‡å®½åº¦                                                     | 768      |
| generation.height           | å›¾ç‰‡é«˜åº¦                                                     | 768      |
| generation.enable_hires     | å¼€å¯é«˜åˆ†è¾¨çŽ‡ä¿®å¤åŠŸèƒ½(å’Œä¸‹é¢ä¸¤ä¸ªä¸€ç»„)                         | false    |
| generation.upscaler_mode    | æ”¾å¤§ç®—æ³•(bilinear, bilinear-antialiased,bicubic,bicubic-antialiased,nearest,nearest-exact) | bilinear |
| generation.scale_slider     | æ”¾å¤§æ¯”ä¾‹                                                     | 1.5      |
| generation.enable_multidiff | å›¾ç‰‡åˆ†å‰²å¤„ç†(å‡å°‘æ˜¾å­˜é”€è€—)(å’Œä¸‹é¢3ä¸ªä¸€ç»„)                    | false    |
| generation.views_batch_size | åˆ†æ‰¹å¤„ç†è§„æ¨¡                                                 | 4        |
| generation.window_size      | åˆ‡å‰²å¤§å°ï¼Œå®½ï¼Œé«˜                                             | 64       |
| generation.stride           | æ­¥é•¿                                                         | 16       |
| generation.init_image       | åˆå§‹åŒ–å›¾ç‰‡ï¼ŒåŸºäºŽè¿™ä¸ªå›¾ç‰‡å¤„ç†(å¿…é¡»ä¼ è¾“base64åŠ å¯†çš„å›¾ç‰‡) (å’Œä¸‹é¢çš„ä¸€ç»„) | None     |
| generation.strength         | é‡ç»˜å¹…åº¦: å›¾åƒæ¨¡ä»¿è‡ªç”±åº¦ï¼Œè¶Šé«˜è¶Šè‡ªç”±å‘æŒ¥ï¼Œè¶Šä½Žå’Œå‚è€ƒå›¾åƒè¶ŠæŽ¥è¿‘ï¼Œé€šå¸¸å°äºŽ0.3åŸºæœ¬å°±æ˜¯åŠ æ»¤é•œ | 0.5      |



## SQL æ”¯æŒ

é™¤äº† Python æŽ¥å£ä¹‹å¤–ï¼ŒByzer-llm åŒæ ·å…¼å®¹ SQL APIã€‚è‹¥è¦ä½¿ç”¨ SQL API åŠŸèƒ½ï¼Œè¯·å…ˆç¡®ä¿å®‰è£… Byzer-SQL è¯­è¨€ã€‚

å¯é‡‡ç”¨å¦‚ä¸‹å‘½ä»¤æ¥å®‰è£… Byzer-SQL è¯­è¨€ï¼š

```bash
git clone https://gitee.com/allwefantasy/byzer-llm
cd byzer-llm/setup-machine
sudo -i 
ROLE=master ./setup-machine.sh
```

å®‰è£…æˆåŠŸåŽï¼Œæ‚¨å¯ä»¥è®¿é—®æœ¬åœ° Byzer æŽ§åˆ¶å°ï¼Œåœ°å€ä¸ºï¼š`http://localhost:9002`ã€‚

åœ¨ Byzer æŽ§åˆ¶å°å†…ï¼Œæ‚¨å¯ä»¥æ‰§è¡Œå¦‚ä¸‹ SQL å‘½ä»¤æ¥éƒ¨ç½² llama2 æ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹çš„åŠŸèƒ½ä¸Žå‰è¿° Python ä»£ç ç‰‡æ®µå®Œå…¨ä¸€è‡´ã€‚

```sql
!byzerllm setup single;
!byzerllm setup "num_gpus=4";
!byzerllm setup "maxConcurrency=1";
!byzerllm setup "infer_backend=transformers";

run command as LLM.`` where 
action="infer"
and pretrainedModelType="custom/llama2"
and localModelDir="/home/byzerllm/models/openbuddy-llama-13b-v5-fp16"
and reconnect="false"
and udfName="llama2_chat"
and modelTable="command";

```

æŽ¥ä¸‹æ¥ï¼Œæ‚¨å¯ä»¥é€šè¿‡è°ƒç”¨åä¸º `llama2_chat` çš„ UDFï¼ˆç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ï¼‰æ¥æ¿€æ´»å’Œä½¿ç”¨è¿™ä¸ªlama2æ¨¡åž‹ï¼š

```sql

select 
llama2_chat(llm_param(map(
              "user_role","User",
              "assistant_role","Assistant",
              "system_msg",'You are a helpful assistant. Think it over and answer the user question correctly.',
              "instruction",llm_prompt('
Please remenber my name: {0}              
',array("Zhu William"))

))) as q 
as q1;
```

å½“ä½ ä½¿ç”¨ç±»ä¼¼ `run command as LLM` çš„æ–¹å¼éƒ¨ç½²äº†æ¨¡åž‹åŽï¼Œå°±èƒ½å¤Ÿå¦‚åŒè°ƒç”¨ SQL å‡½æ•°é‚£æ ·æ¥ä½¿ç”¨è¯¥æ¨¡åž‹ã€‚è¿™ä¸€ç‰¹ç‚¹æžå¤§åœ°ä¾¿åˆ©äº†é‚£äº›å¸Œæœ›å»ºç«‹æ•°æ®åˆ†æžæ¨¡åž‹æ—¶èžå…¥å¤§è¯­è¨€æ¨¡åž‹èƒ½åŠ›çš„æ•°æ®ç§‘å­¦å®¶ï¼Œä»¥åŠæœŸæœ›åœ¨æž„å»ºæ•°æ®å¤„ç†æµæ°´çº¿æ—¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹åŠŸèƒ½çš„æ•°æ®å·¥ç¨‹å¸ˆä»¬ã€‚

---

### QWen 

åœ¨ ByzerLLM ä¸­ä½¿ç”¨ QWen åŠŸèƒ½æ—¶ï¼Œä½ éœ€è¦æ‰‹åŠ¨è®¾ç½®å‡ ä¸ªå…³é”®å‚æ•°ï¼š

1. è§’è‰²å¯¹åº”å…³ç³»ï¼ˆè§’è‰²æ˜ å°„ï¼‰
2. ç»ˆæ­¢æ ‡è¯†ç¬¦åˆ—è¡¨ï¼ˆç»“æŸç¬¦å·IDåˆ—è¡¨ï¼‰
3. ä»Žç”Ÿæˆçš„å›žç­”ä¸­åŽ»é™¤ç»ˆæ­¢æ ‡è¯†ç¬¦ï¼ˆç®€å•åœ°è¯´ï¼Œå°±æ˜¯åœ¨ç”Ÿæˆç»“æžœä¸­è£å‰ªæŽ‰ä»£è¡¨å¯¹è¯ç»“æŸçš„ç‰¹æ®Šç¬¦å·ï¼‰

ä¸ºäº†æ–¹ä¾¿å¤§å®¶æ“ä½œï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªé¢„è®¾æ¨¡æ¿ï¼Œä½ å¯ä»¥è¯•è¯•ä¸‹é¢è¿™æ®µä»£ç ï¼š

```python
from byzerllm.utils.client import Templates

### Here,we setup the template for qwen
llm.setup_template("chat",Templates.qwen())

t = llm.chat_oai(conversations=[{
    "role":"user",
    "content":"ä½ å¥½,ç»™æˆ‘è®²ä¸ª100å­—çš„ç¬‘è¯å§?"
}])
print(t)
```

---
## SaaS æ¨¡åž‹

é‰´äºŽå„ç±» SaaS æ¨¡å¼å…·æœ‰å„è‡ªçš„å®šåˆ¶å‚æ•°ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸ºæ‚¨å‡†å¤‡äº†ä¸€ç³»åˆ— SaaS æ¨¡åž‹éƒ¨ç½²æ‰€éœ€çš„æ¨¡æ¿ï¼ŒåŠ©åŠ›æ‚¨è½»æ¾å®Œæˆä¸åŒ SaaS æ¨¡åž‹çš„éƒ¨ç½²å·¥ä½œã€‚

### ç™¾å·ï¼ˆbaichuanï¼‰

```python

import ray
from byzerllm.utils.client import ByzerLLM

ray.init(address="auto",namespace="default",ignore_reinit_error=True)   

llm = ByzerLLM(verbose=True)

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "baichuan_chat2"
if llm.is_model_exist(chat_name):
    llm.undeploy(udf_name=chat_name)

llm.deploy(model_path="",
           pretrained_model_type="saas/baichuan",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxxxxx",                        
            "saas.model":"Baichuan2-Turbo"
           })

llm.chat_oai(model=chat_name,conversations=[{
    "role":"user",
    "content":"ä½ å¥½",
}])           
```

é’ˆå¯¹ `saas.model` å‚æ•°ï¼Œè¿™é‡Œæœ‰ä¸€äº›æžšä¸¾å€¼å¯ä¾›é€‰æ‹©ï¼š

1. Baichuan2-Turbo
2. Baichuan-Text-Embedding

### é€šä¹‰åƒé—®ï¼ˆqianwenï¼‰

```python
from byzerllm.utils.client import ByzerLLM
llm = ByzerLLM()

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "qianwen_chat"

llm.deploy(model_path="",
           pretrained_model_type="saas/qianwen",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxxxxx",            
            "saas.model":"qwen-turbo"
           })

## here you can use `stream_chat_oai`
v = llm.stream_chat_oai(model=chat_name,conversations=[{
    "role":"user",
    "content":"ä½ å¥½ï¼Œä½ æ˜¯è°",
}],llm_config={"gen.incremental_output":False})

for t in v:
    print(t,flush=True)           
```

é’ˆå¯¹ `saas.model` å‚æ•°ï¼Œè¿™é‡Œæœ‰å‡ ä¸ªé¢„è®¾çš„æžšä¸¾å€¼é€‰é¡¹ï¼š

1. qwen-turbo
2. qwen-max

### azure openai

```sql

!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/azure_openai"
and `saas.api_type`="azure"
and `saas.api_key`="xxx"
and `saas.api_base`="xxx"
and `saas.api_version`="2023-07-01-preview"
and `saas.deployment_id`="xxxxx"
and udfName="azure_openai"
and modelTable="command";
```

### openai

```sql

import ray

from byzerllm.utils.client import ByzerLLM

ray.init(address="auto",namespace="default",ignore_reinit_error=True)  

llm = ByzerLLM()

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "openai_chat"

llm.deploy(model_path="",
           pretrained_model_type="saas/official_openai",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxx",            
            "saas.model":"gpt-3.5-turbo-1106"
           })
```

è‹¥æ‚¨éœ€è¦ç”¨åˆ°ç½‘ç»œä»£ç†ï¼Œå¯ä»¥å°è¯•è¿è¡Œå¦‚ä¸‹ä»£ç æ¥é…ç½®ä»£ç†è®¾ç½®ï¼š

```python
llm.deploy(model_path="",
           pretrained_model_type="saas/official_openai",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxx",            
            "saas.model":"gpt-3.5-turbo-1106"
            "saas.base_url": "http://my.test.server.example.com:8083",
            "saas.proxies":"http://my.test.proxy.example.com"
            "saas.local_address":"0.0.0.0"
           })
```


### æ™ºè°±ï¼ˆzhipuï¼‰

```python
import ray

from byzerllm.utils.client import ByzerLLM

ray.init(address="auto",namespace="default",ignore_reinit_error=True)  

llm = ByzerLLM()

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "zhipu_chat"

llm.deploy(model_path="",
           pretrained_model_type="saas/zhipu",
           udf_name=chat_name,
           infer_params={
            "saas.api_key":"xxxx",            
            "saas.model":"glm-4"
           })
```

é’ˆå¯¹ `saas.model` å‚æ•°ï¼Œè¿™é‡Œæœ‰å‡ ä¸ªé¢„è®¾çš„æžšä¸¾å€¼é€‰é¡¹ï¼š

1. glm-4
2. embedding-2

### minimax

```sql

!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/minimax"
and `saas.api_key`="xxxxxxxxxxxxxxxxxx"
and `saas.group_id`="xxxxxxxxxxxxxxxx"
and `saas.model`="abab5.5-chat"
and `saas.api_url`="https://api.minimax.chat/v1/text/chatcompletion_pro"
and udfName="minimax_saas"
and modelTable="command";

```

### æ˜Ÿç«ï¼ˆsparkdeskï¼‰

```python
import ray

from byzerllm.utils.client import ByzerLLM

ray.init(address="auto",namespace="default",ignore_reinit_error=True)

llm = ByzerLLM()

llm.setup_num_workers(1).setup_gpus_per_worker(0)

chat_name = "sparkdesk_saas"

if llm.is_model_exist(chat_name):
  llm.undeploy(udf_name=chat_name)

llm.deploy(model_path="",
           pretrained_model_type="saas/sparkdesk",
           udf_name=chat_name,
           infer_params={
             "saas.appid":"xxxxxxx",
             "saas.api_key":"xxxxxxx",
             "saas.api_secret":"xxxxxxx",
             "saas.gpt_url":"wss://spark-api.xf-yun.com/v3.1/chat",
             "saas.domain":"generalv3"
           })

v = llm.chat_oai(model=chat_name,conversations=[{
  "role":"user",
  "content":"your prompt content",
}])
```

SparkDesk V1.5 ç‰ˆæœ¬è¯·æ±‚é“¾æŽ¥ï¼Œå…³è”çš„åŸŸåå‚æ•°ä¸ºâ€œgeneralâ€ï¼š
`wss://spark-api.xf-yun.com/v1.1/chat`  

SparkDesk V2 ç‰ˆæœ¬è¯·æ±‚é“¾æŽ¥ï¼Œå…³è”çš„åŸŸåå‚æ•°ä¸ºâ€œgeneralv2â€ï¼š
`wss://spark-api.xf-yun.com/v2.1/chat`  

SparkDesk V3 ç‰ˆæœ¬è¯·æ±‚é“¾æŽ¥ï¼Œå…³è”çš„åŸŸåå‚æ•°æ›´æ–°ä¸ºâ€œgeneralv3â€ï¼ˆçŽ°å·²æ”¯æŒå‡½æ•°è°ƒç”¨åŠŸèƒ½ï¼‰ï¼š
`wss://spark-api.xf-yun.com/v3.1/chat`  

```sql
!byzerllm setup single;
!byzerllm setup "num_gpus=0";
!byzerllm setup "maxConcurrency=10";

run command as LLM.`` where
action="infer"
and pretrainedModelType="saas/sparkdesk"
and `saas.appid`="xxxxxxxxxxxxxxxxxx"
and `saas.api_key`="xxxxxxxxxxxxxxxx"
and `saas.api_secret`="xxxx"
and `gpt_url`="ws://spark-api.xf-yun.com/v1.1/chat"
and udfName="sparkdesk_saas"
and modelTable="command";
```

### AmazonBedrock

```python
import ray

from byzerllm.utils.client import ByzerLLM, Templates

ray.init(address="auto",namespace="default",ignore_reinit_error=True)

llm = ByzerLLM()

chat_name = "aws_bedrock_llama2_70b_chat"

llm.setup_num_workers(1).setup_gpus_per_worker(0)

if llm.is_model_exist(chat_name):
  llm.undeploy(udf_name=chat_name)

llm.deploy(model_path="",
           pretrained_model_type="saas/aws_bedrock",
           udf_name=chat_name,
           infer_params={
               "saas.aws_access_key": "your access key",
               "saas.aws_secret_key": "your secret key",
               "saas.region_name": "your region name",
               "saas.model_api_version": "model api version",
               "saas.model": "meta.llama2-70b-chat-v1"
           })

v = llm.chat_oai(model=chat_name,conversations=[{
  "role":"user",
  "content":"your prompt content",
}])
```

é’ˆå¯¹ `saas.model` å‚æ•°ï¼Œç›®å‰æ”¯æŒä»¥ä¸‹å‡ ä¸ªæ¨¡åž‹ï¼š

1. meta.llama2-70b-chat-v1
2. meta.llama2-13b-chat-v1
3. anthropic.claude-3-sonnet-20240229-v1:0
4. anthropic.claude-3-haiku-20240307-v1:0

---

## é¢„è®­ç»ƒ

åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä¼šè®²è§£å¦‚ä½•åˆ©ç”¨ Byzer-llm å¯¹å¤§åž‹è¯­è¨€æ¨¡åž‹è¿›è¡Œé¢„è®­ç»ƒã€‚ä¸è¿‡ç›®å‰æ¥çœ‹ï¼ŒByzer-SQL ä¸­çš„é¢„è®­ç»ƒåŠŸèƒ½æ›´ä¸ºæˆç†Ÿï¼Œå› æ­¤æˆ‘ä»¬å°†èšç„¦äºŽåœ¨ Byzer-SQL ä¸­å±•ç¤ºé¢„è®­ç»ƒè¿™ä¸€åŠŸèƒ½ã€‚

```sql
-- Deepspeed Config
set ds_config='''
{
  "gradient_accumulation_steps": 1,
  "train_micro_batch_size_per_gpu": 1,
  "prescale_gradients": false,
  "zero_allow_untested_optimizer": true,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 1e-8,
      "eps": 1.0e-8,
      "betas": [
        0.9,
        0.95
      ],
      "weight_decay": 0.1
    }
  },
  "tensorboard": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 3,
    "offload_optimizer": {
         "device": "cpu"         
     },           
    "offload_param": {
         "device": "cpu"
    },
    "contiguous_gradients": true,
    "allgather_bucket_size": 1e8,
    "reduce_bucket_size": 1e8,
    "overlap_comm": true,
    "reduce_scatter": true
  },
  "steps_per_print": 16,
  "gradient_clipping": 1.0,
  "wall_clock_breakdown": true,
  "bf16": {
    "enabled": true
  }
}
''';

-- load data
load text.`file:///home/byzerllm/data/raw_data/*`
where wholetext="true" as trainData;

select value as text,file from trainData  as newTrainData;

-- split the data into 12 partitions
run newTrainData as TableRepartition.`` where partitionNum="12" and partitionCols="file" 
as finalTrainData;


-- setup env, we use 12 gpus to pretrain the model
!byzerllm setup sfft;
!byzerllm setup "num_gpus=12";

-- specify the pretrain model type and the pretrained model path
run command as LLM.`` where 
and localPathPrefix="/home/byzerllm/models/sfft/jobs"
and pretrainedModelType="sfft/llama2"
-- original model is from
and localModelDir="/home/byzerllm/models/Llama-2-7b-chat-hf"
-- and localDataDir="/home/byzerllm/data/raw_data"

-- we use async mode to pretrain the model, since the pretrain process will take several days or weeks
-- Ray Dashboard will show the tensorboard address, and then you can monitor the loss
and detached="true"
and keepPartitionNum="true"

-- use deepspeed config, this is optional
and deepspeedConfig='''${ds_config}'''


-- the pretrain data is from finalTrainData table
and inputTable="finalTrainData"
and outputTable="llama2_cn"
and model="command"
-- some hyper parameters
and `sfft.int.max_length`="128"
and `sfft.bool.setup_nccl_socket_ifname_by_ip`="true"
;
```

å› ä¸ºæ·±é€Ÿè®­ç»ƒä¿å­˜çš„æ¨¡åž‹æ–‡ä»¶æ ¼å¼ä¸Žæ‹¥æŠ±è„¸ä¹¦æ‰€ä½¿ç”¨çš„æ¨¡åž‹æ–‡ä»¶æ ¼å¼äº’ä¸å…¼å®¹ï¼Œæˆ‘ä»¬å¿…é¡»å°†æ·±é€Ÿæ¨¡åž‹æ–‡ä»¶è½¬æ¢ä¸ºæ‹¥æŠ±è„¸ä¹¦èƒ½å¤Ÿè¯†åˆ«çš„æ ¼å¼ã€‚ä¸‹é¢è¿™æ®µä»£ç å°±æ˜¯ç”¨æ¥å®žçŽ°è¿™ä¸€è½¬æ¢ä»»åŠ¡çš„ã€‚

```sql
!byzerllm setup single;

run command as LLM.`` where 
action="convert"
and pretrainedModelType="deepspeed/llama3b"
and modelNameOrPath="/home/byzerllm/models/base_model"
and checkpointDir="/home/byzerllm/data/checkpoints"
and tag="Epoch-1"
and savePath="/home/byzerllm/models/my_3b_test2";
```

çŽ°åœ¨ï¼Œä½ å·²ç»å¯ä»¥é¡ºåˆ©éƒ¨ç½²ç»è¿‡è½¬æ¢çš„æ¨¡åž‹ï¼Œå°†å…¶æŠ•å…¥å®žé™…åº”ç”¨äº†ï¼š

```sql
-- éƒ¨ç½²hugginface æ¨¡åž‹
!byzerllm setup single;

set node="master";
!byzerllm setup "num_gpus=2";
!byzerllm setup "workerMaxConcurrency=1";

run command as LLM.`` where 
action="infer"
and pretrainedModelType="custom/auto"
and localModelDir="/home/byzerllm/models/my_3b_test2"
and reconnect="false"
and udfName="my_3b_chat"
and modelTable="command";
```

## å¾®è°ƒ

```sql
-- load data, we use the dummy data for finetune
-- data format supported by Byzer-SQLï¼šhttps://docs.byzer.org/#/byzer-lang/zh-cn/byzer-llm/model-sft

load json.`/tmp/upload/dummy_data.jsonl` where
inferSchema="true"
as sft_data;

-- Fintune Llama2
!byzerllm setup sft;
!byzerllm setup "num_gpus=4";

run command as LLM.`` where 
and localPathPrefix="/home/byzerllm/models/sft/jobs"

-- æŒ‡å®šæ¨¡åž‹ç±»åž‹
and pretrainedModelType="sft/llama2"

-- æŒ‡å®šæ¨¡åž‹
and localModelDir="/home/byzerllm/models/Llama-2-7b-chat-hf"
and model="command"

-- æŒ‡å®šå¾®è°ƒæ•°æ®è¡¨
and inputTable="sft_data"

-- è¾“å‡ºæ–°æ¨¡åž‹è¡¨
and outputTable="llama2_300"

-- å¾®è°ƒå‚æ•°
and  detached="true"
and `sft.int.max_seq_length`="512";
```

ä½ å¯ä»¥åœ¨Rayä»ªè¡¨æ¿ä¸­æŸ¥çœ‹å¾®è°ƒä½œä¸šè¿›ç¨‹ï¼ˆfinetune actorï¼‰ï¼Œå…¶åç§°é€šå¸¸æ˜¯ `sft-william-xxxxx`ã€‚

å¾…å¾®è°ƒä½œä¸šå®Œæˆä¹‹åŽï¼Œä½ å¯ä»¥èŽ·å–åˆ°æ¨¡åž‹è·¯å¾„ï¼Œè¿™æ ·å°±å¯ä»¥éƒ¨ç½²è¿™ä¸ªç»è¿‡å¾®è°ƒçš„æ¨¡åž‹äº†ã€‚

ä»¥ä¸‹æ˜¯å¾®è°ƒä½œä¸šè¿›ç¨‹çš„æ—¥å¿—è®°å½•ï¼š

```
Loading data: /home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_data/data.jsonl3
2
there are 33 data in dataset
*** starting training ***
{'train_runtime': 19.0203, 'train_samples_per_second': 1.735, 'train_steps_per_second': 0.105, 'train_loss': 3.0778136253356934, 'epoch': 0.97}35

***** train metrics *****36  
epoch                    =       0.9737  
train_loss               =     3.077838  
train_runtime            = 0:00:19.0239  
train_samples_per_second =      1.73540  
train_steps_per_second   =      0.10541

[sft-william] Copy /home/byzerllm/models/Llama-2-7b-chat-hf to /home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_model/final/pretrained_model4243              
[sft-william] Train Actor is already finished. You can check the model in: /home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_model/final   
```

ä½ å¯ä»¥ä»Žè·¯å¾„ `/home/byzerllm/projects/sft/jobs/sft-william-20230809-13-04-48-674fd1b9-2fc1-45b9-9d75-7abf07cb84cb/finetune_model/final` ä¸‹è½½å·²å®Œæˆå¾®è°ƒçš„æ¨¡åž‹ï¼Œæˆ–è€…å°†æ¨¡åž‹å¤åˆ¶åˆ°Rayé›†ç¾¤ä¸­çš„æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ä¸Šã€‚

çŽ°åœ¨ï¼Œå°è¯•éƒ¨ç½²è¿™ä¸ªç»è¿‡å¾®è°ƒçš„æ¨¡åž‹ï¼š

```sql
!byzerllm setup single;
run command as LLM.`` where 
action="infer"
and localPathPrefix="/home/byzerllm/models/infer/jobs"
and localModelDir="/home/byzerllm/models/sft/jobs/sft-william-llama2-alpaca-data-ccb8fb55-382c-49fb-af04-5cbb3966c4e6/finetune_model/final"
and pretrainedModelType="custom/llama2"
and udfName="fintune_llama2_chat"
and modelTable="command";
```

Byzer-LLM åˆ©ç”¨ QLora å¯¹æ¨¡åž‹è¿›è¡Œå¾®è°ƒï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç å°†å¾®è°ƒåŽçš„æ¨¡åž‹ä¸ŽåŽŸå§‹æ¨¡åž‹è¿›è¡Œåˆå¹¶ï¼š

```sql
-- åˆå¹¶lora model + base model

!byzerllm setup single;

run command as LLM.`` where 
action="convert"
and pretrainedModelType="deepspeed/llama"
and model_dir="/home/byzerllm/models/sft/jobs/sft-william-20230912-21-50-10-2529bf9f-493e-40a3-b20f-0369bd01d75d/finetune_model/final/pretrained_model"
and checkpoint_dir="/home/byzerllm/models/sft/jobs/sft-william-20230912-21-50-10-2529bf9f-493e-40a3-b20f-0369bd01d75d/finetune_model/final"
and savePath="/home/byzerllm/models/sft/jobs/sft-william-20230912-21-50-10-2529bf9f-493e-40a3-b20f-0369bd01d75d/finetune_model/merge";

```

## æ–‡ç« 

1. [ä¸€å£æ°”é›†æˆé‚£äº›ä¸ªå¤§æ¨¡åž‹ä½ ä¹Ÿè¯•è¯•](https://www.51xpage.com/ai/yi-kou-qi-ji-cheng-na-xie-ge-da-mo-xing-ni-ye-shi-shi-unknown-unknown-man-man-xue-ai006/)
2. [Byzer-LLM å¿«é€Ÿä½“éªŒæ™ºè°± GLM-4](https://mp.weixin.qq.com/s/Zhzn_C9-dKP4Nq49h8yUxw)
3. [å‡½æ•°å®žçŽ°è¶Šé€šç”¨è¶Šå¥½ï¼Ÿæ¥çœ‹çœ‹ Byzer-LLM çš„ Function Implementation å¸¦æ¥çš„ç¼–ç¨‹æ€æƒ³å¤§å˜åŒ–](https://mp.weixin.qq.com/s/_Sx0eC0WqC2M4K1JY9f49Q)
4. [Byzer-LLM ä¹‹ QWen-VL-Chat/StableDiffusionå¤šæ¨¡æ€è¯»å›¾ï¼Œç”Ÿå›¾](https://mp.weixin.qq.com/s/x4g66QvocE5dUlnL1yF9Dw)
5. [åŸºäºŽByzer-Agent æ¡†æž¶å¼€å‘æ™ºèƒ½æ•°æ®åˆ†æžå·¥å…·](https://mp.weixin.qq.com/s/BcoHUEXF24wTjArc7mwNaw)
6. [Byzer-LLM æ”¯æŒåŒæ—¶å¼€æºå’ŒSaaSç‰ˆé€šä¹‰åƒé—®](https://mp.weixin.qq.com/s/VvzMUV654D7IO0He47nv3A)
7. [ç»™å¼€æºå¤§æ¨¡åž‹å¸¦æ¥Function Callingã€ Respond With Class](https://mp.weixin.qq.com/s/GTVCYUhR_atYMX9ymp0eCg)









##File: /Users/allwefantasy/projects/byzer-llm/.pytest_cache/README.md
# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.


##File: /Users/allwefantasy/projects/byzer-llm/docs/zh/001_ä¸€ä¸ªåŠªåŠ›æˆä¸ºå¤§æ¨¡åž‹ç¼–ç¨‹æŽ¥å£çš„ç¥žå¥‡Pythonåº“.md
# ä¸€ä¸ªåŠªåŠ›æˆä¸ºå¤§æ¨¡åž‹ç¼–ç¨‹æŽ¥å£çš„ç¥žå¥‡Pythonåº“

## å‰è¨€ï¼ˆåŠªåŠ›å‹¾èµ·ä½ çš„å¥½å¥‡å¿ƒï¼‰

å¦‚æžœæœ‰è¿™ä¹ˆä¸€ä¸ªPythonåº“ï¼š

1. å¯ä»¥ç”¨ä¸€è‡´çš„æ–¹å¼éƒ¨ç½²å¼€æºå¤§æ¨¡åž‹ï¼ŒSaaSæ¨¡åž‹ï¼Œæ¯”å¦‚ç”¨ä¸€ä¸ª deploy å‘½ä»¤å°±æžå®š
2. æ”¯æŒå¤§è¯­è¨€æ¨¡åž‹ï¼Œå¤šæ¨¡æ€æ¨¡åž‹ï¼Œå›¾ç”Ÿæ–‡ï¼Œæ–‡ç”Ÿå›¾ï¼Œè¯­éŸ³åˆæˆå•¥çš„ä¸€ä¸ªéƒ½ä¸å°‘ã€‚
3. æä¾›äº†ä¸€è‡´çš„Python API, å¯ä»¥ç§’æ¢ä»»ä½•å¤§æ¨¡åž‹
4. æä¾›è¿‡äº†å…¼å®¹ OpenAI çš„æŽ¥å£ï¼Œç”¨ ä¸€æ¡ serve å‘½ä»¤æžå®šã€‚
5. è¿˜æ”¯æŒé¢„è®­ç»ƒï¼Œå¾®è°ƒå¤§æ¨¡åž‹
6. è¿˜èƒ½æ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²å’Œç®¡ç†ï¼Œç”Ÿäº§So easy.
7. æ”¯æŒGPUï¼ŒCPUæˆ–è€…æ··åˆéƒ¨ç½²
8. æ”¯æŒ Ollama çš„ä»£ç†ã€‚
9. è‡ªå¸¦å‘é‡/å…¨æ–‡æ£€ç´¢æ•°æ®åº“

æœ‰è¿™äº›å¤Ÿäº†ä¹ˆï¼Ÿ No,!No,!No! å®ƒçš„ä½¿å‘½æ˜¯è®©æˆ‘ä»¬æ›´å¥½çš„ä½¿ç”¨å¤§æ¨¡åž‹ï¼Œä½ æ˜¯ä¸æ˜¯è¢«prompt ç®¡ç†æžåˆ°ç„¦å¤´çƒ‚é¢ï¼Ÿä»–æä¾›äº† prompt å‡½æ•°å’Œç±»ï¼Œè®©ä½ å’Œä»£ç ä¸€æ ·ä½¿ç”¨æ–‡æœ¬ã€‚ä½ æ˜¯ä¸æ˜¯è‹¦äºŽ Function Calling ä¸æ˜¯æ‰€æœ‰æ¨¡åž‹éƒ½èƒ½æ”¯æŒï¼Ÿ è¿™ä¸ªç¥žå¥‡çš„ Python åº“è¿˜æä¾›ä¸ä¾èµ–äºŽåº•å±‚æ¨¡åž‹çš„ç›¸å…³å®žçŽ°ï¼Œæ€»ä¹‹ç»™ä½ æä¾›ä¸€åˆ‡è®©ä½ è§‰å¾—ç‰¹åˆ«æ›¼å¦™çš„ç¼–ç¨‹æŽ¥å£ã€‚

è¿™ä¸ªç¥žå¥‡çš„åº“å°±æ˜¯ [byzerllm](https://github.com/allwefantasy/byzer-llm)ï¼Œ å¥¹ä¹Ÿæ˜¯ [AutoCoder](https://github.com/allwefantasy/auto-coder) é»˜è®¤çš„æ¨¡åž‹ç¼–ç¨‹æŽ¥å£ã€‚

## å®‰è£…

ä¸€æ¡å‘½ä»¤æžå®šï¼š

```bash
pip install -U auto-coder
ray start --head
```

å®‰è£… auto-coder ä¼šè‡ªåŠ¨å®‰è£… byzerllmä»¥åŠä¸€äº›ä¾èµ–åº“ã€‚

ç¬¬äºŒæ¡å‘½ä»¤æ˜¯å¯åŠ¨ä¸€ä¸ª server è¿›ç¨‹ï¼ŒæŽ¥å—åŽç»­è¯¸å¦‚ deploy æ¨¡åž‹ç­‰æŒ‡ä»¤ã€‚

å¦‚æžœä½ ä½¿ç”¨

```bash
pip install -U byzer-llm
```

åˆ™éœ€è¦è‡ªå·±å®‰è£…ä¾èµ–ï¼Œæ¯”è¾ƒéº»çƒ¦ï¼Œæˆ‘ä»¬è¿˜æ˜¯é€‰æ‹©ç¬¬ä¸€ä¸ªå§ã€‚

## è®©æˆ‘ä»¬å¼€å§‹çŽ©è€å§

æ¯”å¦‚ä½ æ‰‹å¤´æœ‰ä¸ª kimi, openai æˆ–è€… deepseek ç­‰ä»»æ„ SaaS æ¨¡åž‹çš„Tokenï¼Œæˆ‘ä»¬å°±å¯ä»¥éƒ¨ç½²ä»–ä»¬ï¼š

```bash
byzerllm deploy --pretrained_model_type saas/openai \
--cpus_per_worker 0.001 \
--gpus_per_worker 0 \
--num_workers 3 \
--infer_params  saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=gpt-3.5-turbo-0125 \
--model gpt3_5_chat
```

è¿™æ ·å°±éƒ¨ç½²äº†ä¸€ä¸ª Kimi æ¨¡åž‹çš„ä»£ç†ã€‚å…¶ä»–å‚æ•°ä¸»è¦æ˜¯ä¸€äº›èµ„æºé…ç½®ï¼Œå› ä¸ºæ˜¯SaaSæ¨¡åž‹ï¼Œæˆ‘ä»¬ä¸è®¸å“Ÿå•ŠGPU,ä»…éœ€å°‘é‡CPUå³å¯ã€‚

ä½ é©¬ä¸Šå°±å¯ä»¥é€šè¿‡å‘½ä»¤è¡ŒéªŒè¯ä¸‹ï¼š

```bash
byzerllm query --model gpt3_5_chat --query 'hello, who are you?'
```

![](../source/assets/image.png)

å¦‚æžœæ˜¯éƒ¨ç½²ä¸€ä¸ªå¼€æºå¤§æ¨¡åž‹ï¼Œä¸€æ¯›ä¸€æ ·çš„éƒ¨ç½²æ–¹æ³•ï¼š

```bash
byzerllm deploy --pretrained_model_type custom/auto \
--infer_backend vllm \
--model_path /home/winubuntu/models/openbuddy-zephyr-7b-v14.1 \
--cpus_per_worker 0.001 \
--gpus_per_worker 1 \
--num_workers 1 \
--infer_params backend.max_model_len=28000 \
--model zephyr_7b_chat
```

è¿™é‡Œæˆ‘æŒ‡å®šäº†ç”¨ä¸€å— GPU éƒ¨ç½²ï¼Œæ¨¡åž‹æœ€å¤§tokené•¿åº¦ä¸º 28000ã€‚

ä½ å¯ä»¥é€šè¿‡ `byzerllm stat` æ¥æŸ¥çœ‹å½“å‰éƒ¨ç½²çš„æ¨¡åž‹çš„çŠ¶æ€ã€‚

```bash
byzerllm stat --model gpt3_5_chat
```

è¾“å‡ºï¼š
```
Command Line Arguments:
--------------------------------------------------
command             : stat
ray_address         : auto
model               : gpt3_5_chat
file                : None
--------------------------------------------------
2024-05-06 14:48:17,206	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379...
2024-05-06 14:48:17,222	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at 127.0.0.1:8265
{
    "total_workers": 3,
    "busy_workers": 0,
    "idle_workers": 3,
    "load_balance_strategy": "lru",
    "total_requests": [
        33,
        33,
        32
    ],
    "state": [
        1,
        1,
        1
    ],
    "worker_max_concurrency": 1,
    "workers_last_work_time": [
        "631.7133535240428s",
        "631.7022202090011s",
        "637.2349605050404s"
    ]
}
```
è§£é‡Šä¸‹ä¸Šé¢çš„è¾“å‡ºï¼š

1. total_workers: æ¨¡åž‹gpt3_5_chatçš„å®žé™…éƒ¨ç½²çš„workerå®žä¾‹æ•°é‡
2. busy_workers: æ­£åœ¨å¿™ç¢Œçš„workerå®žä¾‹æ•°é‡
3. idle_workers: å½“å‰ç©ºé—²çš„workerå®žä¾‹æ•°é‡
4. load_balance_strategy: ç›®å‰å®žä¾‹ä¹‹é—´çš„è´Ÿè½½å‡è¡¡ç­–ç•¥
5. total_requests: æ¯ä¸ªworkerå®žä¾‹çš„ç´¯è®¡çš„è¯·æ±‚æ•°é‡
6. worker_max_concurrency: æ¯ä¸ªworkerå®žä¾‹çš„æœ€å¤§å¹¶å‘æ•°
7. state: æ¯ä¸ªworkerå®žä¾‹å½“å‰ç©ºé—²çš„å¹¶å‘æ•°ï¼ˆæ­£åœ¨è¿è¡Œçš„å¹¶å‘=worker_max_concurrency-å½“å‰stateçš„å€¼ï¼‰
8. workers_last_work_time: æ¯ä¸ªworkerå®žä¾‹æœ€åŽä¸€æ¬¡è¢«è°ƒç”¨çš„æˆªæ­¢åˆ°çŽ°åœ¨çš„æ—¶é—´


è‡³äºŽä½¿ç”¨æ–¹å¼ï¼Œåªè¦ä½¿ç”¨ byzerllm éƒ¨ç½²çš„ï¼Œå°±éƒ½å¯ä»¥ç”¨ç›¸åŒçš„æ–¹å¼ä½¿ç”¨ã€‚

æˆ‘ä»¬æ¥çœ‹çœ‹æ€Žä¹ˆåœ¨ Pythonä¸­ä½¿ç”¨ï¼š

```python
import byzerllm

byzerllm.connect_cluster()

@byzerllm.prompt(llm="gpt3_5_chat")
def hello_llm(name:str)->str:
    '''
    ä½ å¥½ï¼Œæˆ‘æ˜¯{{ name }}ï¼Œä½ æ˜¯è°ï¼Ÿ
    '''

hello_llm("byzerllm")
```

ä¸‹é¢æ˜¯è¾“å‡ºï¼š

![](../source/assets/image2.png)

æ˜¯ä¸æ˜¯å¾ˆç¥žå¥‡ï¼Ÿè¿™å°±æ˜¯ prompt å‡½æ•°ã€‚å¦‚æžœä½ æƒ³çŸ¥é“å‘é€ç»™å¤§æ¨¡åž‹çš„å®Œæ•´Promptæ˜¯å•¥ï¼Ÿä½ å¯ä»¥è¿™ä¹ˆç”¨ï¼š

```python
hello_llm.prompt("byzerllm")
```

![](../source/assets/image3.png)

ä½ ä¹Ÿå¯ä»¥æ¢ä¸ªæ¨¡åž‹åŽ»æ”¯æŒ hello_llm å‡½æ•°ï¼š

```python
hello_llm.with_llm("æ–°æ¨¡åž‹").prompt("byzerllm")
```

å½“ç„¶äº†ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡ chat_oai ç­‰ä¼ ç»Ÿæ–¹å¼å’Œå¤§æ¨¡åž‹æ²Ÿé€šï¼Œé™äºŽç¯‡å¹…å°±ä¸ä¸¾ä¾‹å­äº†ï¼ŒåŽé¢ä¼šæœ‰æ‰€æåŠã€‚

å¦‚æžœä½ å¸Œæœ›å¯¹æŽ¥ç¬¬ä¸‰æ–¹çš„æ¯”å¦‚ Jan, NextChat ç­‰èŠå¤©ç•Œé¢ï¼Œä½ å¯ä»¥è¿™æ ·ï¼š

```bash
byzerllm serve --port 80000
```

å°±å¯ä»¥å¯åŠ¨ä¸€ä¸ª OpenAI å…¼å®¹çš„æœåŠ¡ï¼Œç„¶åŽä½ é…ç½®ä¸‹è¿™äº›èŠå¤©ç•Œé¢åŽé¢çš„ç«¯å£å°±å¯ä»¥ç›´æŽ¥ç”¨äº†ã€‚

![](../source/assets/image4.png)

å¦‚æžœä½ å–œæ¬¢ç”¨ Ollama, ä¹Ÿæ²¡é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠä»–å½“åšä¸€ä¸ªSaaSæœåŠ¡æ¥ç”¨ï¼Œæ¯”å¦‚è¿™æ ·æ¥éƒ¨ç½²ï¼š

```bash
byzerllm deploy  --pretrained_model_type saas/openai \
--cpus_per_worker 0.01 \
--gpus_per_worker 0 \
--num_workers 2 \
--infer_params saas.api_key=xxxx saas.model=llama3:70b-instruct-q4_0  saas.base_url="http://192.168.3.106:11434/v1/" \
--model ollama_llama3_chat
```

è¿™æ ·å°±å¯ä»¥åƒå‰é¢ä¸€æ ·ç”¨ Ollama éƒ¨ç½²çš„æ¨¡åž‹äº†ã€‚

å‰é¢æˆ‘ä»¬è¯´ï¼Œå¦‚ä½•æ”¯æŒå„ç§æ¨¡åž‹çš„ function callingå‘¢ï¼Ÿæ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š

```python
t = llm.chat_oai([{
    "content":'''è®¡ç®—å½“å‰æ—¶é—´''',
    "role":"user"    
}],tools=[compute_date_range,compute_now],execute_tool=True)

t[0].values
```
æˆ‘ä»¬åªè¦æŠŠå‡½æ•°å½“ tools ä¼ å…¥å°±å¯ä»¥å•¦ï¼ŒåŸºæœ¬å¤§éƒ¨åˆ†30Bä»¥ä¸Šçš„æ¨¡åž‹éƒ½æ”¯æŒè¿™ä¸ªåŠŸèƒ½ã€‚

è¿˜å¯ä»¥è®©å¤§æ¨¡åž‹è¿”å›ž Pythonå¯¹è±¡ï¼š

```python
import pydantic 

class Story(pydantic.BaseModel):
    '''
    æ•…äº‹
    '''

    title: str = pydantic.Field(description="æ•…äº‹çš„æ ‡é¢˜")
    body: str = pydantic.Field(description="æ•…äº‹ä¸»ä½“")



t = llm.chat_oai([
{
    "content":f'''è¯·ç»™æˆ‘è®²ä¸ªæ•…äº‹ï¼Œåˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ ‡é¢˜ï¼Œä¸€ä¸ªæ•…äº‹ä¸»ä½“''',
    "role":"user"
},
],response_class=Story)

t[0].value

## output: Story(title='å‹‡æ•¢çš„å°å…”å­
```

åªè¦æŒ‡å®šä¸‹ response_class å³å¯ï¼Œæ˜¯ä¸æ˜¯å¾ˆæ–¹ä¾¿ï¼Ÿ

## å†…ç½®çš„å‘é‡/å…¨æ–‡æ£€ç´¢åº“

å¦‚æžœä½ æƒ³è¦ä¸€ä¸ªå‘é‡+å…¨æ–‡æ£€ç´¢çš„æ•°æ®åº“ï¼Œå¯ä»¥è¿™æ ·ï¼š

```bash
byzerllm storage start
```

å®ƒä¼šè‡ªåŠ¨ä¸‹è½½å’Œå®‰è£…ã€‚ç„¶åŽä½ å°±å¯ä»¥é…åˆ auto-coder æ¥æž„å»ºä¸€ä¸ªæœ¬åœ°çŸ¥è¯†åº“äº†ï¼Œä¸€æ¡å‘½ä»¤è§£å†³æˆ˜æ–—ï¼š

```bash
auto-coder doc build --source_dir /Users/allwefantasy/projects/doc_repo \
--model gpt3_5_chat \
--emb_model gpt_emb 
```

çŽ°åœ¨ä½ å°±å¯ä»¥æŸ¥è¯¢ä½ çš„ç§äººçŸ¥è¯†åº“äº†ï¼š

```bash
auto-coder doc query --model gpt3_5_chat \
--emb_model gpt_emb \
--query "å¦‚ä½•é€šè¿‡ byzerllm éƒ¨ç½² gpt çš„å‘é‡æ¨¡åž‹ï¼Œæ¨¡åž‹åå­—å« gpt_emb "
```

è¾“å‡ºï¼š

```
=============RESPONSE==================


2024-04-29 16:09:00.048 | INFO     | autocoder.utils.llm_client_interceptors:token_counter_interceptor:16 - Input tokens count: 0, Generated tokens count: 0
é€šè¿‡ byzerllm éƒ¨ç½² gpt çš„å‘é‡æ¨¡åž‹ï¼Œæ¨¡åž‹åå­—å« gpt_embï¼Œéœ€è¦ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š


byzerllm deploy --pretrained_model_type saas/openai \
--cpus_per_worker 0.001 \
--gpus_per_worker 0 \
--num_workers 1 \
--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \
--model gpt_emb


=============CONTEXTS==================
/Users/allwefantasy/projects/doc_repo/deploy_models/run.txt
```

å¦å¤–ï¼Œbyzerllm storage ä¹Ÿæ˜¯æ”¯æŒå¤šworker åˆ†å¸ƒå¼éƒ¨ç½²çš„å“¦ã€‚

## æ€»ç»“
è¿™ç¯‡æ–‡ç« ç®€å•ä»‹ç»äº†ä¸‹ Python åº“ byzerllm, ç”¨å®ƒå¯ä»¥è½»æ¾ç®¡ç†ä»»æ„å¤§æ¨¡åž‹ï¼Œå¹¶ä¸”è‡ªå¸¦çš„å­˜å‚¨å¯ä»¥è®©ç”¨æˆ·å®Œæˆå¾ˆå¤šæœ‰æ„æ€çš„äº‹æƒ…ï¼Œæœ€åŽï¼Œbyzerllm è¿˜æä¾›äº†éžå¸¸å¼ºå¤§çš„ç¼–ç¨‹æŽ¥å£ï¼Œè®©ä½ ä½¿ç”¨å¤§æ¨¡åž‹å°±å’Œå¹³æ—¶æ™®é€šç¼–ç¨‹ä¸€æ ·ã€‚

##File: /Users/allwefantasy/projects/byzer-llm/build/lib/byzerllm/bark/assets/prompts/readme.md
# Example Prompts Data

## Version Two
The `v2` prompts are better engineered to follow text with a consistent voice.
To use them, simply include `v2` in the prompt. For example
```python
from bark import generate_audio
text_prompt = "madam I'm adam"
audio_array = generate_audio(text_prompt, history_prompt="v2/en_speaker_1")
```

## Prompt Format
The provided data is in the .npz format, which is a file format used in Python for storing arrays and data. The data contains three arrays: semantic_prompt, coarse_prompt, and fine_prompt.

```semantic_prompt```

The semantic_prompt array contains a sequence of token IDs generated by the BERT tokenizer from Hugging Face. These tokens encode the text input and are used as an input to generate the audio output. The shape of this array is (n,), where n is the number of tokens in the input text.

```coarse_prompt```

The coarse_prompt array is an intermediate output of the text-to-speech pipeline, and contains token IDs generated by the first two codebooks of the EnCodec Codec from Facebook. This step converts the semantic tokens into a different representation that is better suited for the subsequent step. The shape of this array is (2, m), where m is the number of tokens after conversion by the EnCodec Codec.

```fine_prompt```

The fine_prompt array is a further processed output of the pipeline, and contains 8 codebooks from the EnCodec Codec. These codebooks represent the final stage of tokenization, and the resulting tokens are used to generate the audio output. The shape of this array is (8, p), where p is the number of tokens after further processing by the EnCodec Codec.

Overall, these arrays represent different stages of a text-to-speech pipeline that converts text input into synthesized audio output. The semantic_prompt array represents the input text, while coarse_prompt and fine_prompt represent intermediate and final stages of tokenization, respectively.





##File: /Users/allwefantasy/projects/byzer-llm/src/byzerllm/bark/assets/prompts/readme.md
# Example Prompts Data

## Version Two
The `v2` prompts are better engineered to follow text with a consistent voice.
To use them, simply include `v2` in the prompt. For example
```python
from bark import generate_audio
text_prompt = "madam I'm adam"
audio_array = generate_audio(text_prompt, history_prompt="v2/en_speaker_1")
```

## Prompt Format
The provided data is in the .npz format, which is a file format used in Python for storing arrays and data. The data contains three arrays: semantic_prompt, coarse_prompt, and fine_prompt.

```semantic_prompt```

The semantic_prompt array contains a sequence of token IDs generated by the BERT tokenizer from Hugging Face. These tokens encode the text input and are used as an input to generate the audio output. The shape of this array is (n,), where n is the number of tokens in the input text.

```coarse_prompt```

The coarse_prompt array is an intermediate output of the text-to-speech pipeline, and contains token IDs generated by the first two codebooks of the EnCodec Codec from Facebook. This step converts the semantic tokens into a different representation that is better suited for the subsequent step. The shape of this array is (2, m), where m is the number of tokens after conversion by the EnCodec Codec.

```fine_prompt```

The fine_prompt array is a further processed output of the pipeline, and contains 8 codebooks from the EnCodec Codec. These codebooks represent the final stage of tokenization, and the resulting tokens are used to generate the audio output. The shape of this array is (8, p), where p is the number of tokens after further processing by the EnCodec Codec.

Overall, these arrays represent different stages of a text-to-speech pipeline that converts text input into synthesized audio output. The semantic_prompt array represents the input text, while coarse_prompt and fine_prompt represent intermediate and final stages of tokenization, respectively.





